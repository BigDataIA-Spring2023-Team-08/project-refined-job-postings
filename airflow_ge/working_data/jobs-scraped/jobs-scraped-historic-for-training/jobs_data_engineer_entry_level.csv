Job ID,Title,Company,Date,Link,Description Length,Description
3580991991,Data Engineer,Nike,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-nike-3580991991?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=a5pwQLEdYEm940EriOYCzw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card,2932,"Become a Part of the NIKE, Inc. Team

NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.

Nike USA, Inc., located in Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.

Applicant must have a Bachelor’s degree in Data Science, Computer Engineering, Computer Science, or Computer Information Systems and five (5) years of progressive post-baccalaureate experience in the job offered or engineering-related occupation. Experience must include the following-

Programming ability (Python, SQL);
Database related concept;
Big Data exposure;
Spark;
Airflow (Orchestration tools);
Cloud Solutions;
Software/Data design ability;
CI/CD understanding and implementation;
Code review;
Data Architecture; and
AWS, Azure.


NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.

]]>


Show more "
3554222370,Data Engineer,Patreon,2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-at-patreon-3554222370?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=DUr0dQpGE%2BYRt%2Fk5TjqGWA%3D%3D&position=2&pageNum=0&trk=public_jobs_jserp-result_search-card,4306,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We’re building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.

We’re leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We’re continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.

This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.

About The Role



Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both “data analytics” and “data infrastructure” type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.


About You



Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.


About Patreon


Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:


Put Creators First | They’re the reason we’re here. When creators win, we win. ****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don’t quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.


Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.

Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.

The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.

San Francisco Pay Range

$139,000 — $193,500 USD
Show more "
3580202862,DATA ENGINEER,Dollar General,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-dollar-general-3580202862?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=3h%2Bmsp39D1%2F3sBXBOvYvng%3D%3D&position=3&pageNum=0&trk=public_jobs_jserp-result_search-card,3484,"Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.




General Summary




Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.




Duties & Responsibilities




Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies.
Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.




Knowledge, Skills and Abilities




Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g IBM Certified Data Engineer) is a plus
Experience with big data tools Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
Experience with Snowflake/Azure cloud services EC2, EMR, RDS, Redshift
Experience with stream-processing systems Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages Python, Java, C++, Scala, etc




Work Experience &/or Education




Degree in information technology or computer science with additional vendor-specific certification.
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with a cloud platform such as Snowflake / Azure or Databricks
Show more "
3576971441,Data Engineer (Entry Level ),Patterned Learning AI,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-entry-level-at-patterned-learning-ai-3576971441?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=Daf4DHrJnm8LvYHYbTy4WQ%3D%3D&position=4&pageNum=0&trk=public_jobs_jserp-result_search-card,1349,"REMOTE (US/Canada Residing people only, with work permit)




Patterned Learning – Data Engineer (Entry Level ) , FULL-TIME, Salary $100K - $130K a year.




About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!




About The Job




Required Skills and Experience:




Experience in writing cloud-based software
Expertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)
Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.
Experience delivering software products built using Python
Experience using professional software development practices, including: Agile processes, CI/CD, etc)
Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)
Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)




Special Benefits You Will Love




Flexible vacation, paid holidays, and paid sick days
401(k) with up to 2% employer match (no match)
Health, vision, and dental insurance.




Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
Show more "
3576230763,Data Engineer,Near,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-near-3576230763?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=xGMZLfrLbZS1xpBCnGqLtQ%3D%3D&position=5&pageNum=0&trk=public_jobs_jserp-result_search-card,2284,"Description




Near is seeking a Data Engineer to join our team. In this role, you will have the opportunity to work on a huge scale of data, a cutting-edge tech stack, and leverage your skills and toolset to help us build a high-value and scalable product. You will be responsible for developing techniques to enhance data. You will also work with our data scientists, data analysts and product stakeholders to implement processes and infrastructure in order to support our data driven reports and analytics. These systems process billions of location data points per day.




You will be part of one of the fastest growing Enterprise SaaS companies – a great opportunity for people who can work independently and are self-driven.




A Day in the Life:




Design, build and maintain the data pipeline for ingress and egress of location-based data
Build and optimize the data warehouse to allow for report generation and analytics
Assemble large, complex data sets that meet business requirements




What you bring to the role:




Moderate experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Moderate knowledge of AWS cloud services: EC2, EMR, RDS, Redshift, S3, Athena
Experience with object-oriented/object function scripting languages (Python)
Experience with Airflow or other orchestration tool
Experience with Docker
Experience with big data technologies: Spark, Hive, Mad/Reduce, Hadoop
Experience with UNIX/Linux including basic commands and shell scripting
Experience breaking down complex problems into manageable steps to solve
Must be a team player with strong attention to detail, and also able to work independently
Proven track record at delivering timely and accurate information in a fast-paced environment
Ability to build rapport with product, project, and QA teams
Excellent critical thinking, problem solving, and mathematical skills, and sound judgment
Experience building and optimizing AWS data pipelines, architectures and data sets
Familiarity with mobile location data intelligence infrastructure and industry
GIS background is nice to have
Able to work independently




Compensation:




$160,000 - 175,000 base salary + bonus + RSUs
Comprehensive benefits package
Show more "
3549205728,Data Engineer,Massachusetts Institute of Technology,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-at-massachusetts-institute-of-technology-3549205728?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=bKveZZzSdFvABQJ1TLdJHQ%3D%3D&position=6&pageNum=0&trk=public_jobs_jserp-result_search-card,97,Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.
3580201929,DATA ENGINEER,Dollar General,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-dollar-general-3580201929?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=tb7RXqLU%2BnBjz7TVYyasYw%3D%3D&position=7&pageNum=0&trk=public_jobs_jserp-result_search-card,3484,"Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.




General Summary




Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.




Duties & Responsibilities




Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies.
Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.




Knowledge, Skills and Abilities




Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g IBM Certified Data Engineer) is a plus
Experience with big data tools Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
Experience with Snowflake/Azure cloud services EC2, EMR, RDS, Redshift
Experience with stream-processing systems Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages Python, Java, C++, Scala, etc




Work Experience &/or Education




Degree in information technology or computer science with additional vendor-specific certification.
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with a cloud platform such as Snowflake / Azure or Databricks
Show more "
3580764388,Data Engineer,Itility US,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-itility-us-3580764388?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=Crfv18L2eHWEcsq%2FvXnfxg%3D%3D&position=8&pageNum=0&trk=public_jobs_jserp-result_search-card,5328,"Do you have experience with writing code to ingest data? Do you like data wrangling, digging into data sources, and processing data to a readable and usable state? Do you love that feeling of accomplishment when data is flowing seamlessly into a data lake, day in day out, hour after hour, based on code that you have carefully crafted? Then this job opening is just what you are looking for!




We need your expertise




For multiple enterprise customers we create data connectors to make data flow from various sources to a data lake of choice: that can be Splunk, Databricks, Hadoop, or any other technical implementation. What they have in common, is that the data will be used in a production environment, so it must flow seamlessly and needs to be monitored for disruption. Data validation and data quality are also important.




What You'll Do




Create data connectors, using Python or other coding languages.
Define data validation tests to run in the data pipeline.
Define monitoring and alerting to ensure visibility when the data flow is interrupted or corrupted.
If incidents occur, you take the lead in getting to the root cause as soon as possible to solve the incident with as little impact on the end users as possible.
You and the team are responsible for building, deploying, maintaining, and optimizing the data ingestion and data connectors to have data flow to the data lake.
Building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets of structured, semi-structured and unstructured data.
Build data products incrementally and integrating and managing data sets from multiple sources.
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production.
Work alongside Business Analysts for requirements gathering, and decide how to implement data pipelines, identifying which data to use and define what tooling should be used.
Collaborate with ETL/data services team, application teams to support the development of data solutions.
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production.




Meet Itility




At Itility we believe in merging technology and data to drive our customers one step beyond. Itility digital consultants are experts in data, cloud, software, and IT infrastructure. Our culture can be described as ‘no-nonsense, with passion’. Working at Itility is about working with people, staying close to our customers. We work for large enterprises and innovative startups. Acting as the ‘digital twin’ of customers, we work shoulder-to-shoulder to exceed business goals and push the boundaries of what you thought was possible. Do you like to go above and beyond? Do you want to work with passion for what you do, in a team of people fueled by the same passion? Then we would love to meet you!




This Is What We Offer




You will be given the opportunity to develop in the best way possible, under the personal guidance of fellow data engineers and architects of Itility. If you do not have the required expertise for the job but do have the passion for data engineering, we offer a substantial trainee program to get you up to speed for the job in a planned manner.




Other requirements:




You have a bachelor’s or master’s degree.
You have experience creating data ingestion scripts.
You are a team player and you have good communication skills.
You have a good understanding of SQL and Python.
Experience with Linux is a prerequisite.
Knowledge of continuous integration & delivery tooling: e.g. Jira, Git, Jenkins, Bamboo.
Experience with big data platforms and tools such as Hadoop, Spark, Kafka.
AWS or Azure Cloud DevOps Services preferred.
You believe in scrum/agile way-of-working and in software practices that enable a professional data flow.
Ideally, you have worked with data platforms and data lakes within an enterprise environment.
You have a hands-on mindset, a strong customer focus, a problem-solving orientation and can show fast results.




You will report to the Itility program manager and work in close harmony with team members while interfacing with the standing client IT organization.




5-10% travel required.




Screening is part of the hiring procedure.




This is a hybrid position, so it is a requirement that you are based within 100 miles of the San Diego area.




Anticipated salary range: $100,000-$135,000




Factors in determining the appropriate compensation for a role include experience, skills, knowledge, abilities, education, licensure and certifications, and other business and organizational needs. The Hiring Pay Scale referenced in the job posting is the budgeted salary or hourly range that Itility reasonably expects to pay for this position. The Annual Full Pay Range may be broader than what Itility anticipates paying for this position, based on internal equity, level and budget.




Location




San Diego, CA




Most of our positions are hybrid/remote. Unless otherwise indicated in the job post, it is a requirement that you are based within 100 miles of the job's stated location.




Powered by JazzHR




GzaYgMrybl
Show more "
3578404900,Data Engineer I - (Remote),Help at Home,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-i-remote-at-help-at-home-3578404900?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=5EIQoimUw6SxTTA0Kno8wg%3D%3D&position=9&pageNum=0&trk=public_jobs_jserp-result_search-card,5738,"Help at Home is the leading national provider of in-home personal care services, where our mission is to enable individuals to live with independence and dignity at home. Our team supports 66,000 clients monthly with the help of 49,000 compassionate caregivers across 12 states. We’re looking for people who care about others, who are willing to listen, lean in and make impactful change. Each role at Help at Home can have a positive impact in supporting our caregivers and clients. If you are someone who leads with passion and integrity and are looking to join a rapidly growing, industry leading team, Help at Home may be a good fit for you.

Job Summary

The Data Engineer is responsible for delivering data warehouse solutions by building enterprise data models and writing ETL/ELT processes to map, cleanse and standardize multiple source systems of data to populate the enterprise data models for business consumption. They will be working throughout a multi-layered data warehouse environment in order to support a wide variety of business needs. This role is responsible for delivering solutions that meet our growing business needs as they relate to our enterprise data and analytics strategy for Care Coordination and Help at Home. The ideal candidate should be comfortable with driving creation of a platform with focus on the Data Mesh architecture.

As a Key Member Of The Team

You are flexible and can embrace change
You value progress over perfection
You care about your work, the team you’re on, and the people we are helping
You make it a priority to get to know the people around you – build relationships with your colleagues and business partners
You say what needs to be said, while considering how it’ll affect culture and output
Hold others to a high standard

Duties/Responsibilities

Leverages CDC to optimize the ETL/ELT processes they develop including being able to develop routines to determine changed records when they are not provided by the source system
Creates GitHub actions and builds pipelines for dev, stage, and prod
Effectively communicates with stakeholders to understand business requirements with the ability to translate requirements into technical designs and solutions and convey the requirements and designs to team members
Profiles the source system data and assess its data quality to design and develop solutions to improve the data quality such that it maps properly into the data warehouse structures and meets the data warehouse standards
Works with the business to determine survivorship rules, builds the golden record based on the rules and then builds and maintains structures for the integrated dimensions and facts; understands master data guiding principles and best practices in terms of the technical de-duplication process, which includes enhancing data quality to support matching and grouping
In alignment with Data Mesh, builds ingestion, integration and sharing patterns and frameworks for better data access
Maps source system data structures into the data warehouse data model (source-target mapping) and enhance the data warehouse data model as needed to meet the business needs
Improves our overall data security posture; strengthens our SDLC and Devops strategy in support of sustained business growth
Maintains knowledge of current trends and developments in the field and actively explore emerging technologies

Required Skills/Abilities

Cloud-first mindset
Ability to work in a fast-paced dynamic, environment delivering solutions that significantly impact the business
Knowledge of testing frameworks and TDD or BDD
Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude who is willing to support and teach fellow team members
Strong data analysis skills
Strong relational database skills including advanced SQL knowledge and the ability to create complex queries and stored procedures.
Strong understanding of data warehouse and business intelligence design principles and industry best practices, including relational and dimensional modeling and ETL/ELT methods
Understanding of trunk-based development.
Working knowledge of Snowflake
Working knowledge of Snowflake DBMS and JSON

Education And Experience

AWS architecture, developer, security, and networking experience.
3+ Years of experience in data engineering required.
Bachelor’s Degree in Computer Science, Data & Analytics, Information Management, Healthcare Informatics, Business Administration, Statistics, or related field required.
Demonstrated experience with automation.
Demonstrated experience with one or more of the following languages: Go, Python, Typescript.
Strong experience with various AWS services like (S3, Lambda, Glue, EMR, CloudFormation, MWAA, Kinesis, MSK).
Cloud (AWS), Warehousing, and Snowflake experience preferred

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions upon request.

Help At Home is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or religion or other legally protected status.
Show more "
3575035401,Data Engineer,Miso Robotics,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-miso-robotics-3575035401?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=XfRltA%2BIdAXeszgHrgM5vA%3D%3D&position=10&pageNum=0&trk=public_jobs_jserp-result_search-card,4065,"Our Company




We are a cutting-edge robotics startup that aims to revolutionize the industry through innovative designs, smart automation, and seamless integration. Our dynamic team is dedicated to pushing the boundaries of robotics by leveraging advanced technologies and creative problem-solving. We are seeking a best-in-class Data Engineer to join our team and help us build the foundation for the next generation of robotic solutions.




Our Values




We live with a TEAM mindset - we win together. Individual performance serves the bigger goal of working as a team.
We are easy on people, hard on problems. We work relentlessly to solve issues.
We use Candor - ego has no place here. Always polite and extremely direct.
We operate with Rigor - superb execution is a core skill.
We are bought in - each of us is dedicated to the mission.
Innovation is in our blood - we are intrepid.
We think big - we’re here to make an impact. Miso plays large ball.




Benefits and Perks We Offer




Unlimited/Flexible Vacation
Comprehensive health benefits
401K plan
A competitive salary and benefits package.
Opportunities for professional growth and development.
A creative and supportive work environment where your ideas can make a real impact.
The chance to work on cutting-edge robotics projects and be part of a rapidly growing industry.
If you are passionate about data engineering, eager to tackle complex challenges, and want to contribute to the future of robotics, we would love to hear from you. Apply now and join our team on this exciting journey!




The Role




As a Data Engineer at our company, you will play a critical role in designing, developing, and maintaining our data infrastructure. You will be responsible for managing and optimizing our Data Lake, implementing AWS services, and integrating IoT and NoSQL solutions to support our robotics systems. Your expertise in identity management will ensure the security and integrity of our data assets.




Additional Information: The pay range for this position is $90,000 - $140,000 + equity + benefits. Our salary ranges are determined by the experience and education required, and level of responsibility. The range posted for this role represents a range that Miso Robotics, in good faith, believes it is willing to pay at the time of this posting. The pay is determined by job related skills, training, education, and experience.




What You’ll Do




Design, develop, and maintain scalable and efficient data pipelines to support our robotics solutions.
Implement and optimize AWS services, including EC2, S3, Lambda, Kinesis, and Redshift for data storage and processing.
Leverage IoT technologies to collect and process data from various sensors and robotic systems.
Develop and maintain NoSQL databases, ensuring high performance, availability, and security.
Implement identity management and access control solutions to safeguard sensitive data.
Collaborate with cross-functional teams to identify data requirements and drive data-related projects.
Continuously improve data engineering processes, tools, and methodologies to maintain a competitive edge.
Troubleshoot data-related issues and provide timely resolutions.
Stay current with industry trends and emerging technologies to ensure the adoption of best practices.




Requirements




Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
A minimum of 4 years of experience as a Data Engineer, preferably in a startup or robotics environment.
Strong expertise in AWS services and Data Lake architectures.
Hands-on experience with IoT technologies and NoSQL databases (e.g., MongoDB, Cassandra, or Couchbase).
Proficiency in identity management and access control solutions.
Excellent programming skills in Python, Java, or other relevant languages.
Strong analytical, problem-solving, and communication skills.
Ability to work effectively in a fast-paced, collaborative environment.
Knowledge of robotics systems and technologies is a plus.
Ability to work in our HQ in Pasadena, CA.
Show more "
3564019904,Data Engineer,"Data Ideology, LLC",2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-data-ideology-llc-3564019904?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=SjTJarhIMXhEwBd6nEPEaA%3D%3D&position=11&pageNum=0&trk=public_jobs_jserp-result_search-card,3787,"Data Ideology



At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious.For more information about Data Ideology, visitwww.dataideology.com



Data Engineer - Contract to Hire (CTH)



We are looking for a Data Engineerto join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.



Key Responsibilities



To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.



Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.
Ability to define and implement best practices across database design and ETL.
Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.

Supervisory Responsibilities: None



Qualifications



Education and Experience:



Proven understanding of Data Warehousing, Data Architecture, and BI.
Experience with data pipelines and architecture/engineering.
Knowledge of modern apps and data platforms.
Cloud-based project implementation.
Google BigQuery experience

Knowledge, Skills, and Abilities:



BI/Data Warehousing (3+ years) - Google BigQuery
Cloud platforms (1+ years) - Google Cloud Platform
Dimensional Data Modeling (3+ years)
ETL (3+ years)
SQL (3+ years)
Business Intelligence (1+ years) - Power BI

Work Environment:



Remote work from home.
Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.

Physical Demands:



Must be able to remain in a stationary position 50% of the time.
The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.

Benefits:



Unlimited Discretionary Time Off Policy
100% company paid - insurance (medical, dental, vision) for employees
100% company paid - short and long-term disability insurance for employees
100% company paid - life insurance and AD&D insurance for employees
100% company paid – employee assistance program
Retirement plans with company match
Training and Certification Reimbursement annually
Performance-based incentive program
Commission incentive program
Profit Sharing Plan
Referral Bonuses

Data Ideology is an EEO Employer



Show more "
3555473156,Data Engineer,Attune,2023-04-05,https://www.linkedin.com/jobs/view/data-engineer-at-attune-3555473156?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=qDDF%2FabfnRkcj8igd6YK6w%3D%3D&position=12&pageNum=0&trk=public_jobs_jserp-result_search-card,2335,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.

We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.

Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.

Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.

Job Description

As a Data Engineer joining our growing Data team, you will help maintain our existing data pipelines and internal web applications. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.

Responsibilities:

Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvements
Work with our data engineers, BI, and other teams across the business to develop/refine ETL processes
Understand and answer questions on the data our team maintains


Qualifications:

3+ years experience in analytics, data science, or data engineering role
Strong Python and Postgresql skills
Solid understanding of relational database design and basic query optimization techniques
Experience working with git, and Gitlab or Github
Nice to have:
Experience with Linux CLI, shell programming
Understanding of CI/CD
Experience with AWS EC2 and S3

What we offer you:

140-170k per year
Unlimited PTO
Generous parental and caregiver leave
401K match
Excellent medical, dental, and vision plans
Remote-first culture
And more!
Show more "
3562659761,Data Engineer,Fastechnowiz,2023-04-12,https://www.linkedin.com/jobs/view/data-engineer-at-fastechnowiz-3562659761?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=R24Wzdm%2BKID0RrtapCwWeQ%3D%3D&position=13&pageNum=0&trk=public_jobs_jserp-result_search-card,1050,"Position Details

Title: Data Engineer

Industry: Banking & Financial

Duration: 12 Months- Long term

Location: Smithfield RI/ Durham, NC/ Westlake, TX

Top Skills: Informatica, SQL, Snowflake, python

Required Qualifications

ETL developer with Informatica
Strong SQL Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required one week every 3 months
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization

Additional Experience

Experience with Metadata management solutions / Data lineage is a plus
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus
Vendor management is a plus
Some QA/Testing experience is a plus
Some Kubernetes / Docker experience is a plus
Strong communication and presentation skills
Show more "
3548578312,Data Engineer,Laguna Games,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-at-laguna-games-3548578312?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=CDSdfWAzOdgktNE6Dv1gBg%3D%3D&position=14&pageNum=0&trk=public_jobs_jserp-result_search-card,3384,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.

As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:

Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platform
Design and build scalable data solutions that support the real-time analytics and reporting needs of the business
Collaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetization
Build and maintain data models, data warehouses and data marts to support business intelligence and reporting needs
Ensure data quality, integrity and security across all data sources and systems
Monitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecks
Keep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructure

Qualifications

Bachelor's degree in Computer Science, Computer Engineering, or related field
3+ years of experience in data engineering or related field
Experience with big data technologies such as Hadoop, Spark, Kafka, and Elasticsearch
Strong proficiency in SQL, Python and/or Java programming languages
Experience with cloud-based data solutions such as AWS, Azure or Google Cloud
Familiarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence tools
Understanding of blockchain technology and its use in gaming platforms is a plus
Excellent communication skills, both written and verbal, and ability to collaborate with cross-functional teams

If you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!

Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!

We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.

Learn more here!

https://laguna.games

https://www.cryptounicorns.fun
Show more "
3581584786,Software Engineer - All Levels,Scratch,2023-03-29,https://www.linkedin.com/jobs/view/software-engineer-all-levels-at-scratch-3581584786?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=hBhu8bQ8VWnakJDc07XO3A%3D%3D&position=15&pageNum=0&trk=public_jobs_jserp-result_search-card,4700,"About Scratch




Scratch's mission is to build a repayment platform to change the way consumers experience debt. Most of us take on debt to help us get ahead, or in many cases, to not fall behind. But for most of us—despite its necessity—once we have debt, it doesn't play a healthy role in our lives. It can be hard to understand, difficult to manage, and unaccommodating to changes in our lives.




This is why we started Scratch. We wanted to reinvent the loan servicing industry, well—from scratch. We wanted to create a world where debt plays a healthier and less taxing role in more people's lives. By bringing world-class product and technical thinking to a multi-trillion industry that hasn't seen innovation in well over two decades, Scratch's platform empowers borrowers to have greater control, visibility and flexibility over their financial health, removing the traditional loan servicer from the equation altogether and bringing borrowers closer to their lenders.




Our team is rapidly growing and comes from diverse backgrounds including Dropbox, Pinterest, Prosper, Bloomberg and Ripple. Our office is based in San Francisco.




With a rapidly growing community of lenders and borrowers, we think it's time the world knows there's a better way to pay back their loans. We'd love for you to join in our mission and help play a role in changing the way we experience debt for the better!




Scratch’s technology stack presents many interesting challenges and opportunities for designing and architecting complex systems. Our APIs and user experiences serve many personas: Enterprise (Lenders, Creditors), Consumers (Borrowers, Borrower Guides, Internal Operations). What's at stake? Scratch aims to be the source of truth for trillions of dollars that will shape the future of loan servicing and debt. It's crucial that our novel platform ensures accuracy and precision on non-trivial math down to seven decimal places with zero data loss or security compromises, resiliency to handle a large number of loans at scale as well as the flexibility to handle new and innovative financial instruments.




Our team is highly collaborative and works from our SF office 3 days per week - please note that this role requires a hybrid work model.




About the role:




Design core pieces of Scratch’s APIs and user experiences, optimizing for long-term maintainability, performance, and reliability.
Architect and scale complex systems like our loan servicing engine and time-series database.
Design core pieces of Scratch’s infrastructure, optimizing for long-term maintainability, performance, and reliability.
Take full ownership and responsibility for building, shipping, and maintaining core Scratch features, end-to-end.
Our team is highly collaborative and works from our SF office 3 days per week - please note that this role requires a hybrid work model




You may be a good fit if:




2+ years industry experience.
You enjoy being a generalist working on anything it takes to solve problems and delight users both internally and externally.
You derive joy from refactoring and abstracting in order to make complex systems fun to develop on and easy to understand.
You have experience leading engineering projects.
You have experience in databases, distributed systems, and backend performance measurement and optimization.
You have a strong background in computer science and mathematics.
You thrive in in-office work cultures and are excited to collaborate with your team in person




We want you!




If you made it this far, chances are you’re as excited about working to change how people experience debt as we are—and we love that. Please apply even if you’re unsure about whether you meet every single requirement in this posting. Scratch is looking for smart, intellectually curious people who are invested in our mission, not just those who can “check all the boxes”.




Benefits we Provide:




Health insurance: We cover 99% of your healthcare premiums and 70% of your dependent’s premiums and offer competitive medical, dental, and vision insurance plans.




Learning and development stipend: We offer a $1,500 per year stipend for your personal career development!




Wellness benefit: We’re proud to provide employees a $100/month pre-tax credit towards any gym or fitness program.




Catered lunches and snacks: We have high-quality catered lunches every day and well-stocked kitchens.




Generous PTO (including your birthday!), sick leave, and parental leave




401k program: We offer a 401k program!




Additional benefits: We offer each employee the opportunity to enroll in pre-tax benefits, such as commuter benefits and an FSA account.
Show more "
3577136428,Data Engineer,Converse,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-converse-3577136428?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=f4tLA%2B6W94Z1PDaVUAtBKw%3D%3D&position=16&pageNum=0&trk=public_jobs_jserp-result_search-card,3780,"Become part of the Converse Team




Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.




Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.




Experience Must Include




Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation.




Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github




Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.







Show more "
3576721950,Data Engineer,Compunnel Inc.,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3576721950?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=WSsrQw%2BZ5SB%2Fn6JPRx4JLw%3D%3D&position=17&pageNum=0&trk=public_jobs_jserp-result_search-card,839,"Description




Summary




Customer is looking for Python and SQL hands on expertise.




Look for Python certified consultants.




Responsibilities




The data engineer will be responsible for architecting and implementing very large-scale data intelligence solutions around Snowflake.




The data engineer will analyze the raw data and develop and maintain datasets on Snowflake based on business needs and objectives.




They would explore ways to enhance data quality and reliability and identify opportunities for data acquisition.




They would also collaborate with data scientists and architects on several projects.




They must have previous experience as a data engineer and should be an expert in both Python and SQL.




Snowflake, Jenkins & Airflow experience is nice to have




Education: Bachelors Degree
Show more "
3573801223,Junior Software Engineer - Jonsson Cancer Center,UCLA Health,2023-03-26,https://www.linkedin.com/jobs/view/junior-software-engineer-jonsson-cancer-center-at-ucla-health-3573801223?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=cUUw6qngMZWvQwSBykcKXg%3D%3D&position=18&pageNum=0&trk=public_jobs_jserp-result_search-card,1769,"Description

The newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Centre is seeking a Junior Software Engineer with research and development experience. In this role, you will work with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer that will transform the lives of cancer patients. Junior Software Engineers on our team are passionate about applying their knowledge of software-development and design to improve scientific research. They develop scalable and distributed software solutions that maximize utilization of both local high performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. You will use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability to boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be responsible for one major and several minor projects at any point in time. We are in a rapid growth-phase and so you will be involved in the hiring of new team members. Salary range: $5525.00-$10925.00 Monthly
Show more "
3580193539,Data Engineer,Refiberd,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-refiberd-3580193539?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=GFdkm2Zds9t5Yf81%2B5jKWg%3D%3D&position=19&pageNum=0&trk=public_jobs_jserp-result_search-card,2960,"Company Description

Refiberd is on a mission to create a circular economy for the fashion industry. Every year 93 billion tons of textile waste are generated and less than 1% of that is recycled into new clothing.  Refiberd is tackling the textile waste crisis with our novel AI and spectroscopy-based sorting system that processes and sorts textile waste for different recycling streams. By being able to accurately sort textile waste we can maximize the amount of textile waste that gets recycled and unlock circularity for the fashion industry. 




Refiberd is a venture-backed startup that is passionate about solving hard problems like climate change and fast fashion. Our passion translates to a collaborative, innovative, inclusive and equitable culture that makes Refiberd an enjoyable place to work.




Role

The Data Engineer will be responsible for developing, maintaining, and optimizing Refiberd’s data warehouse, data pipeline, and data products.




Responsibilities

Design, build and maintain data pipelines for various business needs such as model development and production
Prepare raw data in data warehouses into a consumable dataset for machine learning
Automate data workflows such as data ingestion, aggregation, and ETL (extract, transform, load) processing
Partner with relevant stakeholders to deploy machine learning models in production 
Build, maintain, and deploy data products for analytics on cloud platforms like AWS to provide actionable insights into key business performance metrics
Monitor data systems performance and implement optimization strategies
Ensure data accuracy, integrity, reproducibility, privacy, security, and compliance through quality control procedures




Required Skills and Qualifications

Bachelor's degree in computer science, mathematics, or a related field
3+ experience as a data engineer
Advanced proficiency in programming languages, specifically Python and Bash
Advanced SQL skills and experience with relational databases and database design
Strong working knowledge of cloud-based solutions (e.g. AWS, Azure, or GCP) 
Experience working with cloud data warehouse solutions (e.g. Snowflake, Redshift, BigQuery, Azure, etc.) 
Strong proficiency in data pipeline and workflow management tools (e.g., Airflow, Azkaban, Kafka) 
Experience working with data ingestion tools such (e.g. Fivetran, stitch, or Matillion) 
Experience building and deploying machine learning models in production
Superb analytical and problem-solving abilities.
Great verbal and written communication and collaboration skills.
Takes initiative
Excellent time management and organizational abilities.
Strong experience in programming and statistics
Ability to lead and work independently







Refiberd is an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status or disability status.




Show more "
3567700423,Software Engineer - New Grad,Applied Intuition,2023-04-15,https://www.linkedin.com/jobs/view/software-engineer-new-grad-at-applied-intuition-3567700423?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=SD9FcmgPKC9ThrTKdxlUaw%3D%3D&position=20&pageNum=0&trk=public_jobs_jserp-result_search-card,3536,"About Applied

Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.

Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Los Angeles, Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.

About The Role

We are looking for bright engineers interested in designing elegant solutions to difficult problems in the autonomy space. Our software engineers work across our suite of products, tackling a variety of full-stack, infrastructure, robotics, and graphics challenges. At Applied, we encourage engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.

At Applied you will:

Work across our entire stack to develop new products, features, and tools for our customers' autonomy development workflows
Have an unparalleled opportunity to work with domain experts across a variety of fields: infrastructure, robotics, and graphics engineers, as well as startup veterans
Carve out your own area of expertise and influence product decisions
Collaborate with other members in the autonomy ecosystem and learn about different approaches to solving core issues in autonomy


We're looking for someone who:

Is a self-starter and can quickly become comfortable with new technical tools
Designs efficient and effective solutions to a wide range of engineering challenges
Takes initiative in a fast-paced environment


Nice to have:

Working knowledge of frontend, API layer, database ORM, containerization, or cluster orchestration frameworks (such as React, GraphQL, SQLAlchemy, Docker, or Kubernetes)
Experience working with simulation tools, modeling physical problems, or using robotics middleware (such as ROS)


Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.

Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Show more "
3564951155,Data Engineer (Python) 1129,"Certec, Inc.",2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-python-1129-at-certec-inc-3564951155?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=Z3qAs8EbWH1R0AfrHmjGFw%3D%3D&position=21&pageNum=0&trk=public_jobs_jserp-result_search-card,1603,"Data Engineer (Python) 170929

Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C .

we can use Full time friendly

Location: hybrid 5 days onsite/month. Locations are Smithfield RI, Durham NC or Westlake TX

Duration: 12+ months, open ended

Ideally looking for someone who has strong communication skills since they will be communicating with SR managers. Looking for someone who can upscale themselves is very important- Can work independently, does not need hand holding. Ideally looking for someone who has a passion and gets excited about working with new technologies

ETL developer with Informatica
Strong SQL Snowflake and SQL Server will be a huge plus
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Basic Python knowledge is required
Good working knowledge of Control-M/Automation tools.
Some experience in DevOps
Production Support will be required one week every 3 months
Excellent interpersonal and communication skills

Excellent collaboration skills to work with multiple teams in the organization

What are nice to have skills?

Snowflake

Control M and UDeploy

Go to our Website Job listing here: Job Listing 1129

Please download and complete this Matrix prior to submission.

PM 1129

Then

PLEASE USE EASY ""APPLY BUTTON"" (not just apply button) TO SUBMIT RESUME AND SKILLS MATRIX

Job Listing 1129

The send an email to the listers email address with just candidate name and job number. NO need to attached resume or anything else.

Thanks,

Jay Kernes

Certec Consulting, inc

Fax 888-523-7832

We are certified as a Women's Business Enterprise (WBE)
Show more "
3574440970,Software/Data Engineer (Entry-level),B Capital,2023-04-03,https://www.linkedin.com/jobs/view/software-data-engineer-entry-level-at-b-capital-3574440970?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=WRmKBV%2BNj%2FCXh4M2KlBvUg%3D%3D&position=22&pageNum=0&trk=public_jobs_jserp-result_search-card,4688,"Position Summary

The position will focus on helping build and maintain backend integrations of our various systems (e.g., Financial System, HR System, CRM System), including Data Warehouse archiving and synchronizing. The position will also work with our Business Intelligence team to develop views/queries/tables to power Tableau Dashboards used throughout various venture capital investment lifecycle stages. Secondarily, the position will also maintain custom internal applications utilizing data from our Data Warehouse and various systems.

Basic Job Responsibilities

Implementing processes that are fault-tolerant, resilient, and efficient in Databricks and AWS Lambda
Writing clean code that is maintainable and easy to understand
Participate in code reviews
Maintain custom application plugins in WordPress
Update our project management system, JIRA, with notes, questions, and feedback on current and future tasks

Basic Job Requirements

Recent Graduate with a degree or related studies in computer science, math, or other technical field (e.g., applied mathematics, statistics, physics or engineering)
Prior internship experience in similar field will be highly preferred
Experience with programming languages such as Python or PHP
Experience with SQL databases such as MySQL or PostgreSQL
Experience with a source control system, such as Git
Optional experience with Databricks, WordPress, JIRA, Cloud Computing, such as AWS Lambda/S3
Ability to working in a variety of code bases and production environments.
Good problem-solving skills and the ability to think of solutions.
Good verbal/writing skills for explaining your work to others.
Passion to grow in the role to take on AI/ML projects
Curiosity for technology and a desire to learn new skills, frameworks, and programming languages.
Interest in learning financial terminology and concepts used in the Venture Capital Market
We offer hybrid work, where most of the time will be done remote, although you are required to go into the office once a week or so
This role can be based in Los Angeles, New York and San Francisco

At B Capital, the health and safety of our people is our number one priority. As a condition of employment all new hires are required to be fully vaccinated against Covid-19 and must show proof of such vaccination.

About b Capital

B Capital is a multi-stage global investment firm that partners with extraordinary entrepreneurs to shape the future through technology. With $6.3 billion in assets under management across multiple funds, the firm focuses on seed to late-stage venture growth investments, primarily in the enterprise, financial technology and healthcare sectors. Founded in 2015, B Capital leverages an integrated team across eight locations in the US and Asia, as well as a strategic partnership with BCG, to provide the value-added support entrepreneurs need to scale fast and efficiently, expand into new markets and build exceptional companies. For more information, click here.

b Capital Group Core Values

B Honest & Trustworthy - Our people and our culture are the heart of our business. We are self-aware, supportive, and trust ourselves and each other. We speak the truth with positive intent. We hold ourselves accountable, are intellectually open, and are constantly learning and growing.
B Open & Inclusive - Our diverse composition gives us broad and varied perspectives that drive better investments. We find ways to better ourselves and our communities, increasing transparency, fairness, and respect in every interaction. We thrive on the unique qualities of our people, and how together, these qualities make us special.
B Collaborative - We believe in we vs I, and operate as one global team. We know that no one person has all the answers, and that we are better together. Our successes and failures are equally shared.
B Bold - We take risks and understand that at times we may fail. We learn from our failures; we don’t repeat them and are constantly striving to be better.
B Humble - We are humble and believe in winning together with gratitude, knowing that every finish line is the beginning of a new race. We are low ego, and lift each other up.
B Persistent - When we get knocked down, we rise back up. We persevere, with the enduring perspective that only grit can help us overcome. We know our individual and collective goals and won’t stop short of achieving them.
B Evolving - We innovate and advocate with boundless curiosity and creativity. We always have a startup mentality.

Salary Range for NY & CA Candidates only. The actual salary will commensurate according to relevant experience.

Salary expected range: $80,000 - $110,000
Show more "
3574441894,Software/Data Engineer (Entry-level),B Capital,2023-04-03,https://www.linkedin.com/jobs/view/software-data-engineer-entry-level-at-b-capital-3574441894?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=Bc%2FEaT8bhFa5rGH%2Bh%2FCsCg%3D%3D&position=23&pageNum=0&trk=public_jobs_jserp-result_search-card,4688,"Position Summary

The position will focus on helping build and maintain backend integrations of our various systems (e.g., Financial System, HR System, CRM System), including Data Warehouse archiving and synchronizing. The position will also work with our Business Intelligence team to develop views/queries/tables to power Tableau Dashboards used throughout various venture capital investment lifecycle stages. Secondarily, the position will also maintain custom internal applications utilizing data from our Data Warehouse and various systems.

Basic Job Responsibilities

Implementing processes that are fault-tolerant, resilient, and efficient in Databricks and AWS Lambda
Writing clean code that is maintainable and easy to understand
Participate in code reviews
Maintain custom application plugins in WordPress
Update our project management system, JIRA, with notes, questions, and feedback on current and future tasks

Basic Job Requirements

Recent Graduate with a degree or related studies in computer science, math, or other technical field (e.g., applied mathematics, statistics, physics or engineering)
Prior internship experience in similar field will be highly preferred
Experience with programming languages such as Python or PHP
Experience with SQL databases such as MySQL or PostgreSQL
Experience with a source control system, such as Git
Optional experience with Databricks, WordPress, JIRA, Cloud Computing, such as AWS Lambda/S3
Ability to working in a variety of code bases and production environments.
Good problem-solving skills and the ability to think of solutions.
Good verbal/writing skills for explaining your work to others.
Passion to grow in the role to take on AI/ML projects
Curiosity for technology and a desire to learn new skills, frameworks, and programming languages.
Interest in learning financial terminology and concepts used in the Venture Capital Market
We offer hybrid work, where most of the time will be done remote, although you are required to go into the office once a week or so
This role can be based in Los Angeles, New York and San Francisco

At B Capital, the health and safety of our people is our number one priority. As a condition of employment all new hires are required to be fully vaccinated against Covid-19 and must show proof of such vaccination.

About b Capital

B Capital is a multi-stage global investment firm that partners with extraordinary entrepreneurs to shape the future through technology. With $6.3 billion in assets under management across multiple funds, the firm focuses on seed to late-stage venture growth investments, primarily in the enterprise, financial technology and healthcare sectors. Founded in 2015, B Capital leverages an integrated team across eight locations in the US and Asia, as well as a strategic partnership with BCG, to provide the value-added support entrepreneurs need to scale fast and efficiently, expand into new markets and build exceptional companies. For more information, click here.

b Capital Group Core Values

B Honest & Trustworthy - Our people and our culture are the heart of our business. We are self-aware, supportive, and trust ourselves and each other. We speak the truth with positive intent. We hold ourselves accountable, are intellectually open, and are constantly learning and growing.
B Open & Inclusive - Our diverse composition gives us broad and varied perspectives that drive better investments. We find ways to better ourselves and our communities, increasing transparency, fairness, and respect in every interaction. We thrive on the unique qualities of our people, and how together, these qualities make us special.
B Collaborative - We believe in we vs I, and operate as one global team. We know that no one person has all the answers, and that we are better together. Our successes and failures are equally shared.
B Bold - We take risks and understand that at times we may fail. We learn from our failures; we don’t repeat them and are constantly striving to be better.
B Humble - We are humble and believe in winning together with gratitude, knowing that every finish line is the beginning of a new race. We are low ego, and lift each other up.
B Persistent - When we get knocked down, we rise back up. We persevere, with the enduring perspective that only grit can help us overcome. We know our individual and collective goals and won’t stop short of achieving them.
B Evolving - We innovate and advocate with boundless curiosity and creativity. We always have a startup mentality.

Salary Range for NY & CA Candidates only. The actual salary will commensurate according to relevant experience.

Salary expected range: $80,000 - $110,000
Show more "
3554369037,Data Engineer/Data Analyst,Software Technology Inc.,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-data-analyst-at-software-technology-inc-3554369037?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=lOHvJkv2CrJY7LVilT0TCg%3D%3D&position=24&pageNum=0&trk=public_jobs_jserp-result_search-card,353,"Hi,

Hope you are doing well,

We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.

Role : Data Engineer/Data Analyst

Location : Remote

Skill

GCP Big Query, Python, SQL and knowledge of Healthcare domain
Show more "
3570500828,Data Engineer,Care.com,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3570500828?refId=xrGJB2DtJqp%2BzeAXg1JJcQ%3D%3D&trackingId=%2F8qesxmfpAruUPkfuLflDA%3D%3D&position=25&pageNum=0&trk=public_jobs_jserp-result_search-card,5586,"About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Care.com offers an array of services that enable families to find, manage and pay for care and provide employment opportunities for caregivers. Our engineering organization is reimagining our tech stacks and consolidating them to a single cloud-native platform and a cloud-based Data Lake/Data Warehouse on Snowflake.

Here, entrepreneurs, self-starters, great teammates, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI and the latest technologies to solve universal problems and connect people in new ways. If you enjoy solving big problems and building new things, and if you're all about using your talent for good, this is the place for you.

Office Locations: (This is a hybrid position)

NY, NY 10011
Austin, TX 78746
Shelton, CT 06484


What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:

Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
Identify data sources used for building out data architecture diagrams/models
Establish engineering practices and setup frameworks for ""Data as a Service""
Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
Solve code level problems quickly and efficiently
Participate in demos and code reviews
Promote software best approach, standards, and processes
Shape development processes to promote a high-quality output while continuing to iterate quickly
Incorporate best practices for security, performance, and data privacy into data pipelines


What You'll Need to Succeed:

BS or MS in Computer Science or relevant engineering experience
4+ years work experience in Data Engineering/ETL
3+ years SQL experience preferred
2+ years traditional RDBMS experience (Oracle & Postgres experience preferred)
1+ years Unix/batch scripting preferred
1+ years Python experience is a plus
1+ years Windows server admin experience is a plus
Experience interfacing with business teams and turning requirements and vision into a technical reality
Ability to drive efforts from start to finish as a self-motivator
Knowledge in Data Warehousing is a MUST
Proven ability to maintain performance level in a fast-paced agile environment
Pragmatic and realistic with solutions


For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please contact talent@care.com.**


____________________________________________________________________________________________________________________________

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: $116,000 to $145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).
Show more "
3576704963,Data Engineer (Python/Pyspark),Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-python-pyspark-at-diverse-lynx-3576704963?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=4veuAFZuW8mNhHX8XZbyjw%3D%3D&position=1&pageNum=1&trk=public_jobs_jserp-result_search-card,1529,"Job Description




Role: Data Engineer (Python/Pyspark)




Location: Carlsbad, CA




Note: We need resumes of the candidates who have worked on data integration/building pipelines using Python and/or PySpark as programming language ideally on Databircks platform.




Description




Design, develop, test, deploy, support, enhance data integration solutions seamlessly to connect and integrate enterprise systems in our Enterprise Data Platform.
Innovate for data integration in Apache Spark-based Platform to ensure the technology solutions leverage cutting edge integration capabilities.
Experience with ETL, data pipeline creation to load data from multiple data sources.




Primary Skills




4&plus; years working experience in data integration and pipeline development with BS degree in CS, CE or EE.
2&plus; years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems
Strong real-life experience in python development especially in PySpark in AWS Cloud environment.
Design, develop test, deploy, maintain and improve data integration pipeline.
Experience in Python and common python libraries.
Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.
Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.
Databricks, Redshift Experience is a plus.
Show more "
3554369037,Data Engineer/Data Analyst,Software Technology Inc.,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-data-analyst-at-software-technology-inc-3554369037?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=0zfDDo7HTwqvkoSAmSN1wQ%3D%3D&position=2&pageNum=1&trk=public_jobs_jserp-result_search-card,353,"Hi,

Hope you are doing well,

We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.

Role : Data Engineer/Data Analyst

Location : Remote

Skill

GCP Big Query, Python, SQL and knowledge of Healthcare domain
Show more "
3580192853,Data Engineer,Extend Information Systems Inc.,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-extend-information-systems-inc-3580192853?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=DEQPWhXtfSf%2BDFND1pk35Q%3D%3D&position=3&pageNum=1&trk=public_jobs_jserp-result_search-card,4361,"Hi Jobseekers,

I hope you are doing well!

We have an opportunity Data Engineer / Architect with one of our clients for Houston TX.

Please see the job details below and let me know if you would be interested in this role.

If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.

Job Title: Data Engineer / Architect

Location: Houston TX

Full Time

Relevant Experience(in Yrs): 8+

Technical/Functional Skills

The Data Architect/Sr Data Engineer will be responsible for developing Python modules using NumPy multi-dimensional arrays, pandas, and dynamic programming in AWS and Snowflake and expanding and optimizing our ETL and data pipeline architecture and data flow and collection across our Business Portfolio.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Developer will support our data product owners, developers, data architects, data analysts, and data scientists on BI and Analytic initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
The Data Developer will also assist in issue resolution, job orchestration, automation, and continuous improvement of our data integration processes.

Experience Required

The Data Architect/Sr Data Engineer will be responsible for developing Python modules using NumPy multi-dimensional arrays, pandas, and dynamic programming in AWS and Snowflake and expanding and optimizing our ETL and data pipeline architecture and data flow and collection across our Business Portfolio.

Roles & Responsibilities

Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Cloud Integration ETL Tools, Cloud Data Warehouse, SQL, DBT, AWS Services Lambda, SNS, SQS, Event Bridge, and IoT core.
Design and develop ELT, ETL, and Event-driven data integration architecture solutions
Work with the Data Analysts, Data Architects, BI Architects, Data Scientists, and Data Product Owners to establish an understanding of source data and determine data transformation and integration requirements
Troubleshoot and tune complex SQL
Utilize On-Prem and Cloud-based ETL platforms, Cloud Data warehouse, AWS, GitHub, various scripting languages, SQL, querying, data quality, and metadata management tools.
Develop data validation processes to ensure data quality
Demonstrated ability to work individually and as a part of the team in a collaborative manner

Qualifications

The requirements listed below are representative of the qualifications necessary to perform the job.
Education and Experience
Bachelor's degree (or foreign equivalent) in Computer Science, Computer Engineering, or a related field.
8+ years of experience with Data Engineering, ETL, data warehouse/data mart development, data lake development
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with Cloud Datawarehouse like Snowflake, Google Big Query, Amazon Redshift
Experience with AWS cloud services: EC2, S3, Lambda, SQS, SNS, etc.
Experience with Cloud Integration Tools like Matillion, Dell Boomi, Informatica Cloud, Talend, AWS Glue
Experience with GitHub and its integration with the ETL tools for version control
Experience with Informatica PowerCenter, various scripting languages, SQL, and querying tools
Familiarity with modern data management tools and platforms including Spark, Hadoop/Hive, NoSQL, APIs, Streaming, and other analytic data platforms
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc., a plus.
Experience with Agile/Scrum is valuable

Thanks & Regards

Anoop

Extend Information Systems

Cell: - 571 - 386 - 2431

Email: Anoop@extendinfosys.com

Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166

Web: WWW.extendinfosys.com
Show more "
3574523524,Bigdata Engineer,Zortech Solutions,2023-04-20,https://www.linkedin.com/jobs/view/bigdata-engineer-at-zortech-solutions-3574523524?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=FI8PByWN%2FDiFFQ4MS1GM3A%3D%3D&position=4&pageNum=1&trk=public_jobs_jserp-result_search-card,259,"BNYM

FTE/W2

Candidates needs to be on vendor payroll no layers

Pittsburgh, PA

Big Data Architect Hadoop, Hive, Scala, Spark,PySPark. Airflow and Kafka exp preferred. CloudEra exp preferred
HIve,Pyspark,python,Impala
Hadoop architecture
Sql
kudu
Show more "
3574445416,Software/Data Engineer (Entry-level),B Capital,2023-04-03,https://www.linkedin.com/jobs/view/software-data-engineer-entry-level-at-b-capital-3574445416?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=4YLwdQ%2Fw217zRtBVTvVfVw%3D%3D&position=5&pageNum=1&trk=public_jobs_jserp-result_search-card,4688,"Position Summary

The position will focus on helping build and maintain backend integrations of our various systems (e.g., Financial System, HR System, CRM System), including Data Warehouse archiving and synchronizing. The position will also work with our Business Intelligence team to develop views/queries/tables to power Tableau Dashboards used throughout various venture capital investment lifecycle stages. Secondarily, the position will also maintain custom internal applications utilizing data from our Data Warehouse and various systems.

Basic Job Responsibilities

Implementing processes that are fault-tolerant, resilient, and efficient in Databricks and AWS Lambda
Writing clean code that is maintainable and easy to understand
Participate in code reviews
Maintain custom application plugins in WordPress
Update our project management system, JIRA, with notes, questions, and feedback on current and future tasks

Basic Job Requirements

Recent Graduate with a degree or related studies in computer science, math, or other technical field (e.g., applied mathematics, statistics, physics or engineering)
Prior internship experience in similar field will be highly preferred
Experience with programming languages such as Python or PHP
Experience with SQL databases such as MySQL or PostgreSQL
Experience with a source control system, such as Git
Optional experience with Databricks, WordPress, JIRA, Cloud Computing, such as AWS Lambda/S3
Ability to working in a variety of code bases and production environments.
Good problem-solving skills and the ability to think of solutions.
Good verbal/writing skills for explaining your work to others.
Passion to grow in the role to take on AI/ML projects
Curiosity for technology and a desire to learn new skills, frameworks, and programming languages.
Interest in learning financial terminology and concepts used in the Venture Capital Market
We offer hybrid work, where most of the time will be done remote, although you are required to go into the office once a week or so
This role can be based in Los Angeles, New York and San Francisco

At B Capital, the health and safety of our people is our number one priority. As a condition of employment all new hires are required to be fully vaccinated against Covid-19 and must show proof of such vaccination.

About b Capital

B Capital is a multi-stage global investment firm that partners with extraordinary entrepreneurs to shape the future through technology. With $6.3 billion in assets under management across multiple funds, the firm focuses on seed to late-stage venture growth investments, primarily in the enterprise, financial technology and healthcare sectors. Founded in 2015, B Capital leverages an integrated team across eight locations in the US and Asia, as well as a strategic partnership with BCG, to provide the value-added support entrepreneurs need to scale fast and efficiently, expand into new markets and build exceptional companies. For more information, click here.

b Capital Group Core Values

B Honest & Trustworthy - Our people and our culture are the heart of our business. We are self-aware, supportive, and trust ourselves and each other. We speak the truth with positive intent. We hold ourselves accountable, are intellectually open, and are constantly learning and growing.
B Open & Inclusive - Our diverse composition gives us broad and varied perspectives that drive better investments. We find ways to better ourselves and our communities, increasing transparency, fairness, and respect in every interaction. We thrive on the unique qualities of our people, and how together, these qualities make us special.
B Collaborative - We believe in we vs I, and operate as one global team. We know that no one person has all the answers, and that we are better together. Our successes and failures are equally shared.
B Bold - We take risks and understand that at times we may fail. We learn from our failures; we don’t repeat them and are constantly striving to be better.
B Humble - We are humble and believe in winning together with gratitude, knowing that every finish line is the beginning of a new race. We are low ego, and lift each other up.
B Persistent - When we get knocked down, we rise back up. We persevere, with the enduring perspective that only grit can help us overcome. We know our individual and collective goals and won’t stop short of achieving them.
B Evolving - We innovate and advocate with boundless curiosity and creativity. We always have a startup mentality.

Salary Range for NY & CA Candidates only. The actual salary will commensurate according to relevant experience.

Salary expected range: $80,000 - $110,000
Show more "
3555470438,Data Engineer,Aptos Retail,2023-04-05,https://www.linkedin.com/jobs/view/data-engineer-at-aptos-retail-3555470438?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=oAjsIkDcpwW7W%2BsfYACJOQ%3D%3D&position=6&pageNum=1&trk=public_jobs_jserp-result_search-card,2472,"Revionics guides retailers on the lifecycle pricing journey with leading AI solutions for pricing, promotions, and markdowns. We provide our customers with clarity and confidence to make optimal pricing decisions for powerful results. Retailers in all segments across the world adopt our solutions to improve top-line sales, demand, and margin.

Revionics is investing in their Data Platform and is growing the team. This role would be a junior role in the team that is responsible for driving the overall system design, architecture, scalability, reliability, and performance of end-to-end data systems.

Who you are:

Bachelor's degree in Computer Science, Computer Engineering, CIS/MIS, or a related field
You have 1+ years of development experience with one or more of the following, or another similar language: Python/Scala (or similar)
Some exposure to different non-relational (MongoDB or similar) and/or relational technologies (SQL)
Exposure to orchestration tools (e.g. Luigi, Airflow, Kubeflow etc.)
Interested in big data technologies likes Spark
Exposure to cloud platforms (AWS/GCP) a plus
Able to communicate, collaborate, and work effectively in a distributed team.
Can think about and write high quality code and can demonstrate that capability, be it through job experience, schoolwork, or contributions to community projects.
Solid understanding of software engineering concepts and methodologies
Familiarity with software testing principles
Enjoy tough technical challenges and are naturally intellectually curious


What you'll do:

In this role, you will be defining and developing modular data pipelining solutions to feed the AI/ML systems that power Revionics' pricing products
Work with Product and Science teams to build data products that enable some of the largest retailers in the world to consume and build upon Revionics' AI/ML platform.
Collaborate with data operations to build native observability services that meet the high bar in terms of visibility and precision for AI/ML software.


We also look for

Passion
Initiative and a Pioneering Spirit
Quality orientation
Resourcefulness and application


The pay range for this position is between $75,000 and $95,000 plus Annual; Incentive Bonus. Starting salary may vary based on a number of factors including, but not limited to, the position being offered, location, education, training, and/or experience.

Revionics can support work authorization and visa sponsorship.


Show more "
3577136428,Data Engineer,Converse,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-converse-3577136428?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=OlwwgGUkpY8YRCKJqwHqUg%3D%3D&position=7&pageNum=1&trk=public_jobs_jserp-result_search-card,3780,"Become part of the Converse Team




Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.




Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.




Experience Must Include




Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation.




Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github




Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.







Show more "
3562671866,Data Engineer,PowerReviews,2023-04-12,https://www.linkedin.com/jobs/view/data-engineer-at-powerreviews-3562671866?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=Mcn%2FYXJT9Qvsi2yrSHTEHA%3D%3D&position=8&pageNum=1&trk=public_jobs_jserp-result_search-card,5260,"Who We Are

PowerReviews delivers software that more than 1,000 brands and retailers utilize to collect, display, and syndicate customer ratings and reviews on 5,000+ websites. Now more than ever before, ratings and reviews are an essential resource for consumers as they search and shop online and in-store. Reviews drive traffic, increase sales, and create actionable insights to improve products and services for our clients. With a syndication network that reaches more than 500 million in-market shoppers each month, PowerReviews delivers more reviews to more consumers. And we make it easier than anyone else.

About The Role

At PowerReviews, we leverage data engineering and machine learning techniques to power automated decisions around our data and publications processes and to help deliver key insights to customers and internal stakeholders. We are looking for a Data Engineer to join our talented engineering team to help support our current data pipelines, platform and solutions as well as unlock new capabilities and constantly look for ways to innovate and improve.

This role will be on an integrated application team where you will be surrounded by talented engineers, data scientists, analysts and a product manager. Collectively the team will prioritize, design and deliver solutions that are both customer facing and for internal stakeholders.

Our Tech Stack

At PowerReviews we use lots of open source software and use Amazon Web Services (almost) exclusively. Our current stack consists of Java, Spring-Boot, Python, CircleCi, PostgreSQL, Neo4J, Snowflake, ElasticSearch, Sumologic/DataDog React.js, Docker, and Ruby on Rails. In AWS we make use of EC2, DynamoDB, RDS, Redshift, S3, Elasticache, Elastic Map Reduce, Lambda, Cloudfront, and others.

What You Will Do

Participate in product and engineering discussions, influence the roadmap and user experience, take ownership and responsibility over new projects and features and turn those ideas into a reality
Design, develop, and support existing and new data pipelines leveraging cloud-based tools (SageMaker, ECS, Glue, DMS, EMR, Lambdas, Snowflake, etc.) to scalably and efficiently process data and a variety of cloud based tooling to supply any given needed metrics (Tableau, DataDog, CloudWatch, etc)
Research, design, and implement strategic automation to assist in big data processing, machine learning predictions, or other customer needs.
Build and improve automation tooling around machine learning and data engineering including (CircleCI, SageMaker, EMR, SnowFlake, DBT, etc)
Become an integral part of a team participating in agile processes, peer review, planning, testing, etc.


What You Bring

3+ years of experience building systems that collect and process data in analytics and/or software development in an agile environment
Primary programming experience in a JVM Language (Java, Kotlin, Scala) and/or Python
Experience building, deploying too and supporting cloud solutions in AWS (preferred), GCP, Azure, etc.
Experience building software leveraging big data and/or high traffic and are knowledgeable about data techniques and processes (ETLs, DBT, AWS Glue, Apache Spark, Apache Flume, AWS Athena Pandas, Spacy, etc)
Experience with business intelligence platforms such as Tableau, Looker, PowerBI, AWS QuickSight and the data warehouse's that power them (AWS Redshift, Google BigQuery, SnowFlake, etc)
Experience with the use of SQL or NoSQL for ad hoc analysis.
Excellent communication and effectively communicate with team members as well as non-technical stakeholders and consumers of your work
You have with source tools a such as Git
You have experience with containerization of applications (Docker, ECS, Docker Swarm, Kubernetes, etc)


OUR VALUES

PowerValues represent who we are, what we stand for and how we work with each other and our customers. They are: Accountability, Communication, Collaboration, Continuous Improvement and Customer Focus.

What We Offer

Remote Friendly: Do your best work from our downtown Chicago office or from home; it's your choice. In addition to Illinois, you could also live in AL, AZ, CT, FL, GA, IN, KS, MA, MI, MO, NC, NJ, NY, OH, PA, SC, TN, TX, WI.
Real Career Growth Opportunities – At PowerReviews, you have the opportunity to learn a lot and progress quickly. We love to promote our PowerPeople and filled over 30% of our open roles with internal applicants in the past year.
Work With Nice People: At every level - warm, friendly, collaborative, humble.
Work-Life Balance: We believe in taking the time you need, when you need it with unlimited PTO, 10 Company Holidays, Paid Parental Leave, extra bonus days off at the holidays, Summer Fridays
Great Benefits: We offer affordable, comprehensive medical, dental and vision coverage plus many additional benefits to best fit the needs of you and your family.
Company-Matched 401(k)


PowerReviews is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.
Show more "
3568582712,Data Engineer,Elevance Health,2023-04-03,https://www.linkedin.com/jobs/view/data-engineer-at-elevance-health-3568582712?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=HxacyRtjPzntZ%2FYJzlHV3A%3D%3D&position=9&pageNum=1&trk=public_jobs_jserp-result_search-card,4727,"Description




Responsible for end-to-end application, system development and maintenance on medium or semi complex technology platforms within assigned client group, business unit or corporate department.




Writes correct and clean code with limited guidance while ensuring following best practices.
Partners with internal and/or external stakeholder to better understand business requirements.
Analyzes and classifies semi-complex change request and identifies and documents possible system code enhancements.
Participates in developing application scope and objectives and preparing functional and/or technical specifications.
Partner with designers and other web team members to better understand and apply the right marketing focused content.
Codes and maintains semi-complex components of information systems.
Develops and performs system testing and fixes defects identified during testing and re-executes unit tests to validate results.
Understand testing best practices.
Participates in integrating activities with other IT departments for successful implementation and support of project efforts.
Develops a general understanding of how the work fits in to the larger project and identifies problems or issues in requirements.
Provides on call support and monitors the system for semi-complex issues as needed.
Facilitates small group design sessions or code reviews and participates in vendor evaluation.




Minimum Requirements




BA/BS degree in Information Technology, Computer Science or related field of study and a minimum of 1 year experience on one platform; or any combination of education and experience, which would provide an equivalent background.




Preferred Qualifications




Proficient AWS, SnowFlake, EMR, GLUE.
Proficient with Python or R, both preferred.
Proficient with Java, Microservices.
Proficient with Teradata.




Please be advised that Elevance Health only accepts resumes from agencies that have a signed agreement with Elevance Health. Accordingly, Elevance Health is not obligated to pay referral fees to any agency that is not a party to an agreement with Elevance Health. Thus, any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of Elevance Health.




Be part of an Extraordinary Team




Elevance Health is a health company dedicated to improving lives and communities – and making healthcare simpler. A Fortune 20 company with a longstanding history in the healthcare industry, we are looking for leaders at all levels of the organization who are passionate about making an impact on our members and the communities we serve. You will thrive in a complex and collaborative environment where you take action and ownership to solve problems and lead change. Do you want to be part of a larger purpose and an evolving, high-performance culture that empowers you to make an impact?




We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) +match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few.




The health of our associates and communities is a top priority for Elevance Health. We require all new candidates in certain patient/member-facing roles to become vaccinated against COVID-19. If you are not vaccinated, your offer will be rescinded unless you provide – and Elevance Health approves – a valid religious or medical explanation as to why you are not able to get vaccinated that Elevance Health is able to reasonably accommodate. Elevance Health will also follow all relevant federal, state and local laws.




Elevance Health has been named as a Fortune Great Place To Work in 2022, has been ranked for five years running as one of the 2023 World’s Most Admired Companies by Fortune magazine, and is a growing Top 20 Fortune 500 Company. To learn more about our company and apply, please visit us at careers.ElevanceHealth.com. Elevance Health is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to age, citizenship status, color, creed, disability, ethnicity, genetic information, gender (including gender identity and gender expression), marital status, national origin, race, religion, sex, sexual orientation, veteran status or any other status or condition protected by applicable federal, state, or local laws. Applicants who require accommodation to participate in the job application process may contact ability@icareerhelp.com for assistance.
Show more "
3571631891,Data Engineer,"Akraya, Inc.",2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-akraya-inc-3571631891?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=EPO9APBWGLhD%2FNKpluNHzw%3D%3D&position=10&pageNum=1&trk=public_jobs_jserp-result_search-card,1255,"Primary Skills: Data pipelining, Python (Panda), SQL, Spark

Duration – 6 months CTH

Contract Type: W2

Location – San Mateo, CA (Hybrid preferred / remote ok (1st preference local to San Mateo / Bay Area or alternately CST time zone)

To follow up with any questions, please contact Clyde at 408-907-3201

Grow your skills by working with the best in the industry

Job Responsibilities

Strong experience with Python
Not looking for Pyspark proficiency but must be good in pure Python programming
Must have experience writing python using panda libraries
Also required is Spark and SQL. But if they have Python and SQL expertise Spark could be optional
Solid experience building end to end pipelines in Python and enhancing existing pipelines
Will be responsible to use various libraries and build data sets

Job Requirements

Python programming
SQL skills
Good data warehousing / data engineering /data pipelining background

About Akraya

Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. Akraya was recently voted as a 2021 Best Staffing Firm to Temp for by Staffing Industry Analysts and voted by our employees and consultants as a 2022 Glassdoor Best Places to Work.
Show more "
3573168199,Data Engineer,Christensen Farms,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-at-christensen-farms-3573168199?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=IfUvmhdJw%2BOzHwHxAojtyQ%3D%3D&position=11&pageNum=1&trk=public_jobs_jserp-result_search-card,5532,"Christensen Farms is one of the largest, family-owned pork producers in the United States, marketing approximately 3 million hogs per year. Headquartered in Sleepy Eye, Minnesota, the company operates throughout the Midwest with facilities in Minnesota, Iowa, Nebraska, Illinois, and South Dakota. Christensen Farms owns four feed mills, manages 145,000 sows on 44 farms, and oversees more than 350 nurseries and grow-finish sites. The company employees nearly 1,000 people and maintains 1,500 contract partnerships.




The company is vertically integrated with a strong presence across the pork value chain - from farm to fork. Christensen Farms is the largest shareholder of Triumph Foods LLC, a producer-owned primary pork processing plant in St. Joseph, Missouri. In turn, Triumph Food members own 50 percent of Daily's Premium Meats, a specialty pork processor bacon and other premium pork products. Triumph Foods also holds a 50 percent partnership in Seaboard Triumph Foods, LLC of Sioux City, Iowa, a primary pork processing plant.




Position Overview & Responsibilities




Christensen Farms is looking for a self-motivated Data Engineer to join our Decision Science team. In this position, you will play a significant role in helping grow and modernize our analytics services. As part of the company's data operations, the data engineer will ensure accuracy in decision science and reporting as well as responsibility for collecting, managing, and converting raw data into information that can be interpreted by the business. You will design and develop data collection frameworks for structured and unstructured data including Data Pipelines, ELTs and ETLs to connect large datasets from a variety of sources. You will also engineer reports, dashboards, and visualizations using enterprise business intelligence tools.




This individual will need effective communication skills, communicating up, down and across the organization. This individual will also require the skill to multitask and manage simultaneous project activities that require innovative problem solving. Our ultimate goal is data accessibility to enable the organization to leverage data for performance evaluation and optimization.




Major Areas of Responsibility




Engineer and implement data models for dimensions and facts within the staging and warehouse layer of our enterprise data lake house.
Engineer and implement new and modify existing ETLs, data pipelines, and ELTs to move data from a variety of source systems, including structured and unstructured data, to fit into dimensional data models with large data sets.
Engineer schedules and orchestrations for batch and near real time data loads into the enterprise data lake house.
Troubleshoots, identifies issues and remediates data performance issues that come with developing, querying, and combining large data sets from a variety of sources.
Collaborate with a variety of business users to understand definitions of data, analytics use cases, and assist in building data governance and detailed data requirements.
Support and establish reports, dashboards, and visualizations using Power BI.
Responsible for executing simultaneous project activities that require innovative problem solving.
Provides 2nd level employee support for decision science services.
Research tools, disciplines and industry trends and recommend improvements for team or company consideration.
Other duties assigned within the scope, responsibility, and requirements of the job.




Education, Training And Experience Requirements




Minimum formal education required: Associates/Bachelor's degree in data and analytics or an IT related discipline or 4+ years of equivalent work experience.
2 or more years of experience in a corporate IT Team environment.
Exceptional technical skills required. Cloud based tools (examples include Power BI, Data Factory, Databricks, Snowflake, H20.ai, Excel, database management, etc.).
2-3+ years Programming/scripting experience (SQL and/or Python) and working knowledge of the software development life cycle.
2+ years of experience engineering within a data warehouse or related experience with dimensional data modeling.
2+ years of experience designing and developing ETLs with tools like Microsoft SSIS
Proven ability to gather detailed technical requirements to design and develop business intelligence report solutions from beginning to end.
Clear, concise, and precise written and verbal communication skills to effectively communicate with people of various backgrounds including non-technical audiences.
Ability to multi-task and to prioritize objectives, anticipate situations and take quick action.
Computer skills required: Working knowledge of Microsoft Office Suite including Visio
Strict adherence to company confidentiality and ethical standards.




Additional Desirable Skills/Qualifications




Certifications in any of the following are a plus: DASCA (Associate or Senior), IBM Certified Solution Architect, SAS Certified Big Data Professional, ITIL.
Enterprise application experience
Cloud technologies (Public, Private, Dedicated, Hybrid).
Experience in Level 2 and/or Level 3 employee support (Freshservice or similar ITSM tool).
Employee computing devices including Windows, Android, and iOS.
Experience working in an agricultural business, particularly the swine industry.
Hybrid work schedule at our Sleepy Eye Office Preferred, remote opportunities potentially available for the right candidate.




Job Posted by ApplicantPro
Show more "
3576547108,Data Engineer fully remote,Diverse Lynx,2023-03-25,https://ca.linkedin.com/jobs/view/data-engineer-fully-remote-at-diverse-lynx-3576547108?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=LH90m6Ye%2FC7yDwNbpafm%2Fw%3D%3D&position=12&pageNum=1&trk=public_jobs_jserp-result_search-card,1780,"Job Description




Job title Data Engineer




Location Fulltime Remote




Job type: Fulltime permanent position only




Role Description




We are looking for a Data Engineer to be part of our scrum teams and perform functional system development for Hadoop applications for our Enterprise Data Lake initiative




They should also have experience in data science




This is high visibility fast paced key initiative will integrate data across internal and external sources provide analytical insights and integrate with our critical systems




Primary Skills




Participate in the agile development process Develop functional and technical specifications from business requirements for the commercial platform




Ensure application quality and adherence to performance requirements Help create project estimates and plans




Represent engineering team in project meetings and solution discussions




Participate code review process




Work with team members to achieve business results in a fast paced and quickly changing environment Pair up with data engineers to develop cutting edge Analytic applications leveraging




Big Data technologies Hadoop NoSQL and in memory




Data Grids Mentor and influence up and down the chain of command perform other duties and or projects as assigned




Should be experienced with Data Science concepts




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3576976734,Data Engineer,"Wawa, Inc.",2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-wawa-inc-3576976734?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=8fTOmWVe2Ou%2FMmhLTYxD1g%3D%3D&position=13&pageNum=1&trk=public_jobs_jserp-result_search-card,6070,"Job Description




Job Title: Data Engineer Location: Corporate




Department: Information Technology




Pay Band: Specialist




Job Summary:




The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.




Principal Duties:




Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.




Essential Functions:




Handle multiple priorities simultaneously
Work collaboratively with cross-functional teams
Establish and maintain a working environment conducive to positive morale, individual style, quality, creativity, and teamwork
Ability to build strong trusting relationships with business partners
Passion for innovation and “can do” attitude to thrive in a fast paced environment
Ability to work in a fast-paced, team environment
Excellent communication skills
Basic project management skills required
Work with the team to lead and maintain data strategy standards in all the data team is responsible for.




Basic Qualifications:




Bachelor’s degree in Computer Science/Engineering preferred
5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.




Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center at asc@wawa.com or 1-800-444-9292.




Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Show more "
3569984170,Data Analytics Engineer,Danta Technologies,2023-04-19,https://www.linkedin.com/jobs/view/data-analytics-engineer-at-danta-technologies-3569984170?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=9Y3IM6YAzPiiOBs3eVOWbA%3D%3D&position=14&pageNum=1&trk=public_jobs_jserp-result_search-card,78,"Skills Req.




Use Tableau to create visualizations , Use SQL to build tables"
3576829777,Data Engineer,Revecore,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-revecore-3576829777?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=lOurdazchNJNPzTdyRhwLA%3D%3D&position=15&pageNum=1&trk=public_jobs_jserp-result_search-card,3503,"About Revecore




Revecore is an innovative, technology-driven company that is committed to helping our clients, our employees, our company, and our communities thrive.




With a 20+ year history, Revecore is the leading provider of revenue integrity and complex claims solutions for hospitals.




We offer a dynamic and flexible work environment, full of opportunity for motivated, hands-on team players. We strive each day to solve complex business problems and find new ways to enhance the efficiency, effectiveness, and quality of our services. If those attributes resonate with you, regardless of where you are located—we want you on our team!




Purpose of this role




Revecore is embarking on re-architecting and modernizing its core platform. The Data Technology at Revecore is empowering the business in unlocking new opportunities by integrating data and machine learning across our products and business functions. This team is composed of Data Engineering, Analytics Engineering, Data Science and Machine Learning (ML) Engineering specializations.




The Data Engineer will join the Technology function, solving complex data problems for the Revecore Data Platform. The ideal candidate part of the Data Engineering team contributing to platform, architecture, and automation best practices. You are expected to pave the path for the data teams around data ingestion, data pipelines and data operations. You will be responsible for building a modern data lakehouse platform that will transform Revecore and its capabilities.




What You'll Do




Accountable for the implementation and quality of the data platforms (availability, reliability, resilience, security, etc.) and adherence to consistent standards for engineering excellence.
Contribute to engineering best practices at Revecore, and the quality of the domain’s output.
Design and develop scalable, maintainable, and reliable data pipelines.
Work directly with other Data Engineers to understand their needs and develop tooling and automation to improve developer efficiency.
Monitor data systems and performance and look for ways to drive down our warehouse and infrastructure costs.
Proactively research and understand industry trends on various components of the data platform.




Technical Requirements




Our stack is Azure Synapse (Spark and SQL), dbt, Airflow, Azure ML Studio, MLFlow, Kubernetes, Docker, and Python. Expert experience with the above is desired, but a desire to learn is a must.
Experience with Azure Cloud is required.
Hands-on experience with data engineering, data warehousing, data lakes, ELT process capabilities on Azure using Azure services.
Expertise in Python, .NET & SQL is required. Deep knowledge of .NET Framework and .NET Core
A strong understanding of the data SDLC and CI/CD best practices for a data platform.
Bachelor’s or Master’s degree in computer science or equivalent experience.




Revecore values a diverse workplace and strongly encourages women, people of color, LGBT individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply.




Revecore is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state or federal laws.
Show more "
3568588094,Data Engineer,Elevance Health,2023-04-03,https://www.linkedin.com/jobs/view/data-engineer-at-elevance-health-3568588094?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=OLXn2GLvwl5KGXJTV9eYyg%3D%3D&position=16&pageNum=1&trk=public_jobs_jserp-result_search-card,4727,"Description




Responsible for end-to-end application, system development and maintenance on medium or semi complex technology platforms within assigned client group, business unit or corporate department.




Writes correct and clean code with limited guidance while ensuring following best practices.
Partners with internal and/or external stakeholder to better understand business requirements.
Analyzes and classifies semi-complex change request and identifies and documents possible system code enhancements.
Participates in developing application scope and objectives and preparing functional and/or technical specifications.
Partner with designers and other web team members to better understand and apply the right marketing focused content.
Codes and maintains semi-complex components of information systems.
Develops and performs system testing and fixes defects identified during testing and re-executes unit tests to validate results.
Understand testing best practices.
Participates in integrating activities with other IT departments for successful implementation and support of project efforts.
Develops a general understanding of how the work fits in to the larger project and identifies problems or issues in requirements.
Provides on call support and monitors the system for semi-complex issues as needed.
Facilitates small group design sessions or code reviews and participates in vendor evaluation.




Minimum Requirements




BA/BS degree in Information Technology, Computer Science or related field of study and a minimum of 1 year experience on one platform; or any combination of education and experience, which would provide an equivalent background.




Preferred Qualifications




Proficient AWS, SnowFlake, EMR, GLUE.
Proficient with Python or R, both preferred.
Proficient with Java, Microservices.
Proficient with Teradata.




Please be advised that Elevance Health only accepts resumes from agencies that have a signed agreement with Elevance Health. Accordingly, Elevance Health is not obligated to pay referral fees to any agency that is not a party to an agreement with Elevance Health. Thus, any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of Elevance Health.




Be part of an Extraordinary Team




Elevance Health is a health company dedicated to improving lives and communities – and making healthcare simpler. A Fortune 20 company with a longstanding history in the healthcare industry, we are looking for leaders at all levels of the organization who are passionate about making an impact on our members and the communities we serve. You will thrive in a complex and collaborative environment where you take action and ownership to solve problems and lead change. Do you want to be part of a larger purpose and an evolving, high-performance culture that empowers you to make an impact?




We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) +match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few.




The health of our associates and communities is a top priority for Elevance Health. We require all new candidates in certain patient/member-facing roles to become vaccinated against COVID-19. If you are not vaccinated, your offer will be rescinded unless you provide – and Elevance Health approves – a valid religious or medical explanation as to why you are not able to get vaccinated that Elevance Health is able to reasonably accommodate. Elevance Health will also follow all relevant federal, state and local laws.




Elevance Health has been named as a Fortune Great Place To Work in 2022, has been ranked for five years running as one of the 2023 World’s Most Admired Companies by Fortune magazine, and is a growing Top 20 Fortune 500 Company. To learn more about our company and apply, please visit us at careers.ElevanceHealth.com. Elevance Health is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to age, citizenship status, color, creed, disability, ethnicity, genetic information, gender (including gender identity and gender expression), marital status, national origin, race, religion, sex, sexual orientation, veteran status or any other status or condition protected by applicable federal, state, or local laws. Applicants who require accommodation to participate in the job application process may contact ability@icareerhelp.com for assistance.
Show more "
3544976931,Data Engineer,I-Cube Software LLC,2023-03-27,https://ca.linkedin.com/jobs/view/data-engineer-at-i-cube-software-llc-3544976931?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=05yZsUDCvBcmx3UK%2FEOf7A%3D%3D&position=17&pageNum=1&trk=public_jobs_jserp-result_search-card,2306,"Hybrid Hadoop Engineer and Hadoop Infrastructure Administrator to build and maintain a scalable and resilient Big Data framework to support Data Scientists. As an administrator, your responsibility will be to deploy and maintain Hadoop clusters, add and remove nodes using cluster management and monitoring tools like Cloudera Manager, and support performance and scalability requirements, in support of our Data scientist's needs. Some Relational Database administrator experience will also be desirable to support the general administration of Relational Databases. Design, build, and maintain Big Data workflows/pipelines to process a continuous stream of data with experience in end-to-end design and build process of Near-Real-Time and Batch Data Pipelines. Demonstrated work experience in the following with Big Data and distributed programming models and technologies Knowledge of database structures, theories, principles, and practices (both SQL and NoSQL). Active development of ETL processes using Spark or other highly parallel technologies, and implementing ETL/data pipelines Experience with Data technologies and Big Data tools, like Spark, Kafka, Hive Understanding of Map Reduce and other Data Query and Processing and aggregation models Understanding of challenges of transforming data across distributed clustered environment Experience with techniques for consuming, holding, and aging out continuous data streams Ability to provide quick ingestion tools and corresponding access APIs for continuously changing data schema, working closely with Data Engineers around specific transformation and access needs Preferred: Experience as a Database administrator (DBA) will be responsible for keeping critical tools database up and running Building and managing high-availability environments for databases and HDFS systems Familiarity with transaction recovery techniques and DB Backup Skills and Attributes: Ability to have effective working relationships with all functional units of the organization Excellent written, verbal, and presentation skills Excellent interpersonal skills Ability to work as part of a cross-cultural team Self-starter and Self-motivated Ability to work without lots of supervision Works under pressure and is able to manage competing priorities. Hide
Show more "
3570500828,Data Engineer,Care.com,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3570500828?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=X3PV1xbEX9cEvvxfLwSdTg%3D%3D&position=18&pageNum=1&trk=public_jobs_jserp-result_search-card,5586,"About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Care.com offers an array of services that enable families to find, manage and pay for care and provide employment opportunities for caregivers. Our engineering organization is reimagining our tech stacks and consolidating them to a single cloud-native platform and a cloud-based Data Lake/Data Warehouse on Snowflake.

Here, entrepreneurs, self-starters, great teammates, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI and the latest technologies to solve universal problems and connect people in new ways. If you enjoy solving big problems and building new things, and if you're all about using your talent for good, this is the place for you.

Office Locations: (This is a hybrid position)

NY, NY 10011
Austin, TX 78746
Shelton, CT 06484


What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:

Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
Identify data sources used for building out data architecture diagrams/models
Establish engineering practices and setup frameworks for ""Data as a Service""
Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
Solve code level problems quickly and efficiently
Participate in demos and code reviews
Promote software best approach, standards, and processes
Shape development processes to promote a high-quality output while continuing to iterate quickly
Incorporate best practices for security, performance, and data privacy into data pipelines


What You'll Need to Succeed:

BS or MS in Computer Science or relevant engineering experience
4+ years work experience in Data Engineering/ETL
3+ years SQL experience preferred
2+ years traditional RDBMS experience (Oracle & Postgres experience preferred)
1+ years Unix/batch scripting preferred
1+ years Python experience is a plus
1+ years Windows server admin experience is a plus
Experience interfacing with business teams and turning requirements and vision into a technical reality
Ability to drive efforts from start to finish as a self-motivator
Knowledge in Data Warehousing is a MUST
Proven ability to maintain performance level in a fast-paced agile environment
Pragmatic and realistic with solutions


For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please contact talent@care.com.**


____________________________________________________________________________________________________________________________

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: $116,000 to $145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).
Show more "
3576708419,Data Engineer (Python/Pyspark),Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-python-pyspark-at-diverse-lynx-3576708419?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=W2nSX6n8cd3HNFDOttFioQ%3D%3D&position=19&pageNum=1&trk=public_jobs_jserp-result_search-card,1691,"Description




Job Description




Design, develop, test, deploy, support, enhance data integration solutions seamlessly to connect and integrate enterprise systems in our Enterprise Data Platform.
Innovate for data integration in Apache Spark-based Platform to ensure the technology solutions leverage cutting edge integration capabilities.
Experience with ETL, data pipeline creation to load data from multiple data sources.




Primary Skills




4&plus; years working experience in data integration and pipeline development with BS degree in CS, CE or EE.
2&plus; years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems
Strong real-life experience in python development especially in PySpark in AWS Cloud environment.
Design, develop test, deploy, maintain and improve data integration pipeline.
Experience in Python and common python libraries.
Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.
Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.
Databricks, Redshift Experience is a plus.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3575286924,Software Engineer,First Dollar,2023-04-19,https://www.linkedin.com/jobs/view/software-engineer-at-first-dollar-3575286924?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=yz9edQlGG38ny39tI5ZzZQ%3D%3D&position=20&pageNum=1&trk=public_jobs_jserp-result_search-card,1583,"First Dollar is a technology company that builds health spending benefits infrastructure. We’re at the center of fintech and healthcare, and we’re excited by our opportunity to build tools that simplify health spending benefits like FSAs, HSAs, and others so everyone understands and enjoys using them. Venture-backed and founded by a repeat founding team with prior exits to athenahealth & Expedia, First Dollar has ambitious plans to reshape the healthcare system to work the way it should.

First Dollar is seeking a Software Engineer to join our team. Our web platform powers First Dollar's core business, enabling our members access to their healthcare savings when they need it most. As an early engineering hire, you’ll have a significant opportunity to shape both our technical strategy and engineering culture.

You will:

Work closely with design and product to ship delightful, engaging products using React, TypeScript, NodeJS, and GraphQL
Collaborate on architecture decisions, ensuring that we are adapting and evolving to meet First Dollar's scaling business needs
Contribute to and improve engineering standards, tooling, and processes
Join an environment where different backgrounds, cultures, industry and life experiences are embraced and valued
Be exposed to several facets of building a business outside of just your role as a software engineer


Our engineering team is built on the principle of humans over code. We are a tight-knit group of lifelong learners in a constant quest to be a team that is greater than the sum of its parts. Come join us!
Show more "
3557911421,Data Engineer/Data Analyst,Software Technology Inc.,2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-data-analyst-at-software-technology-inc-3557911421?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=0oHIpygRssW1ObOZQeBU6A%3D%3D&position=21&pageNum=1&trk=public_jobs_jserp-result_search-card,566,"Hi,

Hope you are doing well,

We at Software Technology Inc. hiring for a Data Engineer/Data Analyst, role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or (609) 998-3431.

Note : Looking For 9+ Years Experience

Role : Data Engineer/Data Analyst

Duration : Contract (W2 only)

Location : Remote

Must Have

GCP Big Query, Python, SQL and Airflow and ETL OR SAS OR Teradata

Healthcare domain

Regards ,

Suresh Reddy.k

Technical Recruiter

Software Technology Inc.

Email: ksuresh@stiorg.com

609-998-3431
Show more "
3580741558,Data Engineer,Neurable,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-neurable-3580741558?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=WroTbNUwZglWihpkVOMZJQ%3D%3D&position=22&pageNum=1&trk=public_jobs_jserp-result_search-card,3947,"Neurable is seeking a Data Engineer to join our team and help build wearable devices powered by neurotechnology. Your goal will be to bring cutting-edge data management and AI/ML solutions to bear in the field of neurotechnology.




This position will be responsible for developing new tools to improve our neuroinformatics database and data archival pipeline. You will also work closely with our data science and data collection teams in order to deploy AI/ML models, clean and refine neural data streams, and develop processes for monitoring and visualizing data.




If you enjoy creative problem solving in a fast-paced startup environment. If you can find yourself getting lost tinkering. If you are curious by nature, consider yourself a technologist or someone who wants to affect change, you will fit in well with our team.




This is an opportunity to join a high-impact company, a world class team, and pioneer new technology that will change the way people interact with computers. We want you to have full creative latitude and know that this is your company, not just a job.




What You Will Do:




Maintain and enhance pipelines for filtering, denoising, featurizing, and modeling EEG data
Develop and maintain state-of-the-art methods for data archival and management
Work closely with our experimental team to optimize the quality of data and data labels that are being collected
Develop dashboards for continuous monitoring of data quality
Develop new methods for denoising and preprocessing EEG data in conjunction with other modalities including accelerometer data
Support server configuration (web and application) and deployment
Lead data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS)
Communicate with internal teams and external stakeholders
Define experiments, provide scientific guidance, and help the engineering team throughout R&D and Product life cycles
Manage and contribute to grant applications, studies, and projects




The Ideal Candidate Will Possess:




At least 6 months to 1 year of experience preferred
PhD or Master’s in Computer Science, Engineering, Cognitive or Computational Neuroscience, Physical Sciences, or Applied Mathematics/Statistics
Excellent programming skills in Python, Bash, or MATLAB
Experienced with version control, CI/CD, unit testing, and issue/release management
Knowledgeable with API deployment and containerization
The ability to get new applications up and running, and to overcome hurdles as they arise, is particularly helpful
Experience in brain-computer interfaces or neuroengineering is preferred but not necessary
Enthusiastic for building BCI technology and hungry to learn
Basic experience with signal processing, machine learning, and deep learning frameworks (e.g., PyTorch, Tensorflow) is a plus
Prior experience and continued interest in mentorship and technical development of junior data engineers
Ability to solve problems that have not been solved, in other words, a willingness to explore the unknown and ability to make progress
Ability to embrace uncertainty when working on challenging research questions
Knowledge of perceptual/behavioral evaluations and physiological measurement is desired
Hardware and software experience with multimodal data acquisition, instrumentation, and human-machine interfaces is a plus




Compensation and Benefits:




Competitive salary and equity
High quality health insurance (100% company paid)
401(k) with employer matching contributions
Generous PTO
Pet friendly office, fun team activities, and homemade waffles every Wednesday!




We are not able to provide a visa or sponsorship for this position. All candidates must be authorized to work in the USA.




Powered by JazzHR




sSwN5hThCx
Show more "
3575982734,Data Developer,Apache Corporation,2023-04-21,https://www.linkedin.com/jobs/view/data-developer-at-apache-corporation-3575982734?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=I9BL2BcO4rGY0lMxtj8Emw%3D%3D&position=23&pageNum=1&trk=public_jobs_jserp-result_search-card,5081,"Company Overview




Our primary product is energy, and where there is affordable, abundant energy, people are healthier, have access to better education, and are given greater opportunities to elevate their families to higher standards of living.




Nearly 3 billion people — roughly one-third of the global population — live without electricity or without clean cooking facilities. We are committed to providing energy in innovative and more sustainable ways to help raise the standard of living for those living in energy poverty and to meet the ongoing demands of people and economies around the world.




The products we deliver power increasingly cleaner electricity across the globe, fuel tractors and trucks, make fertilizer to keep the world's food supply on the table, and heat our schools, hospitals and businesses.




Our employees bring a wide range of talents and skills to the job every day to tackle complex business challenges. We believe in providing a truly rewarding work environment supported by a benefits platform that ranks among the best in our peer group. Our company offers career development opportunities where employees can grow personally and professionally. We promote employee benefits that cultivate a family-friendly work environment and focus on our employees' overall well-being.




We are committed to being a workplace where all employees are valued and can thrive with a sense of belonging. Our commitment to diversity and inclusion benefits our individual employees, our company and our external stakeholders; we are better as an organization when various ideas and perspectives are brought to the table.




Apache Corporation is a wholly owned subsidiary of APA Corporation (NASDAQ:APA). Apache has operations in the United States, Egypt's Western Desert and the United Kingdom's North Sea and a sister company with exploration opportunities offshore Suriname. Whether supporting Apache, APA Corporation or one of its subsidiaries, team members are employed by Apache Corporation.




For additional information about APA Corporation, please visit:




Portfolio




Sustainability




Investors




www.apacorp.com




Specific Responsibilities




The Data Developer Position for Apache Corporation will be a member of Apache’s IT Data Management team at the company’s Houston, Texas office reporting to the IT Supervisor. This role will be primarily responsible for developing application code to ingest, transform, and surface business data for analytical users that is optimal, efficient, performant, and maintainable.




This role will be primarily responsible for the following:




Apache Spark (Python / Pyspark)
Apache Kafka
Apache Airflow
Github
Jetbrains IDEs (IntelliJ IDEA, PyCharm)
Trino / Presto SQL Query engine
Oracle / PL-SQL
SAP HANA




Qualifications & Experience




The successful candidate will have the following qualifications and experience:




Minimum Requirements




Strong analytical and problem-solving skills
Excellent written and verbal communication skills
Solid people skills and business partnership orientation
Ability to learn quickly and contribute to a high functioning team
Sense of accountability, owning one’s work and taking pride in it; commitment to quality and delivery
1+ years in ETL/software development
Hands on experience in SQL and/or Python scripting




Preferred Requirements




1+ years of oil and gas industry experience
Experience with Big Data Platforms such as Cloudera or Hortonworks
Experience developing and supporting Python based applications
Experience developing Apache Spark applications using Python
Experience working with Linux operating systems
Experience with real-time data stream platforms such as Kafka and Spark Streaming




Competencies




The successful candidate should demonstrate and exhibit the following core competencies:




Communication: Writes, speaks, and presents information effectively and persuasively across communication setting;
Results: Pursues work with energy, drive, and results orientation to positively impact Apache's business success;
Collaboration: Works in partnership with others and encourages different perspectives, while building and maintaining trust; and
Culture: Willingness and ability to align one's behavior with the needs, priorities, and goals of Apache.




Apache Statement on Hiring




To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to base all employment-related decisions and actions exclusively on employment-related criteria. To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to provide broad dissemination of job opportunities, as consistent with the nature of the positions. To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to review its employment-related policies and actions on a regular basis to ensure that their application is consistent with their intent.




Equal Employment Opportunity
Show more "
3570848637,Data Engineer - Data Science,Bristol Myers Squibb,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-data-science-at-bristol-myers-squibb-3570848637?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=jchQM1N57B1f4pGDfQTNgw%3D%3D&position=24&pageNum=1&trk=public_jobs_jserp-result_search-card,8779,"Working with Us




Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.




Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more careers.bms.com/working-with-us




Senior Manager, Scientific Data Engineering, Data Engineer




The role of Senior Manager in Research IT Data Engineering – Analytics Data Integration, is accountable for the analysis, design, development, support, and provisioning of data products and technologies used for data analytics by Research scientists. This position will ensure that scientific data from disparate sources is combined into meaningful and valuable datasets using APIs, ETLs, and data virtualization by extracting large volumes of data from source systems and loading it into enterprise data stores. This role will work in partnership with a broad range of partners in IT and in Research, including IT Business Partners, Product Teams, and with software development teams to deliver innovative data capabilities while ensuring adherence to data architecture standards and best practices.




Key Responsibilities




Work as a member of the Analytics Data Integration team, dedicated to delivering high-quality data sets for use in analytics platforms, with full API and ETL lifecycle management, to support and accelerate the lifecycle of data to analytics
Contribute to the design, development, and support of Research Analytics Data Platforms using integration frameworks and patterns to enable the transformation of data leveraging modern data services, integration, transformation, and virtualization solutions
Contribute to the provision of knowledge on APIs and underlying logical and physical data models across core platforms
Contribute to the provision of data services and API reference capability blueprints across core analytics platforms
Work with vendors for continuous improvement of APIs and drive for complexity reduction & simpler interfaces
Contribute to the enablement of scalable and efficient data integration across scientific domains at the insight level
Provide understanding of scientific data types, data ingestion and loading patterns, data transformation & transport options
Drive implementation of and adherence to data models, data governance and/or data use policies in partnership with business owners and data custodians




Qualifications & Experience




Minimum of a Bachelor’s Degree in Computer Science, Information Systems, Computer Engineering, or equivalent experience. Alternatively, a degree in a life science discipline with deep computer science expertise
5+ years’ experience in designing, implementing, and/or supporting technical capabilities including information systems, digital solutions, information management, data pipelines, and reporting and analytics platforms in the pharmaceutical industry
Experience in the development of scalable and high performing data models, databases, and data flow architectures for structured, semi-structured, and unstructured data in on-premises or cloud RDBMS, Non-SQL DBs, and cloud storage services like Amazon S3
Experience with data-centric technologies like Data and Analytics Services, Data Lakes, ELT, ETLs
Experience designing and developing data ingestion, integration, virtualization, and transformation platforms
Hands-on experience with data engineering tools and languages such as Data Mesh, Modak Nabu, AWS Database Services, Data Services and Web Services, Informatica, Glue, Spark, Python, Oracle PL/SQL, RESTful API, GraphQL API, Shell scripts, etc.
Experience in source control and deploying in AWS infrastructure and CloudFormation
Knowledge and experience with R&D data types and data flows; data conversions, data migrations, data aggregation; exposure to Machine Learning/Data Science
Experience in information Taxonomy and Ontology development and management, data catalog, data governance and data quality a plus
Experience in using data analytics, visualization, and business intelligence platforms
Proven ability to work as a team player with strong collaboration skills




Why You Should Apply




Around the world, we are passionate about making an impact on the lives of patients with serious diseases. Empowered to apply our individual talents and diverse perspectives in an inclusive culture, our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.




Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives.




Our company is committed to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace adjustments and ongoing support in their roles. Applicants can request an accommodation prior to accepting a job offer. If you require reasonable accommodation in completing this application, or any part of the recruitment process direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.




If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.




Uniquely Interesting Work, Life-changing Careers




With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.




On-site Protocol




Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.




COVID-19 Information




To protect the safety of our workforce, customers, patients and communities, the policy of the Company requires all employees and workers in the U.S. and Puerto Rico to be fully vaccinated against COVID-19, unless they have received an exception based on an approved request for a medical or religious reasonable accommodation. Therefore, all BMS applicants seeking a role located in the U.S. and Puerto Rico must confirm that they have already received or are willing to receive the full COVID-19 vaccination by their start date as a qualification of the role and condition of employment. This requirement is subject to state and local law restrictions and may not be applicable to employees working in certain jurisdictions such as Montana. This requirement is also subject to discussions with collective bargaining representatives in the U.S.




BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.




BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.




Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
Show more "
3578404899,Data Engineer II - Remote,Help at Home,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-ii-remote-at-help-at-home-3578404899?refId=%2Flz2Rzo6nnEhC5Gpzi%2FHFg%3D%3D&trackingId=mBm9YTVKhRZ%2B%2FM5ZOZMp1g%3D%3D&position=25&pageNum=1&trk=public_jobs_jserp-result_search-card,5818,"Help at Home is the leading national provider of in-home personal care services, where our mission is to enable individuals to live with independence and dignity at home. Our team supports 66,000 clients monthly with the help of 49,000 compassionate caregivers across 12 states. We’re looking for people who care about others, who are willing to listen, lean in and make impactful change. Each role at Help at Home can have a positive impact in supporting our caregivers and clients. If you are someone who leads with passion and integrity and are looking to join a rapidly growing, industry leading team, Help at Home may be a good fit for you.

Job Summary

The Data Engineer II is responsible for delivering data warehouse solutions by building enterprise data models and writing ETL/ELT processes to map, cleanse and standardize multiple source systems of data to populate the enterprise data models for business consumption. This role is responsible for delivering solutions that meet our growing business needs as they relate to our enterprise data and analytics strategy for Care Coordination and Help at Home. The ideal candidate should be comfortable with leading teams and driving creation of a platform with focus on the Data Mesh architecture.

As a Key Member Of The Team

You are flexible and can embrace change
You value progress over perfection
You care about your work, the team you’re on, and the people we are helping
You make it a priority to get to know the people around you – build relationships with your colleagues and business partners
You say what needs to be said, while considering how it’ll affect culture and output
Hold others to a high standard

Duties/Responsibilities

Effectively communicates with stakeholders to understand business requirements with the ability to translate requirements into technical designs and solutions and convey the requirements and designs to team members
Serves as the lead or partner with the lead DevOps engineer to build CI/CD pipelines and enforce DevOps standards and policies
Builds frameworks and promotes common patterns
Creates GitHub actions and builds pipelines for dev, stage, and prod
Maps source system data structures into the data warehouse data model (source-target mapping) and enhance the data warehouse data model as needed to meet the business needs
Identifies releases that do not meet defined standards for code, data quality, etc. and delays or block implementation
Understands and implements data warehouse best practices like change data capture (CDC) and slowly changing dimension (SCD) concepts
Promotes and helps with the adoption of Infrastructure as code
Improves our overall data security posture; strengthens our SDLC and DevOps strategy in support of sustained business growth
In alignment with Data Mesh, builds ingestion, integration and sharing patterns and frameworks for better data access
Works closely with stakeholders to understand the business requirements and collaborate with data team members to design and develop flexible, reusable solutions that are consistent with data warehouse architecture standards
Maintains knowledge of current trends and developments in the field and actively explore emerging technologies

Required Skills/Abilities

Cloud-first mindset
Ability to work in a fast-paced dynamic, environment delivering solutions that significantly impact the business
Knowledge of Airflow or MWAA
Knowledge of Spark, Kafka and other batch and stream processing platforms including AWS Kinesis
Knowledge of testing frameworks and TDD or BDD
Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude who is willing to support and teach fellow team members
Strong data analysis skills
Strong relational database skills including advanced SQL knowledge and the ability to create complex queries and stored procedures.
Strong understanding of data warehouse and business intelligence design principles and industry best practices, including relational and dimensional modeling and ETL/ELT methods
Understanding of trunk-based development.
Working knowledge of Snowflake
Working knowledge of Snowflake DBMS and JSON

Education And Experience

AWS architecture, developer, security, and networking experience.
5+ Years of experience in data engineering required.
Bachelor’s Degree in Computer Science, Data & Analytics, Information Management, Healthcare Informatics, Business Administration, Statistics, or related field required.
Cloud (AWS), Warehousing, and Snowflake experience required.
Demonstrated experience with automation.
Demonstrated experience with one or more of the following languages: Go, Python, Typescript.
Strong experience with various AWS services like (S3, Lambda, Glue, EMR, CloudFormation, MWAA, Kinesis, MSK).
Strong experience working with a variety of data warehousing models and design fundamentals (i.e., Kimball, Inman)

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions upon request.

Help At Home is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or religion or other legally protected status.
Show more "
3568557792,Data Engineer (Remote),Blue Orange Digital,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-remote-at-blue-orange-digital-3568557792?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=5L8u6N6WEPpiMuoeAXX21Q%3D%3D&position=1&pageNum=2&trk=public_jobs_jserp-result_search-card,3560,"Company Overview:




Blue Orange Digital is a cloud-based data transformation and predictive analytics development firm with offices in NYC and Washington, DC. From startups to Fortune 500’s, we help companies make sense of their business challenges by applying modern data analytics techniques, visualizations, and AI/ML. Founded by engineers, we love passionate technologists and data analysts. Our startup DNA means everyone on the team makes a direct contribution to the growth of the company.




Position Overview:



Blue Orange is seeking a Data Engineer to join our team to help build up our data engineering practice. Our engineers require a diverse skill set including system administration, DevOps, infrastructure automation, data modeling, and workflow orchestration. Blue Orange builds enterprise data platforms and systems for a variety of clients, so this candidate should have experience with supporting modern data technologies. The ideal candidate will have experience with multiple data engineering technologies across multiple clouds and deployment scenarios. In particular, we’re looking for someone with experience with Azure DevOps, Databricks, and Python.



This is a full-time fully remote position. 



Core Responsibilities & Skills



Work with data teams to help design, build, and deploy data platforms in the cloud (Azure, AWS, GCP) and automate their operation.
Work with Azure DevOps, Azure Pipelines, Terraform, CloudFormation, and other Automation and infrastructure tools to build robust systems.
Work with Databricks, Spark, and Python with data orchestration, and ETL tools to build high-performance data pipelines.
Provide leadership in applying software development principles and best practices, including Continuous Integration, Continuous Delivery/Deployment, and managing Infrastructure as Code, Automated Testing across multiple software applications.
Support heterogeneous technologies environments including both Windows and Linux systems.
Develop reusable, automated processes, and custom tools.

Qualifications



BA/BS degree in Computer Science or a related technical field, or equivalent practical experience.
At least 6 years of experience building and supporting data platforms; exposure to data technologies like Azure Data Factory, Azure Synapse Analytics, Airflow, and Spark.
Experience with Cloud Data Platforms, like Snowflake and Databricks.
Advanced level Python, SQL, and Bash scripting.
Experience designing and building robust CI/CD pipelines.
Strong Linux system administration skills.
Comfortable with Docker, configuration management, and monitoring tools.
Knowledge of best practices related to security, performance, and disaster recovery.
Experience working in cloud environments, at a minimum experience in Azure and AWS.
Enjoys collaborating with other engineers on architecture and sharing designs with the team. 
Excellent verbal and written English communication.
Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment.

Bonus Points



Hold certifications for Azure DevOps, Azure Data Fundamentals, Databricks, and Snowflake.

Our Benefits Include:



Fully remote
Flexible Schedule
Unlimited Paid Time Off (PTO)
Paid parental/bereavement leave
Worldwide recognized clients to build skills for an excellent resume
Top-notch team to learn and grow with

Salary: $5000 - $6650 USD per month



Blue Orange Digital is an equal-opportunity employer.



Background checks may be required for certain positions/projects.



Show more "
3580741558,Data Engineer,Neurable,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-neurable-3580741558?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=ZnP3HWTN1HMjsxrlWB3WdQ%3D%3D&position=2&pageNum=2&trk=public_jobs_jserp-result_search-card,3947,"Neurable is seeking a Data Engineer to join our team and help build wearable devices powered by neurotechnology. Your goal will be to bring cutting-edge data management and AI/ML solutions to bear in the field of neurotechnology.




This position will be responsible for developing new tools to improve our neuroinformatics database and data archival pipeline. You will also work closely with our data science and data collection teams in order to deploy AI/ML models, clean and refine neural data streams, and develop processes for monitoring and visualizing data.




If you enjoy creative problem solving in a fast-paced startup environment. If you can find yourself getting lost tinkering. If you are curious by nature, consider yourself a technologist or someone who wants to affect change, you will fit in well with our team.




This is an opportunity to join a high-impact company, a world class team, and pioneer new technology that will change the way people interact with computers. We want you to have full creative latitude and know that this is your company, not just a job.




What You Will Do:




Maintain and enhance pipelines for filtering, denoising, featurizing, and modeling EEG data
Develop and maintain state-of-the-art methods for data archival and management
Work closely with our experimental team to optimize the quality of data and data labels that are being collected
Develop dashboards for continuous monitoring of data quality
Develop new methods for denoising and preprocessing EEG data in conjunction with other modalities including accelerometer data
Support server configuration (web and application) and deployment
Lead data engineering efforts, including database and API design, data extraction/transformation/load, and data aggregation/integration
Containerize and deploy software and workflows on local high performance computing platforms and cloud computing infrastructure (AWS)
Communicate with internal teams and external stakeholders
Define experiments, provide scientific guidance, and help the engineering team throughout R&D and Product life cycles
Manage and contribute to grant applications, studies, and projects




The Ideal Candidate Will Possess:




At least 6 months to 1 year of experience preferred
PhD or Master’s in Computer Science, Engineering, Cognitive or Computational Neuroscience, Physical Sciences, or Applied Mathematics/Statistics
Excellent programming skills in Python, Bash, or MATLAB
Experienced with version control, CI/CD, unit testing, and issue/release management
Knowledgeable with API deployment and containerization
The ability to get new applications up and running, and to overcome hurdles as they arise, is particularly helpful
Experience in brain-computer interfaces or neuroengineering is preferred but not necessary
Enthusiastic for building BCI technology and hungry to learn
Basic experience with signal processing, machine learning, and deep learning frameworks (e.g., PyTorch, Tensorflow) is a plus
Prior experience and continued interest in mentorship and technical development of junior data engineers
Ability to solve problems that have not been solved, in other words, a willingness to explore the unknown and ability to make progress
Ability to embrace uncertainty when working on challenging research questions
Knowledge of perceptual/behavioral evaluations and physiological measurement is desired
Hardware and software experience with multimodal data acquisition, instrumentation, and human-machine interfaces is a plus




Compensation and Benefits:




Competitive salary and equity
High quality health insurance (100% company paid)
401(k) with employer matching contributions
Generous PTO
Pet friendly office, fun team activities, and homemade waffles every Wednesday!




We are not able to provide a visa or sponsorship for this position. All candidates must be authorized to work in the USA.




Powered by JazzHR




sSwN5hThCx
Show more "
3575982734,Data Developer,Apache Corporation,2023-04-21,https://www.linkedin.com/jobs/view/data-developer-at-apache-corporation-3575982734?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=9UiiqRjIRx6YeVTz6Dgb2A%3D%3D&position=3&pageNum=2&trk=public_jobs_jserp-result_search-card,5081,"Company Overview




Our primary product is energy, and where there is affordable, abundant energy, people are healthier, have access to better education, and are given greater opportunities to elevate their families to higher standards of living.




Nearly 3 billion people — roughly one-third of the global population — live without electricity or without clean cooking facilities. We are committed to providing energy in innovative and more sustainable ways to help raise the standard of living for those living in energy poverty and to meet the ongoing demands of people and economies around the world.




The products we deliver power increasingly cleaner electricity across the globe, fuel tractors and trucks, make fertilizer to keep the world's food supply on the table, and heat our schools, hospitals and businesses.




Our employees bring a wide range of talents and skills to the job every day to tackle complex business challenges. We believe in providing a truly rewarding work environment supported by a benefits platform that ranks among the best in our peer group. Our company offers career development opportunities where employees can grow personally and professionally. We promote employee benefits that cultivate a family-friendly work environment and focus on our employees' overall well-being.




We are committed to being a workplace where all employees are valued and can thrive with a sense of belonging. Our commitment to diversity and inclusion benefits our individual employees, our company and our external stakeholders; we are better as an organization when various ideas and perspectives are brought to the table.




Apache Corporation is a wholly owned subsidiary of APA Corporation (NASDAQ:APA). Apache has operations in the United States, Egypt's Western Desert and the United Kingdom's North Sea and a sister company with exploration opportunities offshore Suriname. Whether supporting Apache, APA Corporation or one of its subsidiaries, team members are employed by Apache Corporation.




For additional information about APA Corporation, please visit:




Portfolio




Sustainability




Investors




www.apacorp.com




Specific Responsibilities




The Data Developer Position for Apache Corporation will be a member of Apache’s IT Data Management team at the company’s Houston, Texas office reporting to the IT Supervisor. This role will be primarily responsible for developing application code to ingest, transform, and surface business data for analytical users that is optimal, efficient, performant, and maintainable.




This role will be primarily responsible for the following:




Apache Spark (Python / Pyspark)
Apache Kafka
Apache Airflow
Github
Jetbrains IDEs (IntelliJ IDEA, PyCharm)
Trino / Presto SQL Query engine
Oracle / PL-SQL
SAP HANA




Qualifications & Experience




The successful candidate will have the following qualifications and experience:




Minimum Requirements




Strong analytical and problem-solving skills
Excellent written and verbal communication skills
Solid people skills and business partnership orientation
Ability to learn quickly and contribute to a high functioning team
Sense of accountability, owning one’s work and taking pride in it; commitment to quality and delivery
1+ years in ETL/software development
Hands on experience in SQL and/or Python scripting




Preferred Requirements




1+ years of oil and gas industry experience
Experience with Big Data Platforms such as Cloudera or Hortonworks
Experience developing and supporting Python based applications
Experience developing Apache Spark applications using Python
Experience working with Linux operating systems
Experience with real-time data stream platforms such as Kafka and Spark Streaming




Competencies




The successful candidate should demonstrate and exhibit the following core competencies:




Communication: Writes, speaks, and presents information effectively and persuasively across communication setting;
Results: Pursues work with energy, drive, and results orientation to positively impact Apache's business success;
Collaboration: Works in partnership with others and encourages different perspectives, while building and maintaining trust; and
Culture: Willingness and ability to align one's behavior with the needs, priorities, and goals of Apache.




Apache Statement on Hiring




To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to base all employment-related decisions and actions exclusively on employment-related criteria. To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to provide broad dissemination of job opportunities, as consistent with the nature of the positions. To provide genuine equal opportunity to all people, it is the policy of Apache Corporation and its subsidiaries to review its employment-related policies and actions on a regular basis to ensure that their application is consistent with their intent.




Equal Employment Opportunity
Show more "
3578404899,Data Engineer II - Remote,Help at Home,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-ii-remote-at-help-at-home-3578404899?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=V5XhuVI0tirooCEsbHGQJg%3D%3D&position=4&pageNum=2&trk=public_jobs_jserp-result_search-card,5818,"Help at Home is the leading national provider of in-home personal care services, where our mission is to enable individuals to live with independence and dignity at home. Our team supports 66,000 clients monthly with the help of 49,000 compassionate caregivers across 12 states. We’re looking for people who care about others, who are willing to listen, lean in and make impactful change. Each role at Help at Home can have a positive impact in supporting our caregivers and clients. If you are someone who leads with passion and integrity and are looking to join a rapidly growing, industry leading team, Help at Home may be a good fit for you.

Job Summary

The Data Engineer II is responsible for delivering data warehouse solutions by building enterprise data models and writing ETL/ELT processes to map, cleanse and standardize multiple source systems of data to populate the enterprise data models for business consumption. This role is responsible for delivering solutions that meet our growing business needs as they relate to our enterprise data and analytics strategy for Care Coordination and Help at Home. The ideal candidate should be comfortable with leading teams and driving creation of a platform with focus on the Data Mesh architecture.

As a Key Member Of The Team

You are flexible and can embrace change
You value progress over perfection
You care about your work, the team you’re on, and the people we are helping
You make it a priority to get to know the people around you – build relationships with your colleagues and business partners
You say what needs to be said, while considering how it’ll affect culture and output
Hold others to a high standard

Duties/Responsibilities

Effectively communicates with stakeholders to understand business requirements with the ability to translate requirements into technical designs and solutions and convey the requirements and designs to team members
Serves as the lead or partner with the lead DevOps engineer to build CI/CD pipelines and enforce DevOps standards and policies
Builds frameworks and promotes common patterns
Creates GitHub actions and builds pipelines for dev, stage, and prod
Maps source system data structures into the data warehouse data model (source-target mapping) and enhance the data warehouse data model as needed to meet the business needs
Identifies releases that do not meet defined standards for code, data quality, etc. and delays or block implementation
Understands and implements data warehouse best practices like change data capture (CDC) and slowly changing dimension (SCD) concepts
Promotes and helps with the adoption of Infrastructure as code
Improves our overall data security posture; strengthens our SDLC and DevOps strategy in support of sustained business growth
In alignment with Data Mesh, builds ingestion, integration and sharing patterns and frameworks for better data access
Works closely with stakeholders to understand the business requirements and collaborate with data team members to design and develop flexible, reusable solutions that are consistent with data warehouse architecture standards
Maintains knowledge of current trends and developments in the field and actively explore emerging technologies

Required Skills/Abilities

Cloud-first mindset
Ability to work in a fast-paced dynamic, environment delivering solutions that significantly impact the business
Knowledge of Airflow or MWAA
Knowledge of Spark, Kafka and other batch and stream processing platforms including AWS Kinesis
Knowledge of testing frameworks and TDD or BDD
Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude who is willing to support and teach fellow team members
Strong data analysis skills
Strong relational database skills including advanced SQL knowledge and the ability to create complex queries and stored procedures.
Strong understanding of data warehouse and business intelligence design principles and industry best practices, including relational and dimensional modeling and ETL/ELT methods
Understanding of trunk-based development.
Working knowledge of Snowflake
Working knowledge of Snowflake DBMS and JSON

Education And Experience

AWS architecture, developer, security, and networking experience.
5+ Years of experience in data engineering required.
Bachelor’s Degree in Computer Science, Data & Analytics, Information Management, Healthcare Informatics, Business Administration, Statistics, or related field required.
Cloud (AWS), Warehousing, and Snowflake experience required.
Demonstrated experience with automation.
Demonstrated experience with one or more of the following languages: Go, Python, Typescript.
Strong experience with various AWS services like (S3, Lambda, Glue, EMR, CloudFormation, MWAA, Kinesis, MSK).
Strong experience working with a variety of data warehousing models and design fundamentals (i.e., Kimball, Inman)

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions upon request.

Help At Home is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or religion or other legally protected status.
Show more "
3580000717,Senior Data Platform Engineer,KingsIsle Entertainment,2023-04-22,https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-kingsisle-entertainment-3580000717?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=MmkCe3dnFEgQveecXBY7Kg%3D%3D&position=5&pageNum=2&trk=public_jobs_jserp-result_search-card,2235,"KingsIsle Entertainment is currently looking for a Senior Data Platform Engineer to join our Data Warehouse/Analytics team. We are the creators of the popular and critically acclaimed Wizard101 and Pirate101, which together have garnered over 69 million registered players to date, and are two of the fastest growing websites in the U.S. This is an opportunity to be a part of a rapidly growing and highly successful entertainment company.

This job is Telework in Plano, Texas. Candidates can be located within any TX city.

Responsibilities

Work with a diverse and experienced team of developers in a creative and fast paced environment
Develop machine learning and data analytics platform components
Develop and maintain ETL (Extraction, Transformation and Loading) scripts and processing components for our multi-terabyte (Big Data) distributed data warehouse environment
Build scalable data pipelines, transform data, manage repositories, expose APIs
Develop and maintain reports for KingsIsle’s Game Design, Marketing, Operations, and Executive teams using advanced data visualization tools


Requirements

A Bachelor’s degree in Computer Science or Engineering
At least 5 years professional data engineering experience or software development experience using a structured programming language (C, C++, Perl, Python or JAVA)
Working knowledge of SQL
Experience using Linux shells and utilities
Good communication skills
Ability to contribute to and work within schedules that are assigned
Knowledge of software design and software engineering methodologies


Preferences

Database development experience in Vertica and MySQL
Interest in Big Data, Analytics or Statistics
Experience with Google Cloud Platform (BigQuery, Firebase)
Knowledge of XML, JSON and other data exchange standards
Experience with Excel, Tableau, Power BI and other reporting tools
Knowledge of distributed system development


Benefits

Competitive compensation
Unlimited PTO
Company fun days and parties
Free game swag (such as plushies)
Medical, dental, and vision care benefits
401(K) with employer match
Flexible Spending Account options
Lifestyle Benefits Program
Employee Learning Program
Diverse and welcoming teams from all backgrounds
Show more "
3580162419,Data Engineer,The Greenbrier Companies,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-the-greenbrier-companies-3580162419?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=Fct8HzKxDXqHuuY%2FmuC%2Bdg%3D%3D&position=6&pageNum=2&trk=public_jobs_jserp-result_search-card,6275,"At Greenbrier, we do the hard work that matters. The Greenbrier Companies (NYSE:GBX) is powering the movement of products around the world as a leading designer, manufacturer and supplier of freight rail and marine transportation equipment and services.

Greenbrier’s heritage of hard work and industrial innovation is celebrated at every level of our organization. We structure our business to support teams that deliver innovative solutions for our customers while positively impacting the world around us.

Greenbrier’s success begins with people. We believe in supporting our global workforce through our unwavering attention to Safety, Quality, Respect for People and Customer Satisfaction. These values are rooted in our IDEAL commitment, which promotes Inclusion, Diversity, Equity, Access, and Leadership, creating a culture where employees are engaged and feel good about coming to work every day. A diverse, qualified, and engaged talent base is the key to our success.

Summary

The Data Engineer is responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The Data Engineer develops and supports a broad range of software capabilities including building data pipelines, managing the ETL/ELT processes, receiving, and delivering data through various interfaces, and processing significant amounts of data related to railcar movements, railcar liability, and financial calculations.

Duties And Responsibilities

To perform this job successfully an individual must be able to perform the following essential duties satisfactorily. Other duties may be assigned to address business needs and changing business practices.

Participates as a member of an Agile team developing Data Engineering solutions.
Engages in requirements gathering, and technical design discussions to meet business needs.
Designs and develops generic, scalable data pipelines in Azure Data factory and Databricks with python for on-prem and cloud data sources
Assembles large, complex sets of data that meet non-functional and functional business requirements
Solves unstructured data problems, manipulates, and optimizes large data sets to advance business problem-solving.
Contributes to documentation, testing and cross-training of other team members.
Works closely with others to assist and resolve production issues.


Qualifications

The following generally describes requirements to successfully perform the assigned duties.

Minimum Qualifications

Bachelor's degree in computer science, computer engineering, a related field, or 4 years equivalent experience in a computer science related field.
5+ years of data engineering or 5 years equivalent experience in a related computer science field.
5+ years of hands-on experience in developing and deploying data architecture strategies or engineering practices.
5+ years of experience with complex SQL queries and knowledge of database technologies.
Expert-level coding experience with PySpark and Python.
Expert-level technical experience with Apache Spark / Azure Databricks.
Proficient in using and designing solutions on Azure Cloud infrastructure (particularly Azure Data Factory) and Azure DevOps.
Proficient with core business intelligence and data warehousing technology.
Proficient designing and developing data integration solutions using ETL tools such as Azure Data Factory and/or SSIS.
Proficient with software development practices such as Agile, TDD, and CI/CD.
Ability to collaborate and communicate professionally, both verbally and in writing, at all levels of the organization, particularly bridging conversations between data and business stakeholders.


Preferred Qualifications

Experience with Snowflake
Experience with graph databases or graph libraries
Kafka or other streaming technologies
Elastic Search
Experience in the rail or other commodities driven industry


Work Environment And Physical Requirements

Work Environment

The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

This is a remote position.

Physical Activities And Requirements

Frequency Key

Not Applicable: Activity is not applicable to this occupation

Occasionally: Occupation requires this activity up to 33% of the time (0- 2.5+ hours/day)

Frequently: Occupation requires this activity from 33% - 66% of the time (2.5- 5.5+ hours/day)

Constantly: Occupation requires this activity more than 66% of the time (5.5+ hours/day)

Working Postures

Sit: Frequently
Stand: Occasionally
Walk: Occasionally
Bend: Occasionally
Kneel/Squat: Not Applicable
Crawl: Not Applicable
Climb: Not Applicable
Reach Forward: Occasionally
Reach Upward: Occasionally
Handling/Fingering: Frequently


Lift / Carry Requirements

5-10 lbs: Occasionally
10-25 lbs: Not Applicable
25-50 lbs: Not Applicable
50-75 lbs: Not Applicable
75+ lbs: Not Applicable


Push / Pull Requirements

Up to 10 lbs: Occasionally
10-25 lbs: Not Applicable
25-50 lbs: Not Applicable
50-75 lbs: Not Applicable
75+ lbs: Not Applicable


EOE including Vet/Disability

Click here for more information: Know Your Rights

Greenbrier makes reasonable accommodations in the application and hiring process for individuals with known disabilities, unless providing accommodation would result in an undue hardship. Any applicant believing that he or she may need reasonable accommodation for any part of the application and hiring process should contact Greenbrier Human Resources at careers@gbrx.com or call us at 503-684-7000.

Email communication from The Greenbrier Companies (Greenbrier) will always come from a corporate email address that ends in @gbrx.com or from our applicant tracking system, iCIMS, after you have created a secure account and submitted your application. During the application process, you will create a secure account in our secure applicant tracking site that ends with “-gbrx.icims.com”. In this portal, we will ask you to provide your contact information, past employment history, education history and other job-related information.
Show more "
3550201745,Data Engineer,FinTech LLC,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-at-fintech-llc-3550201745?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=mRlelfU9dwUHPLLiorp2VA%3D%3D&position=7&pageNum=2&trk=public_jobs_jserp-result_search-card,70,"Data Engineer:







Spark,



AWS,



Python,



Kafka



Show more "
3549984693,Data Engineer,"Acutus Medical, Inc.",2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-at-acutus-medical-inc-3549984693?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=bTFTN5mo2AONBuzo%2FQ4YDg%3D%3D&position=8&pageNum=2&trk=public_jobs_jserp-result_search-card,6576,"Acutus Medical is totally focused on the development and commercialization of solutions that improve the way complex cardiac arrhythmias are diagnosed and treated. Our passionate, driven team of innovative professionals are dedicated to providing better tools for clinicians and making life better for the millions of people who suffer from these challenging conditions around the world. Are you ready to be a part of a dynamic, innovative team with a shared purpose that truly matters? If so, we are currently looking for a Data Engineer to join us in our important mission.




Position Overview




The Data Engineer is a key contributor on the Medical Affairs team overseeing the collection, preparation, quality, accessibility, and organization of Acutus-supported research data. The Data Engineer, working closely with all internal stakeholders, will be responsible for the curation and quality control of high volumes of data. This role will drive the future of our data infrastructure, ensuring data is reliable, and accessible to stakeholders. The data curation platform will integrate data from a variety of internal and external sources to support quantitative data analytics. The Data Engineer will also optimize data flow and collection, ensure data delivery is efficient, robust, standardized, organized and well labeled. The ideal candidate is proficient in Scientific Computing languages (MATLAB, Python or C++), and has experience in populating and maintaining large databases (SQL). Effective performance in this role is demonstrated by delivering high-quality data and making an impact on our decision-making process while proactively enhancing current operational efforts.




Duties And Responsibilities




Collect and assemble large complex datasets that meet technical specifications for use in scientific and developmental purposes
Process and curate high volumes of data through internal/external software sources
Crafting, maintaining efficient data pipeline architecture and ensuring quality control of the data
Optimizing data flow, ensuring data robustness, organization and consistent self-explanatory labeling
Identifying, designing, and implementing internal process improvements, such as automating manual processes, optimizing standardized data delivery, re-designing infrastructure for greater scalability, etc.
Working with Field, Technical and Medical Affairs team members to assist with data retrieval, data-related technical issues and support their data infrastructure needs
Works under general guidance. Must understand overall project goals. Is a collaborative team player and demonstrates open-mindedness and flexibility. Creates innovative and creative solutions to keep projects on-track. Maintains a high degree of quality in all work performed.
Must demonstrate mutual respect, ongoing communication and a positive outlook with both internal and external team members.




Qualifications




Requires a Bachelor's degree in Information Technology or in another Scientific/Engineering discipline preferably with Research experience, or a Master's degree.
Experience in Scientific Computing languages (MATLAB, Python, C++)
Experience scraping data from various sources, centralizing in existing or to be developed databases
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Ability to learn usage of new software with limited guidance (e.g., quick capacity to learn how to use our software “the Frame”)
Demonstrate an aptitude and enthusiasm for learning. Must be able to understand job duties and responsibilities, have the necessary skills/knowledge and be willing and able to continue learning and growing within the field.
Must be accurate, have strong attention to detail, and demonstrated critical thinking
Responds positively to direction from leadership and guidance received on work assignments
Demonstrates strong technical ability at a fundamental level. Will regularly collaborate with senior Medical Affairs and R&D team members. Works on moderately complex problems where analysis of situations or data requires in-depth evaluation. Exercises judgment within generally defined practices in selecting methods and techniques for obtaining solutions.
Committed to high quality standards and proactive in finding solutions to achieve successful outcomes.
Strong verbal and written communication skills with the ability to produce accurate, punctual reports/information. Must be able to read, write and speak effectively. Must be able to effectively communicate to different audiences (no technical background) and skill levels within the organization
Strong listening skills with the ability to seek constructive feedback and remain flexible and open-minded. Able to quickly adapt to change.
Capable of working under pressure and in a timely manner.




Founded in 2011, Acutus Medical is headquartered in Carlsbad, CA. We pride ourselves on being an innovative company comprised of dedicated and talented industry leaders working together to make a distinctive mark within the Electrophysiology market. Our team works diligently to fulfill the mission of bringing advanced tools for physicians and hospitals to access, identify, diagnose and treat complex arrhythmias to in order to optimize and expand the success of cardiac ablation.




The anticipated salary range for candidates is $95,000 to $115,000. The final salary offered to a successful candidate will be dependent on several factors that may include but are not limited to the type and length of experience within the job, type and length of experience within the industry, education, etc.




Our employees enjoy working in a company that truly cares about them, their career and overall wellbeing. We offer competitive salaries, comprehensive benefits, paid time off, holidays and a variety of health and wellness programs. We are steadily growing and look forward to adding more talent to our team.




Acutus Medical provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.




Acutus uses E-Verify for all employment verifications.




We are not accepting resumes from 3 rd party headhunters or agencies.
Show more "
3564359844,Data Engineer,Robert Half,2023-04-03,https://ca.linkedin.com/jobs/view/data-engineer-at-robert-half-3564359844?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=rc98JHlooIuL%2BBjSWshoOg%3D%3D&position=9&pageNum=2&trk=public_jobs_jserp-result_search-card,3909,"Description

A client of ours is looking to bring on a Data Engineering professional for their team in Toronto. As a data engineer on the business side, you will be responsible for designing, developing, and maintaining the data architecture and structures that support our company's analytical and reporting needs. You will work directly with various stakeholders to understand the requirements, and develop solutions that deliver relevant, accurate, and timely data to support decision making. This opportunity requires an in-office presence three times a week.

Responsibilities

Develop, implement and the support the teams' data strategy to manage structured and unstructured data that integrates the data into a data solution to support BI solutions, reporting and business processes
Design and develop data models, structures, and workflows that support data-driven decision making
Develop and maintain automated data pipelines, ETL jobs, and scripts for data integration
Analyze and optimize data storage and processing to ensure optimal performance
Develop and maintain data documentation, data dictionaries, and metadata management
Provide innovative data solutions by working with stakeholders and across functional areas to understand and gather data objectives, and identify opportunities for improvement for data analysis, reporting and visualizations
Automate manual processes and optimize data ingestion and delivery to support departmental data needs, reporting and business processes
Partner with the Business Analysts to develop reports and Power BI dashboards
Collaborate with business stakeholders to identify and understand their data requirements
Support and troubleshoot existing investment data applications and BI reporting solutions as needed

Requirements

Qualifications

Strong experience with data management, investment systems, business intelligence, and reporting technologies
5+ years of experience working with complex data structures and developing efficient queries
5+ years of relevant experience in data science and analytics
3+ years of experience creating/maintaining investment and financial data models
3+ years of experience designing and developing dashboards using Power BI
Proven proficiency in ETL tools (Alteryx), cloud-based data platforms/systems (such as Azure, Snowflake), process automation, and standardization
3+ years of experience with PowerBI
Strong communication skills (written and verbal)
Critical thinking skills with the ability to independently solve problems with data and/or reference documents
Understanding of data governance, data security, and data privacy requirements
Track of record working in a role that intersects with business and technical analysis to support existing solutions, delivering new solutions, and contributing to system implementations and upgrades
Bachelor’s or master’s degree in Computer Science, Engineering, Mathematics, Finance, or a related field

If you have a passion for data management and a strong technical aptitude, we invite you to consider applying for this exciting opportunity.

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go.

Questions? Call your local office at 1.888.490.4429. All applicants applying for Canadian job openings must be authorized to work in Canada.

© 2023 Robert Half. By clicking “Apply Now,” you’re agreeing to
Show more "
3580999541,Data Engineer,Ascendion,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-ascendion-3580999541?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=XjuqMKPnSvdr1IDmlnezTg%3D%3D&position=10&pageNum=2&trk=public_jobs_jserp-result_search-card,2251,"About Ascendion




Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life




We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:




Build the coolest tech for world’s leading brands




Solve complex problems – and learn new skills




Experience the power of transforming digital engineering for Fortune 500 clients




Master your craft with leading training programs and hands-on experience




Experience a community of change makers!




Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.




Minimum Qualifications




5+ years of experience with SQL




Data Engineering experience




Data migration experience




Strong Data Warehousing experience




Experience with ETL tools




Data Modeling - dimensional modelling standards




Strong database knowledge




Key Responsibilities




In this role, you’d be joining the Workplace Investing business unit focused on the Data and Advanced Analytics platform.




The team is in the process of decommissioning and converting their platforms into the datalake, and looking for someone with strong knowledge in data warehousing, SQL, ELT and ETL, amongst others to help along the journey.




Location:




Raleigh,NC




Salary Range




$60-$65




Want to change the world? Let us know.




Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!




Share your updated resume at shagun.chourasia@ascendion.com
Show more "
3557576364,Junior Software Engineer - Jonsson Cancer Center,UCLA Health,2023-04-07,https://www.linkedin.com/jobs/view/junior-software-engineer-jonsson-cancer-center-at-ucla-health-3557576364?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=J4mHfSxkDM3BxyucrxN%2B1g%3D%3D&position=11&pageNum=2&trk=public_jobs_jserp-result_search-card,2366,"Description

The newly formed Cancer Data Sciences group at the UCLA David Geffen School of Medicine and UCLA Jonsson Comprehensive Cancer Center is seeking a programmer/analyst with research and development experience. This position will be working with a broad team of Data Scientists, developing new quantitative strategies to improve our understanding and ability to treat cancer. Our team is passionate about applying their knowledge of software-development and design to improve scientific research. We develop scalable and distributed software solutions that maximize utilization of both local high-performance computer infrastructure and a growing set of cloud-based assets. Our datasets comprise hundreds of terrabytes, and are growing rapidly, creating fascinating problems in storage, access, parallelization, distributability, optimization, containerization and core algorithm design. This requires a strong background in computer science, providing a platform for technical leadership, but linked to strong personal communication and leadership skills, to help ensure insights are broadly adopted. The successful candidate will be helping us perform research that will transform the lives of cancer patients. Your responsibilities will be to use your design, analysis and programming skills to create Data Science software, optimize existing code and improve its quality and improve distributability boost productivity of the entire team. You may have experience in data-intensive software-development or research, or you may be experienced with software-engineering in an enterprise environment. You will help drive professional-level design and development practices throughout the entire team, and serve as a local point of expertise for workflow optimization and containerization. You will be rewsponsible for one major and several minor projects at any point in time. We are in a rapid growth-phase, and the successful candidate will be involved in hiring of new team members. Beyond your strong inter-personal skills and computer science background, you will have experience with either systems software, databases or algorithms, linked to implementation skills at least one of C++, R, Perl or Python. You will be comfortable in UNIX/Linux environments and using continuous integration and CASE tools. Salary Range: $5525-$10925 Monthly
Show more "
3546057550,Data Engineer,Spindl,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-at-spindl-3546057550?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=MS4DH6uEWxUmy%2FoYbUiwaQ%3D%3D&position=12&pageNum=2&trk=public_jobs_jserp-result_search-card,2246,"You’ll be a founding engineer in a well-funded company looking to define Web 3 user identity and attribution.

We plan on running a lean team: expect to totally own your area of responsibility and use your initiative and judgement to keep your part of Spindl running and growing.

Be comfortable with being uncomfortable. There won’t be hard answers for most of what we do. We’re already making it up as we go along.

Speed is a feature, in teams, products, and people: keep up with the Spindl pack. There isn’t a moment to lose.

We’re looking for pirates now that Web 2 has become the navy.

Responsibilities

Own Spindl's data stack, end to end (from our technology stack, to ingest, to ML modeling)
Be a leader within a very flat and fast-moving team.
Design, develop, test, and deploy software regularly.
Own what you build.
Solve complex engineering problems.
Manage priorities, deadlines, and deliverables.
Creatively address business problems when there are no precedents and no clear answers.


Must haves (this is an AND)

N years in a software engineering role supporting real production systems at scale (for reasonable N). You’ve built shit; you’ve broken shit; you’ve gotten the 2am PagerDuty call and saved the day.
Experience with modern data storage, warehouse, and querying technologies (e.g. Snowflake, Databricks, DBT, Airflow, and similar).
Be smart and get shit done.
Be a doer and a risk-taker.


Nice to haves (this is an OR)

Experience in web 2 ad tech, attribution, or user growth. You know how the Web 2 user sausage is made.
Experience in web 2 product analytics and measurement. You know how data in Web 2 is collected, measured, and visualized.
Experience with blockchain data tooling and infrastructure (what little there is).


Location

You can be located anywhere in the world, but we have a strong preference for SF or NYC. If you’re not in either city, expect some travel to work face-to-face with colleagues on a semi-regular basis. If you’re in one of those cities, you’ll find a balance between coming into the office and not. We think in-person work possesses a magic for early stage startups, something unobtainable in a fully remote environment where co-workers are simply images on a screen.
Show more "
3567597652,Data Engineer,Rappi,2023-04-13,https://mx.linkedin.com/jobs/view/data-engineer-at-rappi-3567597652?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=zSWZ5lnzSqA4MEJE8AU9iA%3D%3D&position=13&pageNum=2&trk=public_jobs_jserp-result_search-card,2246,"¡Oye, es hora de que te unas a nosotros para mostrarle al mundo que somos la empresa que está cambiando paradigmas, donde revolucionamos las horas, los minutos y los segundos!

¿Quieres saber por qué Rappi?

⭐️ VEMOS OPORTUNIDADES donde otros ven problemas;

⭐️ VEMOS CERCANIA donde otros ven distancia;

⭐️ VEMOS ADRENALINA donde otros ven presión.

Únete a un equipo donde todos somos capaces de TODO, donde todos tenemos las mismas oportunidades, sin importar género, raza, orientación sexual, religión, nacionalidad, edad, discapacidad, formación o experiencia.

¿Te ha gustado lo que has leído hasta ahora? Descubre cómo entregarás magia junto con nosotros a través de tu misión Rappi 🧡

Revisa cómo impactarás nuestro ecosistema:

Deploy APIs for our ML models and support the team with backend and automated data processing.

Como parte de Rappi, serás responsable de:

Full support the team with Problem-solving solutions based in tech/engineering topics.
Design and implement ETL data processes from a wide variety of sources (relational, NoSQL, web services, flat files, API) on project’s demand.
Build, implement, and maintain end-to-end data pipelines in our products data.
Building APIs for internal use.


Bien y ahora, ¿Cuáles son los requisitos para que yo sea parte de este universo de neón?

Problem-solving attitude is a must!
Experience as a Data Scientist/ Data Engineer.
Advance proficiency writing code in Python.
Expertise in SQL.
Experience developing APIs.
Experience with Docker & Kubernetes.
Proven analytical skills.
Knowledge of data mining and segmentation techniques.
Extensive knowledge of data structures, data modeling, and software architecture.
Ability to communicate technical problems/solutions to technical and business teams.
Intermediate english level


Requirements

ETLs
Python
SQL
Creación y mantenimiento de APIs
Prácticas en desarrollo de software como unit test
Manejo Docker
Git
Scrum


Tipo Empleador:

Regular

Empleador:

Rappi Technology Colombia

Para más información consulta nuestra pagina web https://about.rappi.com/es y revisa nuestras reseñas en Glassdoor https://acortar.link/Eqm07Q

¡Te esperamos con mucha ansiedad y brillo en los ojos para entregar magia juntos! #Rappi 🧡
Show more "
3580619884,Data Analyst (Remote),CareFirst BlueCross BlueShield,2023-04-24,https://www.linkedin.com/jobs/view/data-analyst-remote-at-carefirst-bluecross-blueshield-3580619884?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=a3MU1TvWLjXDQNS3sIwCaw%3D%3D&position=14&pageNum=2&trk=public_jobs_jserp-result_search-card,3036,"Purpose

Resp & Qualifications

The main purpose of a data analyst is to find meaning in data so that the derived knowledge can be used to make informed decisions and assist the organization in making better business decisions.

Essential Functions

Engages in data exploration using various programming languages (reading and writing code), requiring knowledge of multiple database structures.
Supports stakeholder in gathering business and technical requirements to deliver data driven solutions.
Produces technical documentation that is consistent with professional standards.
Research and analyze data flow across data ecosystem including operational and analytical data platforms.
Serve as an knowledgable resource supporting data questions and issues from stakeholders.
Collect, analyze, and interpret data to glean insights, and suggest business appropriate recommendations.
Participate in the design, development, validation and delivery of data to provide insight into business decisions.

Qualifications

Education Level: Bachelor's Degree in Data Science, Mathematics, Computer Science, Statistics, Business or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 3 years of data analysis or relevant work experience.

Preferred Qualifications

Hands on experience with Big Data/NoSQL Platforms with experience delivering production projects at scale.
Experience in health care industry.

Knowledge, Skills And Abilities (KSAs)

Quantitative ,analytical and problem solving skills.
Knowledge and understanding of analytical tools , languages ,applications, platforms.
Excellent communication skills both written and verbal.
Ability to learn quickly and take direction.

Department

Department:Payment Integrity

Equal Employment Opportunity

CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.

Hire Range Disclaimer

Actual salary will be based on relevant job experience and work history.

Where To Apply

Please visit our website to apply: www.carefirst.com/careers

Federal Disc/Physical Demand

Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.

Physical Demands

The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.

Sponsorship in US

Must be eligible to work in the U.S. without Sponsorship.


Show more "
3567948411,Data Engineer,CVS Health,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-cvs-health-3567948411?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=CjaDEDj%2FLH27N34qaBWfww%3D%3D&position=15&pageNum=2&trk=public_jobs_jserp-result_search-card,4628,"Job Description




At CVS Health, everything starts with our vision to reimagine health care by putting people at the center of all we do and redefine health care in America. Our strategy is focused on making bold moves to deliver solutions that are personalized, connected and increasingly digital.




To execute on our vision and strategic imperatives, the Enterprise Data Services & Operations (EDSO) team is focused on building a modern, connected data ecosystem that accelerates the use of data, analytics, and advanced AI. We are looking for passionate and motivated Data Engineers to join our growing team of engineers. We are on a mission to transform technology and health care and you will play a pivotal role in executing our strategies and making our vision come true.




Responsibilities:




Develops scalable, repeatable and secure data structures and pipelines to organize, collect, standardize and integrate data that facilitate more complete, accurate and consistent data for insights and reporting
Builds efficient, high quality, integrated and increasingly real-time ETL/ELT processes using tools, programming languages and services
Delivers data capabilities and new data products to accelerate investments in data and democratize data across the enterprise and within the Health Care Business unit
Collaborates with data science team to transform data and integrate algorithms and models into automated processes
Builds data models, data marts and extracts to support Data Science, internal customers and third-party administrators
Analyzes data, understands complex systems and solves challenging data sourcing and integration problems
Promotes engineering excellence by simplifying, optimizing, and automating processes and workflows
Innovates by leveraging and practicing agile techniques to achieve efficiency, boost productivity and improve processes





Pay Range




The typical pay range for this role is:




Minimum: $ 70,000




Maximum: $ 140,000




Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.




Required Qualifications




2+ years of hands-on experience building and deploying data pipelines using Python, PySpark or any ETL tool.
Working experience in Hadoop or Relational databases with strong understanding of HQL/SQL.





Preferred Qualifications




Experience building ETL data pipelines in Ab-Initio or equivalent technology
Experience in GCP Cloud services such as Big Query, Composer, Airflow, DataProc.
Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment
Experience with bash shell scripts, UNIX utilities & UNIX Commands, CI/CD
Agile practice experience with methodologies like SAFe.
Healthcare domain experience.





Education




Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline




Master’s degree or PhD preferred




Business Overview




Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
Show more "
3554068564,"Data Engineer (AWS, Python, SQL)",Binary Tech Consulting Corp,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-aws-python-sql-at-binary-tech-consulting-corp-3554068564?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=qskIrk0WtcXCzPJTILFa1w%3D%3D&position=16&pageNum=2&trk=public_jobs_jserp-result_search-card,2407,"Dynamic Work schedule - This is 5 days on site a month- in the same week then the remainder of the month is working from home. ( They can fly/drive into the office as well). If your candidate is not open to this please tell them they will not be considered. Fidelity determines the week they are working onsite, not the candidate.

Location- Durham strongly preferred, Westlake is back up option

Top 3 Must Have Skillsets: Strong emphasis on Python and Python OOPs , cloud experience (AWS preferred), SQL Nice to Have: CI/CD Pipeline, Jenkins

We are currently sourcing for a BI/Data Engineer to work in Durham, NC OR Westlake, TX!

The Team

Customer Data Technology in PI builds the platforms to collect and primary customer data needed to deliver the best customer experience. You will be part of a team that is building a Snowflake based enterprise-wide Data Lake on AWS.

The Role

If you are an expert Software engineer with a passion for data and databases, who enjoys data analysis, data modeling and crafting ETL data flows, and wants to be a part of a collaborative team environment where you will have wealth of opportunities to innovate and have intellectual curiosity to learn, a career in Customer Data Technologies in PI may be right for you! You will be part of a team that is building a Snowflake based enterprise-wide Data Lake on AWS.

The Expertise And Skills You Bring

Bachelor's or Primary Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required.
Extensive experience in relational databases like Oracle or Snowflake
Experience in developing data applications in Cloud (AWS, Azure, Google Cloud)
Development experience using Python
Experience in Data Warehousing, data modeling and creation of data marts.
Experience with ETL technologies (Informatica or similar)
Experience with Business Analytics and Dashboards is a plus
Experience with DevOps, Continuous Integration and Continuous Delivery (Maven, Jenkins, Stash, Ansible, Docker) is a plus
Experience in Agile methodologies (Kanban and SCRUM) is a plus
Experience building scalable and robust ETL data flows using a range of technologies
Strong data analysis skills
Ability to deal with ambiguity and work in fast paced environment
Excellent communication skills, both through written and verbal channels
Excellent collaboration skills to work with multiple teams in the organization
Show more "
3570505211,Data Engineer,Care.com,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3570505211?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=cHsDgVAICAoEi6sCzrge9A%3D%3D&position=17&pageNum=2&trk=public_jobs_jserp-result_search-card,5586,"About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Care.com offers an array of services that enable families to find, manage and pay for care and provide employment opportunities for caregivers. Our engineering organization is reimagining our tech stacks and consolidating them to a single cloud-native platform and a cloud-based Data Lake/Data Warehouse on Snowflake.

Here, entrepreneurs, self-starters, great teammates, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI and the latest technologies to solve universal problems and connect people in new ways. If you enjoy solving big problems and building new things, and if you're all about using your talent for good, this is the place for you.

Office Locations: (This is a hybrid position)

NY, NY 10011
Austin, TX 78746
Shelton, CT 06484


What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:

Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
Identify data sources used for building out data architecture diagrams/models
Establish engineering practices and setup frameworks for ""Data as a Service""
Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
Solve code level problems quickly and efficiently
Participate in demos and code reviews
Promote software best approach, standards, and processes
Shape development processes to promote a high-quality output while continuing to iterate quickly
Incorporate best practices for security, performance, and data privacy into data pipelines


What You'll Need to Succeed:

BS or MS in Computer Science or relevant engineering experience
4+ years work experience in Data Engineering/ETL
3+ years SQL experience preferred
2+ years traditional RDBMS experience (Oracle & Postgres experience preferred)
1+ years Unix/batch scripting preferred
1+ years Python experience is a plus
1+ years Windows server admin experience is a plus
Experience interfacing with business teams and turning requirements and vision into a technical reality
Ability to drive efforts from start to finish as a self-motivator
Knowledge in Data Warehousing is a MUST
Proven ability to maintain performance level in a fast-paced agile environment
Pragmatic and realistic with solutions


For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please contact talent@care.com.**


____________________________________________________________________________________________________________________________

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: $116,000 to $145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).
Show more "
3572918780,Data Engineer,RemX - Accounting & Finance Staffing,2023-04-16,https://www.linkedin.com/jobs/view/data-engineer-at-remx-accounting-finance-staffing-3572918780?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=ILAMatnmtuzYdzLahCQ2qQ%3D%3D&position=18&pageNum=2&trk=public_jobs_jserp-result_search-card,3260,"New Opportunity Systems Engineer Greeneville, TN On-site $110-120k plus excellent benefits We are looking for an enthusiastic Systems Engineer to design, develop and install software solutions. The successful candidate will be able to review software applications standards and technical design. Implement new software platforms work with third party vendors. Support and/or install software applications/operating systems. Participate in the testing process through test review and analysis, test witnessing and certification of software. Requires a bachelor's degree in a related area and 2-5years of experience in the field or in a related area. Has knowledge of commonly used concepts, practices, and procedures within a particular field. Rely on instructions and pre-established guidelines to perform the functions of the job. Work as subject manner expert with minimum supervision. Primary job functions will exercise independent judgment when required. Typically reports to a department manager. Responsibilities:




Performance tuning, improvement, balancing, usability, automation
Support, maintain and document software functionality
Integrate new systems with existing systems
Evaluate and identify innovative technologies for implementation
Project planning and Project management
Maintain standards compliance
Implement new software platforms work with third party vendors
Document and demonstrates solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code
Improve operations by conducting systems analysis, recommending changes in policies and procedures
Protect operations by keeping information confidential
Provide information by collecting, analyzing, and summarizing development and service issues
Accomplish engineering and organization mission by completing related results as needed
Develop software/system solutions by studying information needs; conferring with users; studying systems flow, data usage and work processes; investigating problem areas; following the software/system development lifecycle.
Produce specifications and determine operational feasibility
Document and maintain software functionality
Serve as a subject matter expert
Comply with project plans and industry standards *
Requirements: Software Engineer top skills & proficiency:
Proven work experience in software engineering and/or Database Management
Firsthand experience in implementing and maintaining software/system applications
Firsthand experience in Relational Databases, SQL and etc.
Knowledge of ERP Systems
Experience with test-driven development
Ability to document requirements and specifications
Familiarity with software/system development methodology and release processes
Installing and configuring operating systems and application software
BS degree in Computer Science or relevant degree in Information Systems
Proficient in SQL Queries, stored procedures and working with in relational databases like SQL Server, MySQL, and ERP Systems
Analytical & Problem-Solving Skills
Ability to Learn Quickly
Team Player
Project Management
Written and Verbal Communication
Customer-Oriented
Analysis
General Programming Skills
SharePoint, MS Dynamics, HTML, and other related software a PLUS
Show more "
3581058800,DATA ENGINEER (i360),i360,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-i360-at-i360-3581058800?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=mY2Iha7yLpI1luDcAOoU1g%3D%3D&position=19&pageNum=2&trk=public_jobs_jserp-result_search-card,4098,"Description




i360, where The Data is the Difference, is the leading data and technology provider for those advancing a free and prosperous society through the campaign, nonprofit, and advocacy communities. i360 is a dynamic workplace sitting at the intersection of public policy, technology, and business, and is seeking team members who are excited about building the next generation of political technology.




i360 is seeking a mid-level Database Engineer to join its data engineering team. With your technical expertise, you will design, implement, and improve processes, procedures, and automation for all database-centric areas. You will tune our relational database systems and NoSQL systems for performance and reliability. You are responsible for building tools and scripts to monitor, troubleshoot and automate our systems. You propose test plans and interface with other teams, developers, and application owners to arrive at optimal solutions. Successful candidates will solve problems unique in scale and concept in the pursuit of new and original features. So, bring your ingenious mind, great team spirit and excellent communication skills to this great opportunity at i360.




What You Will Do In Your Role




Work with senior engineers and architects on the team to build and test SQL and NoSQL database solutions
Proficient with programming languages such as Python, Java, Spark, or Scala
Proficiency with relational database concepts & query optimization
Experience working with cloud platforms such as AWS or Azure
Experience with building data pipelines and implementing ETL processes
Perform code reviews and QA data imported by various processes
Investigate, analyze, correct and document reported data defects
Create and maintain technical specification documentation
Communicate effectively with stakeholders and collaborate with cross-functional teams




What You Will Need




BA/BS degree in Computer Science, Computer Engineering, Statistics, or other Engineering disciplines
4+ years of experience with relational database(s)
4+ years of coding experience in Python and/or Java




What Will Put You Ahead?




Knowledgeable on NoSQL, Columnar and/or Graph databases such as Elasticsearch, neo4j, Redshift, & Snowflake
Knowledgeable on architecting, developing, and maintaining solutions in AWS
Working knowledge of UNIX/Linux platforms
Familiarity with streaming services & event buses




Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.




At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.




Equal Opportunity Employer, including disability and protected veteran status.




Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test.




This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf




Show more "
3574008497,Data (Databricks) Engineer,Prismagic Solutions Inc.,2023-04-17,https://www.linkedin.com/jobs/view/data-databricks-engineer-at-prismagic-solutions-inc-3574008497?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=povVDQowUw88xzCUXKWzgA%3D%3D&position=20&pageNum=2&trk=public_jobs_jserp-result_search-card,1176,"Elite Databricks team within one of the world's most fascinating organizations is looking to scale and add a Senior Databricks Engineer. This role is for someone who highly skilled in engineering and development with the Databricks platform across multiple cloud (AWS, Azure, GCP) environments. You will design, build and maintain data pipelines using Databricks extensively and must be able to adapt to rapidly changing environments. Responsibilities: -Design, build and maintain data pipelines using Databricks -Must be able to fully envision and understand requirements and build/design solutions accordingly. -Monitor, troubleshoot and maintain data pipelines -Improve/optimize data pipelines for efficiency and performance Qualifications: -Must have extensive experience working with Databricks -Data Engineering and Data pipeline experience required -SQL, Python development -Spark, Hadoop and Big Data tech experience -Multi Cloud Experience (AWS, GCP, Azure) This opportunity is perfect for someone who is looking for a team of elite Databricks specialists. Room for growth and rapid advancement is NEEDED for this person to be successful- sky is the limit.
Show more "
3582340050,Software Engineer I,Transcat,2023-03-29,https://www.linkedin.com/jobs/view/software-engineer-i-at-transcat-3582340050?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=kn%2FKV76%2BMeo4cCvdz%2F9QLw%3D%3D&position=21&pageNum=2&trk=public_jobs_jserp-result_search-card,3430,"Who we are—




Transcat is a dynamic, innovative, growing company that has been recognized as the leading calibration and compliance services provider in North America and beyond. With over 1,000 employees—in technical, consulting, operational, sales, finance, and corporate roles—we have stood the test of time by delivering on our Trust in Every Measure promise to our customers in vital industries, including life sciences, aerospace, defense, energy, and utilities. We fulfill this promise through our employees, who live Our Values every day, the Transcat Way. Our employees are at the center of the rewarding, challenging, and life-changing work we do for our customers and those they serve. Are you ready to join a company where the work you do makes a difference, and where you can grow in your career?




Here’s what Transcat has to offer—




Work that matters
A values-based culture where people care about each other and the work they do together
Flexibility
Training and development to accelerate learning and career advancement
Competitive compensation and benefits, including paid time off, health insurance, tuition reimbursement, retirement, stock purchase plan, and MORE!




The software engineer I position will design, develop and debug desktop/web applications that function in a hybrid environment.
Participate in all aspects of the application development life cycle, including
Business requirements translation
Technical design
Test case creation (unit test)
Coding/development
Unit testing
Debug/troubleshooting
Peer-review
Deployment
Post-deploy support
Create clear and accurate technical documentation
Strong discipline of accountability and task management




Required Skills




Working knowledge of one or more object-oriented languages
Be motivated to continue to learn new skills
Have strong interpersonal skills to facilitate working within a team
Effective verbal and written communication skills
Effective time management skills




Qualifications




Bachelor’s degree in Software Engineering, Computer Science or related degree
Experience with PHP, HTML5, CSS, Javascript, Java, C#, SQL/TSQL
Experience developing stateless web applications, RESTful web services and APIs
Experience with AWS environments and SDKs including S3, SNS, SQS, SES, ElasticCache, Lambda, CloudSearch
Understanding of source control practices with TFS and Git
Experience with databases, including working knowledge of MS SQL, PostgreSQL and MongoDB
Experience with transactional web development, RESTFul APIs, ecommerce or related applications
Excellent communication skills
MINIMUM 3+ years of software development experience




Equal Opportunity and Non-Discrimination




Transcat is an equal-opportunity employer and prohibits discrimination on the basis of any protected status. All qualified applicants will receive consideration for employment without regard to age, color, creed, disability, domestic violence victim status, gender identity, genetic predisposition or carrier status, marital status, national origin, pregnancy, race, religion, sex, sexual orientation, status as a protected veteran or as a member of any other protected group or activity.




We will make reasonable accommodations for personnel with disabilities to enable them to perform the essential functions of this position unless doing so poses an undue hardship on the company or a direct threat to health or safety.
Show more "
3566953589,Data Engineer (All Levels),Noblis,2023-04-17,https://www.linkedin.com/jobs/view/data-engineer-all-levels-at-noblis-3566953589?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=NYq6%2Brlw4Oy0hWT%2BhD%2B9zw%3D%3D&position=22&pageNum=2&trk=public_jobs_jserp-result_search-card,6808,"Responsibilities

Noblis is seeking to hire a Data Engineer (All Levels) in Reston, VA

As a Data Engineer with our team, you will be working with our federal clients to rapidly develop innovative solutions for our clients’ immediate mission challenges. In your role, you will work with a team of developers, data scientists, SMEs, and cyber analysts to design, develop, build, and analyze data management systems. You will be asked to analyze our client’s challenges and provide solutions by identifying and applying new tools and technologies to help design new data repositories. Working alongside our team and cyber analysts, you will perform data engineering to identified data sets. As a data engineer your responsibilities will include designing, developing, optimizing, and maintaining data architecture and ETL pipelines.

On Our Team, You Will

Develop robust data platforms for analytics platforms.
Develop and perform ETL on large datasets.
Develop data visualizations to showcase valuable insights extracted from large volumes of data.
Create and optimize data pipeline architectures.
Support infrastructure and data querying processes.
Work in an Agile environment to rapidly iterate with our clients.

Qualifications

Required Skills:
Programming knowledge in some of the following languages: Python, SQL, Java.
Database experience with some of the following products: PostgreSQL, MySQL, Oracle, MongoDB
Experience working with Docker, Kubernetes
Proficient with git and pull request workflows.
Experience performing extract, transform, load (ETL) development for data pipelines (specifically cyber data).
Ability to perform API service development.
Bachelors degree in a related field.
Experience Levels:
Entry-Level: Bachelors degree + 0-1 years of experience.
Junior-Level: 1-2 years of professional work experience.
Mid-Level: 2-4 years of professional work experience.
Senior-Level: 3-5 years of professional work experience.
Expert- Level:5+ years of professional work experience.
Compensation will vary based on level.
Ability to obtain TS clearance with Polygraph.

Desired Skills And Knowledge

Familiarity with the cyber domain
Experience with open-source cyber data sets such as Shodan and Censys.IO
Experience working in cloud environments.
Programming knowledge of: Java, JavaScript.
Experience querying databases (SQL, Hive).
Experience working with data platforms like Hadoop and Spark.
Experience working with cloud providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure.


#hotjobs

Client Engagement

Learn about the Noblis business development lifecycle, processes, tools, and account structure
Contribute specialized domain or technical content to proposal sections or client white papers
Build a productive relationship with your client and understand their structure and goals

Overview

At Noblis we recognize and reward your contributions, provide you with growth opportunities, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, and work-life programs. Our award programs acknowledge employees for exceptional performance and superior demonstration of our service standards. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in our benefit programs. Other offerings may be provided for employees not within this category. We encourage you to learn more about our total benefits by visiting the Benefits page on our Careers site.

Salary at Noblis is determined by various factors, including but not limited to, the combination of education, certifications, knowledge, skills, competencies, and experience, internal and external equity, location, and clearance level, as well as contract-specific affordability and organizational requirements and applicable employment laws. The projected compensation range for this position is provided within the posting and are based on full time status. Part time staff receive a prorated salary based on regularly scheduled hours. The estimated minimum and maximum displayed represents the broadest range for this position (inclusive of high geographic and high clearance requirements), and is just one component of Noblis’ total compensation package for employees.

With the continuing impacts of COVID-19 around the world, we are taking action to protect the health and well-being of our colleagues and maintain the safety of the communities where we operate. Noblis seeks an environment free from COVID-19 and prefers all employees to be fully vaccinated for COVID-19. Attestation of vaccination status will be required for employment with Noblis. Unvaccinated employees may be subject to additional health and safety requirements to include any federal, state and/or client restrictions.

Noblis and our wholly owned subsidiaries, Noblis ESI , and Noblis MSD tackle the nation's toughest problems and apply advanced solutions to our clients' most critical missions. We bring the best of scientific thought, management, and engineering expertise together in an environment of independence and objectivity to deliver enduring impact on federal missions. Noblis works with a wide range of government clients in the defense, intelligence and federal civil sectors. Learn more at Noblis -About Us

Why work at a Noblis company?

Our employees find greater meaning in their work and balance the other things in life that matter to them. Our people are our greatest asset. They are exceptionally skilled, knowledgeable, team-oriented, and mission-driven individuals who want to do work that matters and benefits the public. Noblis has won numerous workplace awards . Noblis maintains a drug-free workplace.

Noblis is an Equal Opportunity Employer. Employment decisions are made without regard to race (as well as because of or on the basis of traits historically associated with race, including hair texture, hair type, and protective hairstyles such as braids, locks, and twists), color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, pregnancy, childbirth, lactation and related medical conditions, genetic factors, military/veteran status, or other characteristics protected by law.

Noblis is committed to the full inclusion of all qualified individuals. As part of this commitment, Noblis will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact employee-relations@noblis.org .
Show more "
3556444313,Data and Analytics Engineer,Extend Information Systems Inc.,2023-04-06,https://www.linkedin.com/jobs/view/data-and-analytics-engineer-at-extend-information-systems-inc-3556444313?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=VUyqXhRggkaGJ%2BGDiXasIQ%3D%3D&position=23&pageNum=2&trk=public_jobs_jserp-result_search-card,1209,"Hi,

Naveen, this side I have a new an excellent opportunity for you. This opportunity is a Fulltime position as Data and Analytics Engineer. Please have a look at the job description below and let me know if you or someone you know is interested in this role. You can mail me at naveen@extendinfosys.com.

Job Title Data and Analytics Engineer

Location Remote

Job Type Fulltime

Job Description

Role and Responsibilities
Draw and set up distributed Search Platform with Indexer, Engine and WebApp Nodes
Create functional and technical design.
Ability to communication and collaborate with various teams and vendors.
Participation in Agile development activities Scrum, Kanban.
Ability to work on CI/CD tools and technologies
Ensure coding, testing, debugging and implementation activities completed as required.

MUST HAVE Skill sets

Text Mining
Text Processing
Natural Language Processing
Data Science
Windows Server Management
Data Analyst
AWS experience

Thanks and Regards,

Naveen Shukla |Technical Recruiter| Extend Information Systems

Cell: (571) 547-2799

Email: naveen@extendinfosys.com

Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA - 20166

Web: www.extendinfosys.com
Show more "
3575812118,Software Engineer - Early Career,Lockheed Martin,2023-03-29,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3575812118?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=Mufhy72aoIgeanIq4hL2BA%3D%3D&position=24&pageNum=2&trk=public_jobs_jserp-result_search-card,4926,"Description:At Lockheed Martin, we believe that by applying the highest standards of business ethics and forward-thinking, everything is within our reach – and yours as a Lockheed Martin employee. We value your unique skills and expertise, and we aim to give back continuously by offering a wide variety of benefits and amenities to help our employees live flexible, balanced, and fulfilling lives at, and outside of, work.




Lockheed Martin Rotary and Mission Systems has an opportunity in Syracuse, NY for an entry level Software Engineer. As a new hire on our Software Engineering team, you can help us take on the world’s most important and complex challenges by performing limited scope assignments with support of your team. In this important role, you will collaborate with a diverse team of technical professionals. The successful candidate will be flexible, motivated, dedicated, detail-focused, team-oriented, and capable of multi-tasking.




Position Responsibilities Performed In a Supportive/assistive Capacity




Designs, modifies, develops, writes, and implements and integrates software programming applications for target system using agile methods.
Writes source code for new applications, and/or generates and enhances code for existing applications.
Performs code reviews, optimizes algorithms, and models and conducts experiments to ensure the functionality and performance of products or solutions.
Plans, conducts, and coordinates software development activities.
Applies the appropriate standards, processes, procedures, and tools throughout the development life cycle.
Applies knowledge of computer hardware and software, subject matter to be programmed in business/mission applications, information processing techniques used, and information gathered from system users to develop software.
Corrects program errors, prepares operating instructions, compiles documentation of program development, and analyzes system capabilities to resolve questions of program intent, output requirements, input data acquisition, programming techniques, and controls.
Ensures program specific software standards are met.




Basic Qualifications




BS in Engineering STEM field
Academic or professional experience using Java, C++, C# or similar programming languages
Security Clearance required Prior to Start: Interim Secret




Desired Skills




Basic understanding of Agile Software Development
Academic or professional experience/understanding of Object Oriented development
Operating systems development experience with Linux, including shell scripting, in academic or professional environment
Basic understanding of Software configuration management




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: Secret




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: 4x10 hour day, 3 days off per week




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: RMS




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3572303657,Mid-Level Data Engineer,"Technomics, Inc.",2023-04-19,https://www.linkedin.com/jobs/view/mid-level-data-engineer-at-technomics-inc-3572303657?refId=r0kREbTLeoDV8QWeFvu3mg%3D%3D&trackingId=yyXBIjIkNnnsTIMjLGUcnA%3D%3D&position=25&pageNum=2&trk=public_jobs_jserp-result_search-card,5421,"Overview

Technomics is a growing employee-owned, decision analytics company that specializes in cost and economic analysis to facilitate better decisions faster. We enable a wide range of clients across the Federal government, from senior level policy makers to program managers, to choose smartly, buy effectively and operate efficiently. We deliver practical, credible and defensible results offering actionable insights by applying data-driven and analytics-based approaches in combination with multidisciplinary talent, subject matter experts, and tangible and repeatable assets in the form of databases, models, approaches and techniques.

Our data engineers have the knowledge, skills, and initiative to deliver timely and innovative data management solutions to our clients. You will function as part of high-performing project team that is responsible for ensuring that organizations throughout the federal government are able to leverage their data to improve both their day-to-day operations and decision-making power. Our data engineers are responsible for shaping the future of analytics for your clients by identifying new data sources, modeling techniques, and ways of leveraging that data.

Our employee-owners pride themselves on their ability to apply deep analytical rigor and innovative thought that assist clients in understanding and solving a myriad of challenging resource planning and management problems.

This position is based out of Arlington, VA.

Responsibilities

Develop a thorough understanding of the purpose of particular assignment, the overall scope, the required level of detail, the customer(s) of the eventual work product(s) and any other situational awareness that warrants consideration with guidance from Senior level analysts
Identify potential data sources, both within and outside of the host organization, and work with senior analysts and subject matter experts to devise a strategy for collection
Gain a thorough understanding of modeling techniques that can be used to solve client problems, and structure data management processes accordingly to support
Prepare data for analysis via cleansing, normalization, and manipulation (either directly or supervising junior level analysts)
Implement the methodology or approach to address a particular problem. This could require development of a custom database or ETL process
Prepare written documentation (e.g., technical reports, memoranda and presentations) describing the data and normalization techniques used, defend results produced and associated insights and recommendations
Verbally explain/defend in-process and completed work to colleagues, management and stakeholders in informal and formal settings
Develop proficiency in various software applications to support the development of robust data management solutions in an efficient, repeatable, intuitive, and transparent manner
Continue to develop proficiency in a variety of software tools and scripting languages for use in applied analysis or for situational awareness
Train/guide less experienced team members
Lead and deliver projects by: developing project plans; leading technical performance; managing, developing and inspiring project staff; managing and reporting project technical and schedule performance; and managing customer and other stakeholder expectations
Apply data storage/data architecture best practices to client problems

Qualifications

Minimum of 4 years demonstrated experience working through various stages of the data management lifecycle (data engineer, economist, data analyst, cost analyst, data scientist, actuary, investment analyst)
Minimum 4 years of experience working with a high-level programming language such as R, Python, Stata, SAS, Matlab
Intermediate knowledge of relational database design and applications, to include SQL
Significant experience with the data management lifecycle, to include data source identification, collection, cleansing and normalizations, and modeling
Minimum 2 years of experience creating professional visualizations to communicate advanced analyses to senior level stakeholders (using software such as Tableau or Power BI
Advanced proficiency using Microsoft Excel
Success communicating effectively (i.e., clear, organized, succinct, knowledgeable and convincing) verbally and in writing
Success independently representing Business Support in substantive technical-related interactions with customers and other stakeholders
Success managing day-to-day customer relationships at peer customer levels
Interest in earning industry recognized certifications

Preferred Qualifications

Corporate experience using git productivity tools, including use of a collaborative environment such as GitHub or GitLab
Experience processing and cleaning data using R or Python
Practical experience in text analytics and natural language processing techniques
Experience with VBA in Microsoft Excel and Access
Experience developing complex relational databases (i.e., SQL) and a working knowledge of database concepts
Experience working with modern database formats (i.e., NoSQL), including implementation
Familiarity with DoD weapon system acquisition process preferred, not required

We are an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, age, marital status, disability or veteran status.
Show more "
3545832489,Data Engineer,Stori,2023-03-29,https://mx.linkedin.com/jobs/view/data-engineer-at-stori-3545832489?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=NHeKvLkuIDAfhkbVuW7fCw%3D%3D&position=1&pageNum=3&trk=public_jobs_jserp-result_search-card,4556,"About Stori

Stori is a fast-growing, venture-backed financial technology company, on a mission to democratize credit access for 400 million underbanked LatAm consumers. Stori currently operates in Mexico and has a global team with offices in Arlington Virginia, Mexico City, and Asia. We have quickly made our mark as one of the top digital banks in Mexico with more than two million applicants for our credit card product since launching.

Stori is one of the top-funded startups in the region with US$250 million raised to date. We are backed by top global venture capital funds, such as GGV Capital, GIC, Lightspeed Venture Partners, General Catalyst, Goodwater Capital, Mexico's Tresalia Capital, Vision Plus Capital, BAI Capital and Source Code Capital; who have successfully invested in startups such as Affirm, Airbnb, Alibaba, Stripe, and TikTok.

Stori has a standout founder team among fintechs, leveraging 100+ years of accumulated experience in consumer finance, banking and technology across Mastercard, Intel, Capital One, Morgan Stanley, GE Capital, and HSBC in the U.S., Mexico and Asia. The team has launched and managed many multi-million-customer credit card products globally, providing a wide breadth of experience and knowledge to our team.

We welcome diversity of background, experience and thinking. Storians are passionate about our mission and take pride in the products we build. Our culture thrives off of a flat structure and an inclusive environment where all of our employees can be their authentic selves, with boundless opportunities for professional growth.

The Role

Stori's Data team's primary objective is to build easy-to-use, trustworthy, and reliable data products to serve the analytical needs across different product and business functions.

Data Engineers in Stori solve problems that get in the way of doing analysis. In this role, you are essential to our data strategy in reducing burdens associated with our data models and analytical tools and platforms. You will leverage cutting edge data pipeline and data management tools to innovate, design, build, and maintain well-managed data pipeline and data infrastructure solutions and capabilities.

You will

Partnership with engineers or vendors to explore various upstream data sources that can be leveraged for analytics or ML modeling.
Work closely with data analysts and data scientists to understand the analytical data needs.
Design, build and launch extremely efficient and reliable data pipelines to transform the raw data into well-designed data products with sophisticated business logics.
Work closely with internal analysts and stakeholders to translate the analytical data needs to technical data pipeline solutions.
Educate your engineer, program lead, or operation partners: Use your data and analytics experience to 'see what's missing', identifying and addressing data gaps in their existing system logging and processes.
Implement automated workflows and routines using workflow scheduling tools.
Solve data issues and perform root cause analysis to proactively resolve product and operational issues.


Requirements

A bachelor degree or foreign equivalent in Computer Science, Electronic Engineering, Mathematics, Information System, or a related quantitative analytical field.
2+ years of Python development experience.
2+ years of SQL experience.
2+ years experience with Data Modeling.
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders.
Experience working with cloud or on-prem Big Data analytics platform (i.e. AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).


Strong preference

Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning.
Experience with notebook-based Data Science workflow.
2+ years of experience with workflow management engines (i.e. Airflow, AWS Step Functions).
2+ years experience in custom ETL design, implementation and maintenance.


What We Offer

Make a positive impact on the lives of our customers via financial inclusion
Professional development opportunities
International exposure & work experience
Competitive salaries
Flexible schedule including remote work
Generous vacation
English classes
Mental health support
Extended maternity and paternity leave
Yoga and exercise classes
Company swag
Legally required benefits
Show more "
3571785328,Data Analytics Engineer,State Farm,2023-03-26,https://www.linkedin.com/jobs/view/data-analytics-engineer-at-state-farm-3571785328?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=XUduFz72uwEoTGZFxuEE%2BQ%3D%3D&position=2&pageNum=3&trk=public_jobs_jserp-result_search-card,2818,"Job Description Overview

We are not just offering a job but a meaningful career! Come join our passionate team!

As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.

We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!

Visit our Careers page for more information on our benefits , locations and the process of joining the State Farm team!

Responsibilities

As a Data Engineer you will:

Work closely with data scientists and business experts to develop modeling solutions for actuarial and underwriting business problems
Building and maintaining data pipelines for the development, implementation, execution, validation, monitoring, and improvement of data science solutions
Establish business domain knowledge for State Farm data sources
Investigate, recommend, and initiate acquisition of new data resources from internal and external data sources
Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend quantitative analytic deployment solutions

Qualifications

We are looking for Candidates who have-

Required Skills

Experience in programming languages such as Python, SAS, R
Experience with any opensource database such as PostgreSql, MySQL. Etc.
Experience with developing solutions on AWS or other distributed compute platforms.
Ability to learn and adopt new technologies and languages
Critical thinking skills to challenge current thinking and apply right technology to solve problems.
Bachelor’s Degree in Computer Science, Software Engineering, or related field

Preferred Skills

Experience with the Model Building Lifecycle
Experience with CI/CD systems, preferably with GitLab, Jenkins, or AWS Code Deploy.
Experience using deployment automation technologies, preferably Terraform
Experience with P&C Insurance Data
Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity*****

Office Location: Corporate office located in Bloomington, IL, OR State Farm Hubs: Richardson, TX; Dunwoody, GA; Phoenix AZ.

SFARM

#JOA

S


Show more "
3580000717,Senior Data Platform Engineer,KingsIsle Entertainment,2023-04-22,https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-kingsisle-entertainment-3580000717?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=yul3GR%2BvxJkXQEoHcZw%2BHQ%3D%3D&position=3&pageNum=3&trk=public_jobs_jserp-result_search-card,2235,"KingsIsle Entertainment is currently looking for a Senior Data Platform Engineer to join our Data Warehouse/Analytics team. We are the creators of the popular and critically acclaimed Wizard101 and Pirate101, which together have garnered over 69 million registered players to date, and are two of the fastest growing websites in the U.S. This is an opportunity to be a part of a rapidly growing and highly successful entertainment company.

This job is Telework in Plano, Texas. Candidates can be located within any TX city.

Responsibilities

Work with a diverse and experienced team of developers in a creative and fast paced environment
Develop machine learning and data analytics platform components
Develop and maintain ETL (Extraction, Transformation and Loading) scripts and processing components for our multi-terabyte (Big Data) distributed data warehouse environment
Build scalable data pipelines, transform data, manage repositories, expose APIs
Develop and maintain reports for KingsIsle’s Game Design, Marketing, Operations, and Executive teams using advanced data visualization tools


Requirements

A Bachelor’s degree in Computer Science or Engineering
At least 5 years professional data engineering experience or software development experience using a structured programming language (C, C++, Perl, Python or JAVA)
Working knowledge of SQL
Experience using Linux shells and utilities
Good communication skills
Ability to contribute to and work within schedules that are assigned
Knowledge of software design and software engineering methodologies


Preferences

Database development experience in Vertica and MySQL
Interest in Big Data, Analytics or Statistics
Experience with Google Cloud Platform (BigQuery, Firebase)
Knowledge of XML, JSON and other data exchange standards
Experience with Excel, Tableau, Power BI and other reporting tools
Knowledge of distributed system development


Benefits

Competitive compensation
Unlimited PTO
Company fun days and parties
Free game swag (such as plushies)
Medical, dental, and vision care benefits
401(K) with employer match
Flexible Spending Account options
Lifestyle Benefits Program
Employee Learning Program
Diverse and welcoming teams from all backgrounds
Show more "
3556130862,Data Science Engineer,Glide,2023-04-06,https://www.linkedin.com/jobs/view/data-science-engineer-at-glide-3556130862?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=UURB91hOlydBxSg3R892KQ%3D%3D&position=4&pageNum=3&trk=public_jobs_jserp-result_search-card,1797,"Summary

Glide is looking for a data engineer to help improve our data infrastructure and enhance our reporting, analytics, and automated workflows.

You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for sales, marketing, engineering, and growth. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing existing data systems and building them from the ground up.

You will support our software engineers and data analysts and will ensure the data architecture is consistent throughout ongoing projects. You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

You Will

Work with BigQuery, Firestore, Postgres, Stripe, Mixpanel, and Hubspot.
Collaborate with sales, marketing, engineering, and growth teams to build data pipelines.
Build reports and tools to extract insights from the data.

This job might be perfect for you if have…

Previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of Python, R, and SQL
Knowledge of the GCP data stack (DataProc, Dataflow, Data Studio, BigQuery)
Extra points for Kafka, PubSub or Kinesis. Debezium and/or other streaming ETL.
Knowledge of ETL tools like Airflow/AirByte/FiveTran.
Degree in Statistics, CompSci, Math or IT a plus but not required

Definitely mention if you have...

Any open source work you’d like to show off.
Experience with Retool
Experience with Typescript
Show more "
3579987384,Junior Data Scientist/ Data Engineer,Compunnel Inc.,2023-04-21,https://www.linkedin.com/jobs/view/junior-data-scientist-data-engineer-at-compunnel-inc-3579987384?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=%2F%2F%2FrfyNrgsMEBlTTnumD1w%3D%3D&position=5&pageNum=3&trk=public_jobs_jserp-result_search-card,1419,"Skills




1+ years GIT




1+ years Linux




1+ years J-Son




Strong Organizational And Project Management Skills




Python – basic to intermediate




Azure cloud – basic




Databricks – basic or ability to quickly learn how to work within Databricks




Jupyter notebooks – basic to intermediate




Git version control – basic to intermediate




Linux, basic comfortability to work on the command line




Ability to edit JSON files




Good interpersonal and communication skills




1 or more years of experience working within a highly technical team




Job Description




Schedule and execute the templated jobs required to optimize pricing for each grocery category




Move data files from cloud environment to on-prem servers




Edit existing code, where necessary, to establish pricing specific business rules provided by Kroger stakeholders




Load data files into a Dash based interactive dashboard used by EPP and Kroger stakeholders to facilitate pricing optimization




Running batch scripts




Collaborate with 84.51° EPP consultants to appropriately deploy pricing optimization solution within each scheduled grocery category; attend meetings, in-take business requirements, translate requirements to code, align on and complete action items




Adhere to stringent quality assurance and documentation standards (e.g., git, markdown, Confluence)




Education: Bachelors Degree
Show more "
3541779976,Data Center Engineer,Extend Information Systems Inc.,2023-03-27,https://www.linkedin.com/jobs/view/data-center-engineer-at-extend-information-systems-inc-3541779976?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=AUtfhMU5ewiVlVWvf5akcA%3D%3D&position=6&pageNum=3&trk=public_jobs_jserp-result_search-card,940,"Hi,

Naveen, this side I have a new an excellent opportunity for you. This opportunity is a Fulltime position as Data Centre Engineer. Please have a look at the job description below and let me know if you or someone you know is interested in this role. You can mail me at naveen@extendinfosys.com.

Job Title Data Centre Engineer

Location Pheonix, Arizona

Job Type Fulltime

Job Description

Need resource to be onsite in Data Center- Phoenix, Arizona

Experience with Server configurations, data center services (rack/stack, touch services)

Experience on working Application layer hosted on Linux/Unix platform.

Experience on Virtualization & Hypervisors (ESXi & AHV)

Vendor Management

Thanks and Regards,

Naveen Shukla |Technical Recruiter| Extend Information Systems

Cell: (571) 547-2799

Email: naveen@extendinfosys.com

Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA - 20166

Web: www.extendinfosys.com
Show more "
3576726514,Python Data Engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/python-data-engineer-at-diverse-lynx-3576726514?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=iXHjZij223yB9peXzyhk1Q%3D%3D&position=7&pageNum=3&trk=public_jobs_jserp-result_search-card,532,"Job Description




Job profile: Python Data Engineer




Location: Austin(TX)/ Sunnyvale(CA)




Job type: FTE




Job Description




Role : Python Data Engineer




# of positions : Austin - 2 currently and might need one in Sunnyvale, CA in Jan/Feb




Skill set : Python based Data engineer




Python- with Big Data experience
Ability to Process and transform large data and and integrate with enterprise systems
Good to have Machine learning application experience
Quality of engineering skills is primary criteria
Show more "
3576578427,Data Engineer Staff - Remote,Lockheed Martin,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-staff-remote-at-lockheed-martin-3576578427?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=uz%2F%2ByW94Yz2diAdscJJTmA%3D%3D&position=8&pageNum=3&trk=public_jobs_jserp-result_search-card,1937,"The coolest jobs on this planet… or any other… are with Lockheed Martin Space. Lockheed Martin is a pioneer, partner, innovator and builder. Our amazing people are on a mission to make a difference in the world and every single day we use our skills and experiences to create, design and build solutions to some of the worlds’ hardest engineering problems.




This position is for a experienced Data Engineer within the Lockheed Martin Space System’s Business IT&DE Organization – Space CDAO. This is a great opportunity for someone who is looking to work in a fast-paced and evolving Data & Analytics organization where the latest data strategies will be developed and applied. We are looking for someone with strong experience in data, architecting, designing, and delivering full-stack data solutions across the entire data processing pipeline.




The work location for this position is virtual.




Duties And Responsibilities Include, But Are Not Limited To




Develop, design and deliver comprehensive, full-stack solutions across the entire data processing pipeline that meet functional requirements, minimizes technical debt, and adheres to data governance standards in support of a given initiative.
Participate in the strategy and development of architecture and technical roadmaps.
Establish architectural/design vision and direction and provide oversight, advice, and guidance to developers, users, customers, and stakeholders to ensure viability of proposed data solutions.
Facilitate design reviews and governance to ensure adherence to data engineering standards and consistency across Data & Analytics organization, product teams, and citizen developers.
Exercise solution design leadership on initiatives, driving alignment between Enterprise, Space, and Technical architecture to meet business needs.
Facilitate large groups of diverse stakeholders to obtain consensus on target solution.
U.S. Citizenship
Show more "
3562452848,Data Engineer,Pyx Health,2023-04-12,https://www.linkedin.com/jobs/view/data-engineer-at-pyx-health-3562452848?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=wt5YDOt2NTDHuGUKSaEKHA%3D%3D&position=9&pageNum=3&trk=public_jobs_jserp-result_search-card,3619,"Loneliness is a devastating and pervasive problem, fraught with steep social, personal, and economic costs, and Pyx Health is looking for a




Senior Data Engineer with the ability to hit the ground running to help solve it. We're changing the landscape of care, resulting in improved




health. To do so requires ingesting, processing, and transforming large amounts of information. The ideal candidate will be dedicated and willing




to take ownership of the data processing systems, learn quickly, and work in a team environment with humility and compassion.




ONLY CANDIDATES RESIDING IN THE USA MAY APPLY.




Responsibilities




Help make architectural standards a reality and always look for ways to improve.
Manage the hydration and data hygiene of a large data warehouse.
Eliminate technical debt across Database infrastructure; Build and optimize data ETL processes, and flat file ingestion mechanisms.
Maintain and improve existing stored procedures, views, and table schemas.
Administer large, high-volume production database environments.
Work closely with data analytics and development partners to create robust data solutions.
Strive to write code that is high quality and easily maintainable.
Work in a fast-paced, Agile environment where the requirements and needs are constantly evolving.
Keep up with new technologies when sensible and make improvements where possible.
Develop highly available database systems that can sustain operational and disaster scenarios specifically in multi-regional cloud




Required Skills




Is self-motivated, needs very little oversight, can take ownership of the product, and can deliver features on time.
Is humble, approachable, easy to work with and enjoys tackling complex problems.
Is constantly seeking challenges and takes pride in creating solutions that clients love.
Has experience ingesting files with large amounts of data efficiently and expediently using ETL pipelines.
Has experience with Cloud Computing, preferably with Azure using tools like Azure Data Factory, Azure Databricks, and Azure DevOps.
Possesses strong knowledge of Python and relevant data transformation libraries such as Pandas and PySpark.
Has strong SQL development skills including stored procedures, writing performance-optimized views, and query analysis.
Familiarity with version control systems, preferably Git or BitBucket.
Understanding of Data Architecture and how it affects different environments such as Application, Analytics, and Processing.




The Ideal Candidate Will Also Have




History of working in a high speed, aglie environment, using tools like JIRA, Conflucene, or other ticketing systems
Experience performing schema migrations across multiple environments.
Experience securing databases for compliance and security frameworks (e.g. HIPPA, HITECH, HITRUST, PCI, SOC).
Has experience administering RDBMS like SQL Server / MySQL / Oracle.
Interest or experience in developing AI models.
Knowledge of common health care file exchange formats.
Experience working in or with the healthcare industry or healthcare-related products that require compliance with HIPAA.




About Pyx Health




We solve some of the biggest problems in healthcare - loneliness and social isolation.




Pyx Health is a mobile solution that reduces loneliness and social isolation by connecting with your members outside of the traditional care




setting. By providing critical and timely interventions and addressing social determinants of health, we take care of health plan members when




they are most vulnerable, especially after a transition of care.
Show more "
3577530172,Data Engineer,Orange County's Credit Union,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-orange-county-s-credit-union-3577530172?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=zd09nGB6nWe09gsM68WDjw%3D%3D&position=10&pageNum=3&trk=public_jobs_jserp-result_search-card,5315,"Great Opportunity At Orange County's Credit Union

Must reside in the state of CA, AZ, NV or TX.

Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.

Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.

Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.

Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!

More About Orange County's Credit Union

Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.

As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.

Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!

Overview

Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.

Essential Functions

Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).

Technical Must Haves For This Role

5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.
Show more "
3573907482,Data Engineer,Patterned Learning AI,2023-04-17,https://www.linkedin.com/jobs/view/data-engineer-at-patterned-learning-ai-3573907482?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=3KoP11pIzAmLdL1pK7C9Lw%3D%3D&position=11&pageNum=3&trk=public_jobs_jserp-result_search-card,1346,"REMOTE (US/Canada Residing people only, with work permit)




Patterned Learning – Data Engineer , FULL-TIME, Salary $130K - $180K a year.




About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!




About The Job




Required Skills and Experience:




4+ years of experience in writing cloud-based software
Expertise with AWS data processing products (Kinesis, S3, Lambda, etc.) or comparable (Redis, Kafka, Google DataFlow, etc.)
Strong knowledge of relational DBs (Postgres, Snowflake, etc.) including SQL, data modeling, query tuning, etc.
Experience delivering software products built using Python
Experience using professional software development practices, including: Agile processes, CI/CD, etc)
Familiarity with distributed data processing frameworks (AWS Glue, Spark, Apache Beam, etc.)
Familiarity with modern data analysis, dashboarding and machine learning tools (DBT, Fivetran, Looker, SageMaker, etc.)




Special Benefits You Will Love




Flexible vacation, paid holidays, and paid sick days
401(k) with up to 2% employer match (no match)
Health, vision, and dental insurance.




Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
Show more "
3580603120,Data Engineer,HelloFresh,2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-at-hellofresh-3580603120?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=V9GtGni0IevHM3Fv3l9eXQ%3D%3D&position=12&pageNum=3&trk=public_jobs_jserp-result_search-card,2492,"Data Engineer, Hello Fresh

This role can be based out of the following facilities:

NYC, Newark, NJ, Totowa, NJ, Phoenix, AZ, Newnan, GA, Aurora, CO, Irving, TX and Grand Prairie, TX

Job Description

As a Data Engineer, you will work with the Fulfillment Planning Technology team to help build the next generation suite of internal tools that enable Planning and Operations teams to see and act quickly to changing business conditions. The Data Engineer will build scalable data pipelines, infrastructure and tools that power our products and services.

You will …

Work with analysts, engineers and planners to design, build, and maintain efficient, scalable and reliable data pipelines to support our business-critical needs in data ingestion, processing, and analysis

Develop and maintain efficient, scalable and reliable code in Python and SQL

Collaborate with other team members to troubleshoot, perform root cause analysis and optimize existing data pipelines and tools

Work with other team members to ensure effective tool integration with other systems and workflows

At a minimum, you have …

Bachelor's or Master's degree in Computer Science, Engineering or related field

2+ years of data engineering experience in Fulfillment, Logistics, Supply Chain, Production, or related field working with physical goods

Strong proficiency in Python and SQL

Experience working with distributed systems, building and maintaining pipelines using cloud technologies (AWS, Snowflake)

Experience in containerization and orchestration (Docker, Kubernetes, Airflow, GCP)

Startup experience a plus

You'll get...

Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 4 weeks & PTO policy, as well as paid holidays off
$0 monthly premium and other flexible health plans
Amazing discounts, including up to 75% off HelloFresh subscription
Flexible shift scheduling & advancement opportunities
Emergency child and adult care services
Snacks & monthly catered lunches
Collaborative, dynamic work environment within a fast-paced, mission-driven company


It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran.

New York Pay Range

$110,000—$137,500 USD

Colorado Pay Range

$99,200—$124,000 USD
Show more "
3555458031,IT Engineer (Data Engineer) JP00010552,RADISH CONSULTANTS,2023-04-05,https://www.linkedin.com/jobs/view/it-engineer-data-engineer-jp00010552-at-radish-consultants-3555458031?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=QieU5rvvUAtrdOhgVPX%2FSg%3D%3D&position=13&pageNum=3&trk=public_jobs_jserp-result_search-card,804,"Job Description

To research, evaluate, design, implement, and maintain system and product solutions, applying knowledge of engineering principles. To provide technical direction and engineering support for projects and infrastructure. Develop and maintain expert functional knowledge of evolving IT engineering industry technologies/competition, concepts and trends.

Data deliverables

Data integration framework (Data Lake, Data Warehouse),
Data quality control & monitoring
Performance optimization
Data modeling
Data profiling

Hands on data service/programming lang. experience

Databricks
PySpark
SQL
Synapse
Azure Data Factory
ADLS
Delta table

Agile Delivery - Azure DevOps/Boards, JIRA

Desired Test support, Spark Streaming, Testing, Azure Function & Logic App, Enable Data Security
Show more "
3569941227,Data Engineer,Perennial Resources International,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-perennial-resources-international-3569941227?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=Ou2sWCqJhcXjuCoW3hVJ6w%3D%3D&position=14&pageNum=3&trk=public_jobs_jserp-result_search-card,2267,"Hope all is well,

I have a full time Data Engineer Opportunity in Manhattan that may be of interest to you...

Responsibilities

Design, Build & Implement
Design & build batch and real-time data pipelines using some of the latest technologies available on Microsoft Azure, Google Cloud Platform and AWS
Implement large-scale data platforms to meet the analytical & operational needs across various organizations
Build products & frameworks that can be re-used across different use-cases in increase efficiency in coding and agility in implementation of solutions
Build streaming ingestion processes to efficiently read, process, analyze & publish data for real time need of applications and data science models
Perform analyses of large structured and unstructured data to solve multiple & complex business problems
Investigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess & advise
Understand business use cases to design engineering routines to affect the outcomes
Review & assess data frameworks & technology platforms with the goal of suggesting & implementing improvements on the existing frameworks & platforms
Understand quality of data used in existing use cases to suggest process improvements & implement data quality routines
Requirements

An Engineer interested in working in both streaming and batch processing environments
A tech-enthusiast excited to work with Cloud Based Technologies like GCP, Azure, Kubernetes
A doer who loves to produce meaningful analytic insights for an innovative, data-intensive products
Always curious about analytics frameworks and you are well-versed in the advantages and limitations of various big data architectures and technologies
Technologist who loves studying software platforms with an eye towards modernizing the architecture
Believer in transparency & communication
Tools

To Give You An Example Of Our Current Tools

Tools can be learned, so please don't shy away from applying if you're a general strong engineer.

Language: Python, Scala, Java, SQL
Streaming: Spark Streaming, Pub/sub, Kafka

Matt Hayes

Senior Technical Recruiter

PRI Technology

Denville, New Jersey 07834

Matt.Hayes@PRITechnology.com

973-668-9683
Show more "
3573168199,Data Engineer,Christensen Farms,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-at-christensen-farms-3573168199?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=PVF67kQzGKIX3E14qjRgiA%3D%3D&position=15&pageNum=3&trk=public_jobs_jserp-result_search-card,5532,"Christensen Farms is one of the largest, family-owned pork producers in the United States, marketing approximately 3 million hogs per year. Headquartered in Sleepy Eye, Minnesota, the company operates throughout the Midwest with facilities in Minnesota, Iowa, Nebraska, Illinois, and South Dakota. Christensen Farms owns four feed mills, manages 145,000 sows on 44 farms, and oversees more than 350 nurseries and grow-finish sites. The company employees nearly 1,000 people and maintains 1,500 contract partnerships.




The company is vertically integrated with a strong presence across the pork value chain - from farm to fork. Christensen Farms is the largest shareholder of Triumph Foods LLC, a producer-owned primary pork processing plant in St. Joseph, Missouri. In turn, Triumph Food members own 50 percent of Daily's Premium Meats, a specialty pork processor bacon and other premium pork products. Triumph Foods also holds a 50 percent partnership in Seaboard Triumph Foods, LLC of Sioux City, Iowa, a primary pork processing plant.




Position Overview & Responsibilities




Christensen Farms is looking for a self-motivated Data Engineer to join our Decision Science team. In this position, you will play a significant role in helping grow and modernize our analytics services. As part of the company's data operations, the data engineer will ensure accuracy in decision science and reporting as well as responsibility for collecting, managing, and converting raw data into information that can be interpreted by the business. You will design and develop data collection frameworks for structured and unstructured data including Data Pipelines, ELTs and ETLs to connect large datasets from a variety of sources. You will also engineer reports, dashboards, and visualizations using enterprise business intelligence tools.




This individual will need effective communication skills, communicating up, down and across the organization. This individual will also require the skill to multitask and manage simultaneous project activities that require innovative problem solving. Our ultimate goal is data accessibility to enable the organization to leverage data for performance evaluation and optimization.




Major Areas of Responsibility




Engineer and implement data models for dimensions and facts within the staging and warehouse layer of our enterprise data lake house.
Engineer and implement new and modify existing ETLs, data pipelines, and ELTs to move data from a variety of source systems, including structured and unstructured data, to fit into dimensional data models with large data sets.
Engineer schedules and orchestrations for batch and near real time data loads into the enterprise data lake house.
Troubleshoots, identifies issues and remediates data performance issues that come with developing, querying, and combining large data sets from a variety of sources.
Collaborate with a variety of business users to understand definitions of data, analytics use cases, and assist in building data governance and detailed data requirements.
Support and establish reports, dashboards, and visualizations using Power BI.
Responsible for executing simultaneous project activities that require innovative problem solving.
Provides 2nd level employee support for decision science services.
Research tools, disciplines and industry trends and recommend improvements for team or company consideration.
Other duties assigned within the scope, responsibility, and requirements of the job.




Education, Training And Experience Requirements




Minimum formal education required: Associates/Bachelor's degree in data and analytics or an IT related discipline or 4+ years of equivalent work experience.
2 or more years of experience in a corporate IT Team environment.
Exceptional technical skills required. Cloud based tools (examples include Power BI, Data Factory, Databricks, Snowflake, H20.ai, Excel, database management, etc.).
2-3+ years Programming/scripting experience (SQL and/or Python) and working knowledge of the software development life cycle.
2+ years of experience engineering within a data warehouse or related experience with dimensional data modeling.
2+ years of experience designing and developing ETLs with tools like Microsoft SSIS
Proven ability to gather detailed technical requirements to design and develop business intelligence report solutions from beginning to end.
Clear, concise, and precise written and verbal communication skills to effectively communicate with people of various backgrounds including non-technical audiences.
Ability to multi-task and to prioritize objectives, anticipate situations and take quick action.
Computer skills required: Working knowledge of Microsoft Office Suite including Visio
Strict adherence to company confidentiality and ethical standards.




Additional Desirable Skills/Qualifications




Certifications in any of the following are a plus: DASCA (Associate or Senior), IBM Certified Solution Architect, SAS Certified Big Data Professional, ITIL.
Enterprise application experience
Cloud technologies (Public, Private, Dedicated, Hybrid).
Experience in Level 2 and/or Level 3 employee support (Freshservice or similar ITSM tool).
Employee computing devices including Windows, Android, and iOS.
Experience working in an agricultural business, particularly the swine industry.
Hybrid work schedule at our Sleepy Eye Office Preferred, remote opportunities potentially available for the right candidate.




Job Posted by ApplicantPro
Show more "
3568557792,Data Engineer (Remote),Blue Orange Digital,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-remote-at-blue-orange-digital-3568557792?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=ubvphx1T0t%2BfqY1OmwLOhA%3D%3D&position=16&pageNum=3&trk=public_jobs_jserp-result_search-card,3560,"Company Overview:




Blue Orange Digital is a cloud-based data transformation and predictive analytics development firm with offices in NYC and Washington, DC. From startups to Fortune 500’s, we help companies make sense of their business challenges by applying modern data analytics techniques, visualizations, and AI/ML. Founded by engineers, we love passionate technologists and data analysts. Our startup DNA means everyone on the team makes a direct contribution to the growth of the company.




Position Overview:



Blue Orange is seeking a Data Engineer to join our team to help build up our data engineering practice. Our engineers require a diverse skill set including system administration, DevOps, infrastructure automation, data modeling, and workflow orchestration. Blue Orange builds enterprise data platforms and systems for a variety of clients, so this candidate should have experience with supporting modern data technologies. The ideal candidate will have experience with multiple data engineering technologies across multiple clouds and deployment scenarios. In particular, we’re looking for someone with experience with Azure DevOps, Databricks, and Python.



This is a full-time fully remote position. 



Core Responsibilities & Skills



Work with data teams to help design, build, and deploy data platforms in the cloud (Azure, AWS, GCP) and automate their operation.
Work with Azure DevOps, Azure Pipelines, Terraform, CloudFormation, and other Automation and infrastructure tools to build robust systems.
Work with Databricks, Spark, and Python with data orchestration, and ETL tools to build high-performance data pipelines.
Provide leadership in applying software development principles and best practices, including Continuous Integration, Continuous Delivery/Deployment, and managing Infrastructure as Code, Automated Testing across multiple software applications.
Support heterogeneous technologies environments including both Windows and Linux systems.
Develop reusable, automated processes, and custom tools.

Qualifications



BA/BS degree in Computer Science or a related technical field, or equivalent practical experience.
At least 6 years of experience building and supporting data platforms; exposure to data technologies like Azure Data Factory, Azure Synapse Analytics, Airflow, and Spark.
Experience with Cloud Data Platforms, like Snowflake and Databricks.
Advanced level Python, SQL, and Bash scripting.
Experience designing and building robust CI/CD pipelines.
Strong Linux system administration skills.
Comfortable with Docker, configuration management, and monitoring tools.
Knowledge of best practices related to security, performance, and disaster recovery.
Experience working in cloud environments, at a minimum experience in Azure and AWS.
Enjoys collaborating with other engineers on architecture and sharing designs with the team. 
Excellent verbal and written English communication.
Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment.

Bonus Points



Hold certifications for Azure DevOps, Azure Data Fundamentals, Databricks, and Snowflake.

Our Benefits Include:



Fully remote
Flexible Schedule
Unlimited Paid Time Off (PTO)
Paid parental/bereavement leave
Worldwide recognized clients to build skills for an excellent resume
Top-notch team to learn and grow with

Salary: $5000 - $6650 USD per month



Blue Orange Digital is an equal-opportunity employer.



Background checks may be required for certain positions/projects.



Show more "
3576976734,Data Engineer,"Wawa, Inc.",2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-wawa-inc-3576976734?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=7N0%2Bih8tauA8DpzOD0Nqfw%3D%3D&position=17&pageNum=3&trk=public_jobs_jserp-result_search-card,6070,"Job Description




Job Title: Data Engineer Location: Corporate




Department: Information Technology




Pay Band: Specialist




Job Summary:




The Data Engineer role designs and develops scalable data solutions using data integration tools and technologies. The individual utilizes big data computation, data platforms and storage tools to create prototype and data products. Conduct build and testing of data pipelines and solutions. Additionally, Data Engineer integrates, tests data pipelines with Advance Analytics and AI platforms. Must be proficient with multiple data engineering and integration tools such as Scala, Python, Spark, Snowflake etc. in an AWS environment.




Principal Duties:




Responsible for designing and implementing solutions for loading both structured and semi-structured data design into multiple target data systems.
Design, develop, optimize, and maintain data pipelines and processes that adhere to data integration principles and business goals.
Solve complex data problems to deliver insights that helps our business to achieve their goals.
Code, test, and document new or modified data systems to create robust and scalable applications for data analytics.
Ensure that data pipelines are scalable, repeatable, and secure, and can serve multiple users within the company.
Design and implement data ingestion techniques for real time and batch processes for structured and semi-structured data sources into Wawa’s data lake and data warehouse platforms.
Understand complex business requirements and propose end to end and simplified enterprise information architecture solutions.
Develop and implement data design methods, data structures, and modeling standards which work with multiple business intelligence tools.
Work closely with Analytics team and implement their self-service and analytics requirements.
Work with Data Science practitioners and developers to make sure that all data solutions are
Collaborate with Analytics team to build solutions that enable business analytics. Develop quality scalable, tested, and reliable data services using industry best practices.
Manage all activities centered on obtaining data and loading into a data lake environment.
Assess the suitability and quality of candidate data sets for the Data Lake.
Balance business requirements with technical feasibility and set expectations on new projects. Recommend changes in development, maintenance and system standards.
Design and build integration components and interfaces in collaboration with Architects and Infrastructure Engineers as necessary. Perform unit, component, integration testing of software components including the design, implementation, evaluation and execution of unit and assembly test scripts.
Determine if the data received from the upstream systems are of good quality based on the rules and data quality validations defined and in case of any issues with the data quality analyze and come up with a preliminary summary of the root cause/issue.
Assist the Analytics team by leveraging Wawa’s Enterprise Data Platform ecosystem to design, and develop capabilities to deliver our solutions using Spark, Scala, Python and
Follow security standards for all data and tools that are being introduced in the team.




Essential Functions:




Handle multiple priorities simultaneously
Work collaboratively with cross-functional teams
Establish and maintain a working environment conducive to positive morale, individual style, quality, creativity, and teamwork
Ability to build strong trusting relationships with business partners
Passion for innovation and “can do” attitude to thrive in a fast paced environment
Ability to work in a fast-paced, team environment
Excellent communication skills
Basic project management skills required
Work with the team to lead and maintain data strategy standards in all the data team is responsible for.




Basic Qualifications:




Bachelor’s degree in Computer Science/Engineering preferred
5+ years database, data integration experience
3+ years’ experience with Spark, Scala/Python, SQL and Big Data solutions
Preferred experience with Databricks and Snowflake
3+ years’ experience in designing and implementing the data architecture (conceptual, logical, physical & dimensional models).
Developing Enterprise Business Intelligence solutions on one or more of the following
EDW platforms: Snowflake, Redshift, Google Big Query
Experience implementing Big Data solutions using open source technologies
Strong knowledge of key scripting and programming languages such as Python, Java, Scala, etc
Experience with data integration tools such as Talend would be helpful
Experience designing and implementing various data pipeline patterns and strategies
Hands-on experience with dimensional modeling techniques and creation of logical and physical data models (entity relationship modeling, exposure to data warehouse design)
Strong knowledge of data security principles
Proven track record working with complex, interrelated systems and bringing that data together on Big Data platforms.




Wawa will provide reasonable accommodation to complete an application upon request, consistent with applicable law. If you require an accommodation, please contact our Associate Service Center at asc@wawa.com or 1-800-444-9292.




Wawa, Inc. is an equal opportunity employer. Wawa maintains a work environment in which Associates are treated fairly and with respect and in which discrimination of any kind will not be tolerated. In accordance with federal, state and local laws, we recruit, hire, promote and evaluate all applicants and Associates without regard to race, color, religion, sex, age, national origin, ancestry, familial status, marital status, sexual orientation or preference, gender identity or expression, citizenship status, disability, veteran or military status, genetic information, domestic or sexual violence victim status or any other characteristic protected by applicable law. Unlawful discrimination will not be a factor in any employment decision.
Show more "
3569761443,Data Engineer,SynapOne,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-synapone-3569761443?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=Zx%2Bt0aORXNdsUOSU6iUp2w%3D%3D&position=18&pageNum=3&trk=public_jobs_jserp-result_search-card,2870,"Job title : Data Engineer




Location: Chicago, IL




Onsite, Hybrid




Need exp with : Python , Databricks, Spark, Azure




Role Description




We are looking for a Data Engineer to join our growing team to rebuild healthcare. In this role, you will be responsible for building data pipeline architecture from the ground up, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder, who enjoys optimizing data systems and implementing architectural best practices. The Data Engineer will support our software engineers, DW architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.




Job Responsibilities




Ï Design, Develop, Test, and optimize new or existing data pipeline solutions




Ï Assemble large, complex healthcare data sets(payors, EHR Data) that meet functional / non-functional business requirements




Ï Identify, design, and implement process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.




Ï Keep our data secure following HIPAA standards and Hitrust best practices




Ï Create data tools for Analytics, PopHealth, and Data science team members that assist them in building and optimizing our product into an innovative industry leader.




Ï Build API interfaces for interoperability between EHR, HIE's, and OSH systems




What we're looking for




We're Looking For Motivated, Experienced Developers With




Ï Bachelor's degree in Computer Science, Engineering, or related field from an accredited university




Ï Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.




Ï Experience building and optimizing 'big data' data pipelines, architectures, and data sets.




Ï Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores using Python, Spark




Ï Experience in building APIs and following security best practices




Ï Experience with our Tech Stack: Python, Databricks, Spark, Azure DF, Synapse Warehousing, and Analytics with PowerBI




Ï Experience in Dev/Data Ops best practices and exposure to Observability platforms




Ï Knowledge of FHIR, CCDA, ADTs, HL7 standards - Preferred




Ï Experience working with EMRs (Cerner, Meditech, Epic, etc) preferred




Ï Working knowledge with Interface engines: Rhapsody, QVera
Show more "
3572569234,Data Engineer,Stellent IT,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-stellent-it-3572569234?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=BwRg0BZu7jVnUBb3KsEyBw%3D%3D&position=19&pageNum=3&trk=public_jobs_jserp-result_search-card,1067,"Role: Data Engineer

Location: (Nashville, Tennessee)

Duration: 6+ Months (C2H)

Interview: Phone + Skype

Job Description

Work within a collaborative Agile environment
Analyze requirements and create performance, scalable database structures, objects and stored procedures
Analyze, create and tune complex T-SQL code to ensure optimal performance and data integrity across very large data sets
Create, modify and deploy SQL Server Integration Services packages to extract, transform and load large sets of data
Troubleshoot database performance issues in an MSSQL environment
Review and tune code to ensure best practices, quality standards and data integrity
Configure database parameters and define data repository requirements, data dictionaries, and warehousing
Design and implement approaches to improve database performance, capacity, and scalability
Confer with appropriate managers and peer team members regarding problems with and capabilities of databases
Experience with Oracle database, Elasticsearch, GitLab, MySQL, PostgreSQL, Kafka a plus
Show more "
3577824107,Data Engineer - Data Science & Analytics,Costco Wholesale,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-data-science-analytics-at-costco-wholesale-3577824107?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=9RbKsRGInGs7GRQXUXL2lw%3D%3D&position=20&pageNum=3&trk=public_jobs_jserp-result_search-card,5919,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.




The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both written and verbal.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ of hands on experience with Informatica PowerCenter.
2+ years’ of hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
3+ years’ hands on experience with Git / Azure DevOps
Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.
Able to work in a fast-paced agile development environment.




Recommended




Microsoft Azure/similar certifications.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
BA/BS in Computer Science, Engineering, or equivalent software/services experience.




Required Documents




Cover Letter
Resume




California applicants, please click here to review the Costco Applicant Privacy Notice.




Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.




Pay Ranges




Level 2 - $100,000 - $135,000,




Level 3 - $125,000 - $165,000




Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible




We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States.
Show more "
3549988214,Data Science Engineer,STEMBoard,2023-03-31,https://www.linkedin.com/jobs/view/data-science-engineer-at-stemboard-3549988214?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=Ut8Pdbs0DfD5nNePNsABEg%3D%3D&position=21&pageNum=3&trk=public_jobs_jserp-result_search-card,3486,"SOCOM – Tampa, FL (Hybrid Schedule) – Top Secret Clearance Required

Position Overview:

STEMBoard is seeking a data engineer with an understanding of performance optimization and data pipelining. The data engineer will create and integrate application programming interfaces (APIs) and apply multiple programming languages including knowledge of SQL/NoSQL database design.

The data engineer role requires knowledge in programming for integrating complex models and using a software library frameworks to distribute large, clustered data sets. Data engineers collect and arrange data in a form that is useful for analytics. A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight.

Principal Duties and Responsibilities:

Collaborate with a team of data Stewards in the development of the program and data analytics projects.
Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources.
Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure. Create and maintain optimal data pipeline architecture.
Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources.
Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration.
Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig.
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms.
Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS).
Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs


Requirements

Required Education/Experience:

Experience: 1+ years of experience with software engineering, data engineering or related experience.
Education: Bachelor’s in STEM with a preference towards Data Science, Computer Science, or Software Engineering.
Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines.
Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations.
Candidate must be willing to travel at least 25%


Benefits

Healthcare, Vision, and Dental Insurance
20 Days of PTO
401K Matching
Training/Certification Reimbursement
Short term/Long term disability
Parental/Maternity Leave
Life Insurance


STEMBoard is committed to hiring and retaining a diverse workforce. All qualified candidates will receive consideration for employment without regard to disability, protected veteran status, race, color, religious creed, national origin, citizenship, marital status, sex, sexual orientation/gender identity, age, or genetic information. Selected applicant will be subject to a background investigation. STEMBoard is an Equal Opportunity/Affirmative Action employer.
Show more "
3562220373,Software Engineer - Early Career,Lockheed Martin,2023-04-12,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3562220373?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=Zq36KqPBVIV64nTT0A9diQ%3D%3D&position=22&pageNum=3&trk=public_jobs_jserp-result_search-card,6184,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




The coolest jobs on this planet… or any other… are with Lockheed Martin Space.




Lockheed Martin Space in Littleton, CO is seeking a full-time Early Career Software Engineer. Consolidated Analysis Orchestration Services (CAOS) is the preeminent Geospatial-Intelligence program in the world – over 10,000 intelligence analysts and decision-makers worldwide daily rely on CAOS for critical intelligence and geospatial data management. In addition to meeting today’s critical intelligence mission needs, we are working on evolving exciting future automation and artificial intelligence solutions for the National Geospatial-Intelligence Agency (NGA). We need the best engineers on our team to ensure mission success! Think you have the skills, come join the CAOS Team that builds software for the next generation of geospatial intelligence.




Must be a US Citizen ; this position will require a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable bachelors degree programs will include only science and technology (STEM) related programs such as Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Mathematics etc.
Must be a US Citizen ; this position will require a government security clearance. This position is located at a facility that requires special access.




Desired Skills




Proficiency with one or more of the following software languages: Java, C++, C#. Candidate will have knowledge of basic software practices such as coding standards, unit testing and configuration management, database and System administration. Good communication skills. Strong team player. Candidate will work in a team environment utilizing software methodologies and processes. Familiarity with Agile Development Process a plus. Strong communication skills a results oriented team player, creative thinker and problem-solver and follow all ethical standards of the Lockheed Martin Corporation.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: 9x80 every other Friday off




Pay Rate




The annual base salary range for this position in Colorado or Washington is $57,800 - $110,800. Please note that the salary information is a general guideline only. Lockheed Martin considers factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience, education/ training, key skills as well as market and business considerations when extending an offer.




Benefits offered: Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Flexible Spending Accounts, EAP, Education Assistance, Parental Leave, Paid time off, and Holidays.




(Washington state applicants only) Non-represented full time employees: accrue 10 hours per month of Paid Time Off (PTO); receive 40 hours of Granted PTO annually for incidental absences; receive at least 90 hours for holidays. Represented full time employees accrue 6.67 hours of PTO per month; accrue up to 52 hours of sick leave annually; receive at least 96 hours for holidays. PTO is prorated based on hours worked and start date during the calendar year.




This position is incentive plan eligible.




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3577817921,Data Engineer - Data Science & Analytics,Costco Wholesale,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-data-science-analytics-at-costco-wholesale-3577817921?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=asDJi3FeNs3CSNN2IZXfqw%3D%3D&position=23&pageNum=3&trk=public_jobs_jserp-result_search-card,6078,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.




TheData Engineer - Data Analyticsis responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both written and verbal.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ of hands on experience with Informatica PowerCenter.
2+ years’ of hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
3+ years’ hands on experience with Git / Azure DevOps
Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.
Able to work in a fast-paced agile development environment.




Recommended




Microsoft Azure/similar certifications.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
BA/BS in Computer Science, Engineering, or equivalent software/services experience.




Required Documents




Cover Letter
Resume
Last two performance reviews
Attendance records for current year (Do not include absences covered by paid sick/personal time, FMLA or other protected absences.)




California applicants, please click here to review the Costco Applicant Privacy Notice.




Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.




Pay Ranges




Level 2 - $100,000 - $135,000,




Level 3 - $125,000 - $165,000




Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible




We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States.
Show more "
3580606304,Data Analyst,Rang Technologies Inc,2023-04-24,https://www.linkedin.com/jobs/view/data-analyst-at-rang-technologies-inc-3580606304?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=sueGwb%2BDgGi%2BikCX%2Fiy64A%3D%3D&position=24&pageNum=3&trk=public_jobs_jserp-result_search-card,802,"The ideal candidate will use their passion for big data and analytics to provide insights to the business covering a range of topics. They will be responsible for conducting both recurring and ad hoc analysis for business users. 

 

Responsibilities

Understand the day-to-day issues that our business faces, which can be better understood with data
Compile and analyze data related to business' issues
Develop clear visualizations to convey complicated data in a straightforward fashion




Qualifications

Bachelor's or Master's degree in Statistics or Applied Mathematics or equivalent experience
1 - 2 years' Data Analysis experience




Primary Skills

Proficient in SQL, Python, R
BIO STATS, health informatics
Basic SAS Skill




Certification (optional):

BASE SAS certification




Show more "
3581063232,DATA ENGINEER (i360),i360,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-i360-at-i360-3581063232?refId=RkCeTXQwny45PPF1%2F1yDIw%3D%3D&trackingId=xs5kGS0EAZ1%2Fg4yHhW%2FfyQ%3D%3D&position=25&pageNum=3&trk=public_jobs_jserp-result_search-card,4098,"Description




i360, where The Data is the Difference, is the leading data and technology provider for those advancing a free and prosperous society through the campaign, nonprofit, and advocacy communities. i360 is a dynamic workplace sitting at the intersection of public policy, technology, and business, and is seeking team members who are excited about building the next generation of political technology.




i360 is seeking a mid-level Database Engineer to join its data engineering team. With your technical expertise, you will design, implement, and improve processes, procedures, and automation for all database-centric areas. You will tune our relational database systems and NoSQL systems for performance and reliability. You are responsible for building tools and scripts to monitor, troubleshoot and automate our systems. You propose test plans and interface with other teams, developers, and application owners to arrive at optimal solutions. Successful candidates will solve problems unique in scale and concept in the pursuit of new and original features. So, bring your ingenious mind, great team spirit and excellent communication skills to this great opportunity at i360.




What You Will Do In Your Role




Work with senior engineers and architects on the team to build and test SQL and NoSQL database solutions
Proficient with programming languages such as Python, Java, Spark, or Scala
Proficiency with relational database concepts & query optimization
Experience working with cloud platforms such as AWS or Azure
Experience with building data pipelines and implementing ETL processes
Perform code reviews and QA data imported by various processes
Investigate, analyze, correct and document reported data defects
Create and maintain technical specification documentation
Communicate effectively with stakeholders and collaborate with cross-functional teams




What You Will Need




BA/BS degree in Computer Science, Computer Engineering, Statistics, or other Engineering disciplines
4+ years of experience with relational database(s)
4+ years of coding experience in Python and/or Java




What Will Put You Ahead?




Knowledgeable on NoSQL, Columnar and/or Graph databases such as Elasticsearch, neo4j, Redshift, & Snowflake
Knowledgeable on architecting, developing, and maintaining solutions in AWS
Working knowledge of UNIX/Linux platforms
Familiarity with streaming services & event buses




Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.




At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.




Equal Opportunity Employer, including disability and protected veteran status.




Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test.




This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf




Show more "
3572337158,Data Engineer,Ivy Energy,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-ivy-energy-3572337158?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=cqJftPyXUxlIci18QLlPKQ%3D%3D&position=1&pageNum=4&trk=public_jobs_jserp-result_search-card,3428,"Today's financial system is built to favor those with money. Grid's mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.




Grid is a fast growing team that's deeply passionate about making a difference in the lives of millions. We're solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.




Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.




Problems We Work On




We're looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.




Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly




We Practice




Open collaboration
Code reviews
Testing
Agile development




We Use




Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We're looking for Engineers to:




Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring




Requirements




Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.




Really Strong Candidates May Have




Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go




PI212207179
Show more "
3562220373,Software Engineer - Early Career,Lockheed Martin,2023-04-12,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3562220373?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=8YDK9FGpWKfyBImrX08ZRQ%3D%3D&position=3&pageNum=4&trk=public_jobs_jserp-result_search-card,6184,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




The coolest jobs on this planet… or any other… are with Lockheed Martin Space.




Lockheed Martin Space in Littleton, CO is seeking a full-time Early Career Software Engineer. Consolidated Analysis Orchestration Services (CAOS) is the preeminent Geospatial-Intelligence program in the world – over 10,000 intelligence analysts and decision-makers worldwide daily rely on CAOS for critical intelligence and geospatial data management. In addition to meeting today’s critical intelligence mission needs, we are working on evolving exciting future automation and artificial intelligence solutions for the National Geospatial-Intelligence Agency (NGA). We need the best engineers on our team to ensure mission success! Think you have the skills, come join the CAOS Team that builds software for the next generation of geospatial intelligence.




Must be a US Citizen ; this position will require a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable bachelors degree programs will include only science and technology (STEM) related programs such as Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Mathematics etc.
Must be a US Citizen ; this position will require a government security clearance. This position is located at a facility that requires special access.




Desired Skills




Proficiency with one or more of the following software languages: Java, C++, C#. Candidate will have knowledge of basic software practices such as coding standards, unit testing and configuration management, database and System administration. Good communication skills. Strong team player. Candidate will work in a team environment utilizing software methodologies and processes. Familiarity with Agile Development Process a plus. Strong communication skills a results oriented team player, creative thinker and problem-solver and follow all ethical standards of the Lockheed Martin Corporation.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: 9x80 every other Friday off




Pay Rate




The annual base salary range for this position in Colorado or Washington is $57,800 - $110,800. Please note that the salary information is a general guideline only. Lockheed Martin considers factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience, education/ training, key skills as well as market and business considerations when extending an offer.




Benefits offered: Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Flexible Spending Accounts, EAP, Education Assistance, Parental Leave, Paid time off, and Holidays.




(Washington state applicants only) Non-represented full time employees: accrue 10 hours per month of Paid Time Off (PTO); receive 40 hours of Granted PTO annually for incidental absences; receive at least 90 hours for holidays. Represented full time employees accrue 6.67 hours of PTO per month; accrue up to 52 hours of sick leave annually; receive at least 96 hours for holidays. PTO is prorated based on hours worked and start date during the calendar year.




This position is incentive plan eligible.




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3572569234,Data Engineer,Stellent IT,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-stellent-it-3572569234?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=nnLfJHs3Cfv2pDagXKifIA%3D%3D&position=4&pageNum=4&trk=public_jobs_jserp-result_search-card,1067,"Role: Data Engineer

Location: (Nashville, Tennessee)

Duration: 6+ Months (C2H)

Interview: Phone + Skype

Job Description

Work within a collaborative Agile environment
Analyze requirements and create performance, scalable database structures, objects and stored procedures
Analyze, create and tune complex T-SQL code to ensure optimal performance and data integrity across very large data sets
Create, modify and deploy SQL Server Integration Services packages to extract, transform and load large sets of data
Troubleshoot database performance issues in an MSSQL environment
Review and tune code to ensure best practices, quality standards and data integrity
Configure database parameters and define data repository requirements, data dictionaries, and warehousing
Design and implement approaches to improve database performance, capacity, and scalability
Confer with appropriate managers and peer team members regarding problems with and capabilities of databases
Experience with Oracle database, Elasticsearch, GitLab, MySQL, PostgreSQL, Kafka a plus
Show more "
3581006905,Data Engineer,Ellianse LLC,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-ellianse-llc-3581006905?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=B5zTEWsya9rRnPKDkc8kOA%3D%3D&position=5&pageNum=4&trk=public_jobs_jserp-result_search-card,645,"Job Description




Strong experience with NoSQL database, including Postgres
Background in working with Azure Cloud Services: Data Factory, SQL database, Functions, Data Lake, Databricks, Logic Apps, and Azure Automation.
Knowledge in object-oriented and functional script language: Python, Scala, and C#.
Advanced working knowledge of SQL Server database - writing advanced SQL script, profiling, and optimization. Working knowledge of Business Intelligence tools: Microsoft Integration Services, Reporting Services, and Analysis Services, as well as PowerBI.
Experience with other Big Data tools such as Spark, Snowflake, and Kafka
Show more "
3582795048,Data Engineer (Python & Snowflake),Diverse Lynx,2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-python-snowflake-at-diverse-lynx-3582795048?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=V6f1Iv9yfRMHThjWyhP8Wg%3D%3D&position=6&pageNum=4&trk=public_jobs_jserp-result_search-card,1165,"Job Description




Job Title-Data Engineer (Python & Snowflake)




Location-: Plano,TX




Job Type-Full Time




JD-




Very good experience with Python.




Have ETL and basic troubleshooting skills in SnowSQL and snowflake connector.




Good experience in working in the ADB environment, Knows PySpark, PySpark SQL, ETL processes.




Ability to navigate across multiple hosts.




Ability to understand complex scripts and make modifications.




Need to understand SQL and make modifications, troubleshooting performance issues.




Need good understanding on how to run complex jobs streams and scheduling.




Need a be a good communicator. Need to communicate issues, status. Ability to use MS tools like Word, Excel, SharePoint.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3564359844,Data Engineer,Robert Half,2023-04-03,https://ca.linkedin.com/jobs/view/data-engineer-at-robert-half-3564359844?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=a54SuMS3mvqVNrjlxOL6%2FQ%3D%3D&position=7&pageNum=4&trk=public_jobs_jserp-result_search-card,3909,"Description

A client of ours is looking to bring on a Data Engineering professional for their team in Toronto. As a data engineer on the business side, you will be responsible for designing, developing, and maintaining the data architecture and structures that support our company's analytical and reporting needs. You will work directly with various stakeholders to understand the requirements, and develop solutions that deliver relevant, accurate, and timely data to support decision making. This opportunity requires an in-office presence three times a week.

Responsibilities

Develop, implement and the support the teams' data strategy to manage structured and unstructured data that integrates the data into a data solution to support BI solutions, reporting and business processes
Design and develop data models, structures, and workflows that support data-driven decision making
Develop and maintain automated data pipelines, ETL jobs, and scripts for data integration
Analyze and optimize data storage and processing to ensure optimal performance
Develop and maintain data documentation, data dictionaries, and metadata management
Provide innovative data solutions by working with stakeholders and across functional areas to understand and gather data objectives, and identify opportunities for improvement for data analysis, reporting and visualizations
Automate manual processes and optimize data ingestion and delivery to support departmental data needs, reporting and business processes
Partner with the Business Analysts to develop reports and Power BI dashboards
Collaborate with business stakeholders to identify and understand their data requirements
Support and troubleshoot existing investment data applications and BI reporting solutions as needed

Requirements

Qualifications

Strong experience with data management, investment systems, business intelligence, and reporting technologies
5+ years of experience working with complex data structures and developing efficient queries
5+ years of relevant experience in data science and analytics
3+ years of experience creating/maintaining investment and financial data models
3+ years of experience designing and developing dashboards using Power BI
Proven proficiency in ETL tools (Alteryx), cloud-based data platforms/systems (such as Azure, Snowflake), process automation, and standardization
3+ years of experience with PowerBI
Strong communication skills (written and verbal)
Critical thinking skills with the ability to independently solve problems with data and/or reference documents
Understanding of data governance, data security, and data privacy requirements
Track of record working in a role that intersects with business and technical analysis to support existing solutions, delivering new solutions, and contributing to system implementations and upgrades
Bachelor’s or master’s degree in Computer Science, Engineering, Mathematics, Finance, or a related field

If you have a passion for data management and a strong technical aptitude, we invite you to consider applying for this exciting opportunity.

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go.

Questions? Call your local office at 1.888.490.4429. All applicants applying for Canadian job openings must be authorized to work in Canada.

© 2023 Robert Half. By clicking “Apply Now,” you’re agreeing to
Show more "
3576259125,Data Engineer,D1 Brands,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-d1-brands-3576259125?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=enC2SNKm3eVeziJaga8NJA%3D%3D&position=8&pageNum=4&trk=public_jobs_jserp-result_search-card,4412,"A Little Bit About D1




D1 Brands is a global operator and acquirer of 3rd-party consumer products brands. Starting from a small group of Amazon-native sellers, we’ve built D1 into a global consumer brands platform with a vision to define the future of 21st century retail. Since our founding in late 2020, we’ve expanded the D1 Brands family into more than 110 talented professionals located around the globe; we’ve raised more capital in our first year of business than most start-ups raise in a lifetime; and we’ve partnered with dozens of exceptional entrepreneurs to help protect and grow the brands they’ve built. But, as our namesake indicates, we believe it is always “Day 1”, so if you’re interested in helping us build the future of digital commerce, we’d love to meet you.




Here Are a Few




The D1 team comes from a wide variety of professional backgrounds and countries, but we all share a few core beliefs.




We are fanatical about creating value for our brand sellers, customers, and D1 team




members




We’d rather write the rules than follow someone else’s – there is always a better way




to do things




When it comes to building a world-class business, we shouldn’t compromise between




hard work and enjoying the journey – if we’re not having fun, we’re failing




About The Role




At D1 our Data Engineer role is end-to-end, from unearthing data at its source to insights and automation for the team. Data is at the center of everything we do, and you’ll be a key partner to teams across the business. This role involves integrating data from a variety of sources, designing and building scalable pipelines, transforming data for use, and generating insights and tools that power the business. We’re entrepreneurial and enjoy experimenting with and leveraging the industry’s latest capabilities.




Responsibilities




Build and maintain scalable infrastructure to provide teams seamless access to data and insights
Extract, prepare and ingest data from various sources, set up ETLs to ready data for efficient access
Monitor and Audit data flows regularly to ensure accuracy
Understand and document underlying data from multiple sources
Engineer the most efficient data housing, transformation and querying
Manage self-service tools and model data for end users across the company to explore a wide variety of data with little assistance
Leverage emerging technologies to automate insights, workflows and tasks
Partner with teams to automate actions based on data, engineer creative solutions for passing data between tools




Qualifications




3+ years of experience in data roles at high growth companies
Bachelor's or Master's degree in Computer Science, Data Science, or a related field preferred
Experienced in SQL, ETL/ELT, data modeling / transformation, data collection and instrumentation
Experience with cloud-based data storage and processing technologies, snowflake, fivetran, and dbt experience preferred
System monitoring and alerting, and dashboarding experience
Self starter comfortable in fast past environments passionate about learning, experimenting and having an impact




What else does D1 have to offer?




Do you offer any benefits to your employees?




We do offer medical, vision, dental, short term disability and life insurance benefits for U.S. employees. We offer medical stipends for our international staff. We also offer unlimited PTO, a generous parental leave, and remote work options.




Some Other Fun Perks




Monthly company socials to get to know your fellow D1ers
An in-house resource library
Opportunities for professional development




What if I don't see a job description that fits my background?




If this position isn't a fit for your background, but you are still interested in D1 Brands please consider dropping your resume to our General Consideration posting. Our company is growing fast and our Talent Acquisition Team does review all resumes.




If you are at least a little intrigued at the potential of working at D1, drop your resume!




We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Show more "
3542508857,Data Engineer,Jetty,2023-03-27,https://www.linkedin.com/jobs/view/data-engineer-at-jetty-3542508857?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=Gh6uweiLxr5r8p5%2Fs%2BeceQ%3D%3D&position=9&pageNum=4&trk=public_jobs_jserp-result_search-card,4675,"Welcome to Jetty, the financial services platform on a mission to make renting a home more affordable and flexible. We've built multiple financial products that benefit both renters and property managers - and we're just getting started.

As a member of the Jetty engineering team, you're passionate about building fintech products that provide value to our customers and to Jetty. You are motivated by designing engineering systems around complex business problems. You love to learn, take on challenges, and are empowered in a fast-paced and transparent culture. You're comfortable finding the right tool or pattern for the job, and advocating for improvements to the way we work.

As a Data Engineer, your goal is to cultivate a data-informed culture and create insights that will be leveraged across the entire organization. You have experience executing at a high level, solving complex problems, and delivering solutions with real business impact - and you're excited by the opportunity to apply those principles to a new, best in class function.

Role & Responsibilities

Build / Support our modern data stack (Snowflake / Fivetran / DBT / Tableau)
Implement the Five Pillars of Data Observability
Write ELT code using modern software engineering practices (Git, automated testing and deployments)
Build and maintain data pipelines to support various business processes and reporting (Fivetran / AWS Lambdas)
Document our data models in a user friendly way for our business stakeholders
Partner with the Product Engineering team to ensure we are capturing the data we need from our applications for analytics and to iterate on our development practices for the data analytics team.
Be an enthusiastic evangelist of our modern data stack (Fivetran / DBT / Snowflake / Tablea)
Be the resident resource on building standard reports and BI dashboards


Experience & Qualifications

4+ years of experience working in a data / analytics engineering role
High proficiency in Snowflake / Fivetran / dbt / Tableau
High proficiency in SQL and Python
Ability to collect, interpret, and synthesize inputs from various parts of the business into data model requirements
Ability to simplify without being simplistic - ability to communicate complex topics and actionable insights in a compelling way that can be understood by a variety of audiences
Inherent curiosity and analytical follow-through — you can't help but ask ""why?"" and love using data and logic to explore potential solutions
Ability to balance ""Rigor"" and ""Scrappiness"" — you know the difference between 80/20 and giving something 110%; as well as when each is appropriate.
Deep understanding of the first and second order effects of reporting — you know the power of presenting the right data to the right people at the right time
Experience in a data/analytics function at a high-growth startup managing multiple stakeholders and delivering actionable insights


About Jetty

At Jetty, we know renting a home can be a financial challenge. That's why we're on a mission to make renting accessible to everyone. Jetty offers four financial products designed to help our members every step of the renting process: Jetty Deposit, a low-cost security deposit product that dramatically reduces move-in costs; Jetty Rent, a flexible rent payment program to eliminate pricey late rent fees; Jetty Credit, a credit building service that helps renters build credit just by paying rent; and Jetty Protect, an affordable renters insurance product that provides comprehensive coverage in just a few clicks.

Jetty has raised multiple rounds of venture capital from investors including Khosla Ventures, Ribbit Capital, Citi, Valar, and strategic investors. We've built a highly collaborative team working remotely around the country, and we believe in finding the best talent—regardless of where they live. To learn more about life at Jetty, visit jetty.com/careers.

Jetty is firmly committed to building a team as diverse as our Members. We are proud to provide equal employment opportunities for all candidates regardless of race, ancestry, citizenship, sex, gender identity or expression, religion, sexual orientation, marital status, age, disability, or veteran status.

Benefits & Perks

Health (with HSA and FSA options), dental, and vision insurance through Aetna & MetLife
401(k) retirement savings program
Optional life and disability coverage
20 days of PTO + 12 holidays, ""Jetty Winter Break,"" and flexible sick days
Generous parental leave policy
Flexible remote work in any US location (keeping east coast hours)
Stipends to cover WFH set-up, childcare, phone/internet bill, and optional co-working space
Show more "
3576726502,Python Data Engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/python-data-engineer-at-diverse-lynx-3576726502?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=LwaguUYDBhaaICfbH4oCTg%3D%3D&position=10&pageNum=4&trk=public_jobs_jserp-result_search-card,873,"Job Description




Job Title: Python Data Engineer




Location: Austin, TX




Duration: Fulltime




Job Description




Relocation is required.




Role : Python Data Engineer




Skill set : Python based Data engineer




Python- with Big Data experience
Ability to Process and transform large data and and integrate with enterprise systems
Good to have Machine learning application experience
Quality of engineering skills is primary criteria.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3545828798,Data Engineer,Caliva,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-at-caliva-3545828798?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=gW%2BZPit0ULngX4euE%2Fk3UQ%3D%3D&position=11&pageNum=4&trk=public_jobs_jserp-result_search-card,4241,"The Data Engineer is responsible for the design of an end-to-end data engineering strategy, operation of ETL pipelines from all TPCo internal and external data sources and creation of federated data models for consumption by the Business Intelligence team to drive impactful insights and analysis. You will support a production-grade data warehouse working with software engineers and multiple partners and ultimately serve as the data foundation for all decision support tools.

Salary: $105k-$160k

What You'll Do

Assist with the implementation of new systems and updates to existing systems by leading the data strategy for each, assuring data integrity, value and access
Establish best practices in our data engineering practice and strategy
Develop appropriate data schemas and structures for use in downstream models/reports
Develop data management and oversight program spanning dozens of source systems across all departments, creating new ETL pipelines and maintenance of existing ones, ensuring data richness and quality.
Engineer capacity and performance in addition to providing forecasting and future planning as well as review and consideration of technology trends.
Recommend and develop changes to source data structures/systems based on observations of data within the context of operational use.
Assemble large, complex data models to meet the needs of operational and strategic stakeholders.
Work closely with our in-house analysts to integrate SQL data models to a dependency tree.
Document and maintain our data lineage and data dictionary
Other duties and responsibilities as assigned by management.


What You Have

A successful candidate should come from a strong data engineering background, is self-directed and comfortable supporting the data needs of multiple teams, systems and products. You need to have experience with NoSQL and SQL data structures and be able to analyze/transform the data using various tools. Your analytical skills and knowledge of schema metadata will be essential. The right candidate will be excited by the prospect of optimizing our company's data architecture to support our next generation of products and data initiatives.

Bachelor's degree or higher in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering, Business Administration, or similar or equivalent combination of education and experience
4+ Years' experience in a data engineering role supporting production systems
1+ years experience manipulating data using Python (experience with Pandas is a plus)
1+ years experience extracting data from REST APIs
1+ years experience managing a codebase in GitHub
Previous experience developing ETL pipelines using technologies such as Airflow (preferable), Luigi, Oozie, Azkaban, etc.
Previous experience developing data models to support a data warehouse
Experience manipulating and de-normalizing data in JSON format for storage in relational databases
Experience with Google Cloud Platform or AWS cloud services
(Preferred) Knowledge and experience with Kubernetes and/or Docker
(Preferred) Advanced knowledge of SQL and experience working with relational databases. BigQuery experience is an extra plus


Working Environment

This job operates in a professional office environment and routinely uses standard office equipment such as computers, phones, copiers, filing cabinets and may work in proximity with customers and staff.


Job Requirements

Work revolves around objectives, projects and priorities, not hours; must be able to work weekends, holidays, and occasional overtime as needed.
Must be able to stand, walk, lift, sit, and bend for a majority of their work schedule.
Must be able to travel to other office locations.
Ability to use computer and calculator for 8 hours or more.
Must be 21 years of age or older.
Must comply with all legal or company regulations for working in the industry.
Selected candidate will be required to complete a post offer, pre-employment background check with the local law enforcement or San Jose Police Department.


Applicants must be authorized to work for ANY employer in the US.

We are unable to sponsor or take over sponsorship of employment Visa at this time.
Show more "
3560424080,Data Engineer (Remote),The Hartford,2023-04-10,https://www.linkedin.com/jobs/view/data-engineer-remote-at-the-hartford-3560424080?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=lisnwWE6irK11ScUaSeeLw%3D%3D&position=12&pageNum=4&trk=public_jobs_jserp-result_search-card,4124,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.




Ready to grow your career leveraging the latest DATA technologies? Join a fast-paced and talented Performance Analytics Data Engineering team to unlock Data Capabilities for The Hartford. You will have an opportunity to participate in the ETL lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies.




The Performance Analytics Data Engineering group is seeking a Data Engineer to support Claims Insight Analytics with a focus on Global Specialty lines of business. This position combines design, development, and implementation of data-focused solutions as well as analysis to identify and develop innovative monitoring metrics to customers across Global Specialty Lines.




In this role the candidate will partner closely with the Process Owners and Analysts as an expert resource in providing data, business, and technical support for a wide range of solutions. The candidate should have the skills of a data research expert and be able to work in an iterative collaborative way in balancing the solution with the technical outcome. This individual will bring with them a creative solution design approach to data exploration and discovery.




Responsibilities




Provide technical expertise to analysts, and project teams, working closely in clarifying business needs and requirements, to enable design, development, integration, and production of data & reporting solutions.
Maintain expertise of programming languages including SQL and Python, software technologies, and broad knowledge of database systems
Support the development of advanced claim monitoring metrics, which includes becoming the subject matter expert of the data and ownership of metric definitions.
Assisting in deep dive data analysis for Global Specialty customers, utilizing both structured and unstructured data.
Collaboration within the team to support data needs and team members’ analyses.




Experience & Skills




Bachelor’s degree or equivalent experience in related field required
Demonstrated ability to build strong partnerships (essential to success in this role)
2+ years of experience working with relational databases and SQL development
Experience with data warehousing tools, databases, and concepts (e.g. Oracle, Snowflake, AWS, ETL processes, Python, TOAD, Thoughtspot)
Knowledge of Global Specialty products and systems preferred but not required
Ability to analyze source systems and provide technical solutions
Experience creating, debugging, and optimizing SQL queries
Self-starter with a willingness to become a data expert
Demonstrate a passion to both learn new skills and lead discovery of the data research
Results oriented with the ability to multi-task and adjust priorities when necessary
Ability to work both independently and in a team environment
Effective written and verbal communication skills
Claims DNA badges (if currently in the claims organization): core and intermediate data, core innovation, core communication.




Compensation




The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:




$68,080 - $142,800




Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age




About Us | Culture & Employee Insights | Diversity, Equity and Inclusion | Benefits




Data Engineer - GE08AE




Skills:
Show more "
3552188324,Data Engineer,Nexus Staff Inc.,2023-04-06,https://www.linkedin.com/jobs/view/data-engineer-at-nexus-staff-inc-3552188324?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=36%2BAPbonUbvfijuu7XKUtw%3D%3D&position=13&pageNum=4&trk=public_jobs_jserp-result_search-card,3504,"Job Title: Data Engineer                                                  
Employment Type: Full Time 
Work Hours: 40+ hrs./week                                             
Work site: Hybrid
Location: Midtown Manhattan                                       
Salary: 80k-180k
 

Overview:
At our technology subsidiary, we are dedicated technology practitioners who work with a diverse range of industries and geographies to solve complex problems. Our team leverages cutting-edge tools to transform our clients' portfolios and create value by using data to create transparency and promote collaboration.
 
Responsibilities:
As a Data Engineer, you will be responsible for creating solutions that extract value from rich datasets, implementing technology transformations across industries such as healthcare, retail/consumer, telecom, and industrial.
 
Minimum Education:
Bachelor's degree in Computer Science, Software Engineering, or a related field.
 
Work Experience:
Proven track record as a Data Engineer, designing, building, and implementing batch, event-driven, and real-time data pipelines.
 
Experience with:
SQL Server, MySQL, or Postgres databases.
Azure tools, OAuth 2.0, and Key Vault for security.
Azure DevOps, Jenkins, Jira, and Git for DevOps.
C#, NodeJS, Java, and Scala programming languages.
Synapse, Snowflake, Redshift, and BigQuery warehousing.
Google Cloud Platform (GCP) and Amazon Web Services (AWS) cloud technologies.
Spark Streaming, Pub/Sub, and Kafka streaming.
Kubernetes, Docker, GKE, AKS, EKS for microservices.
GraphQL, OpenAPI, Swagger, SOAP, and EDI APIs.
OpenID for security.
Power BI and Tableau for BI.
 
 
Job Duties:
Design, build, and implement batch, event-driven and real-time data pipelines using Microsoft Azure, GCP, and AWS technologies.
Implement large-scale data platforms to meet the analytical and operational needs across various organizations.
Build products and frameworks that can be re-used across different use-cases to increase efficiency in coding and agility in the implementation of solutions.
Build streaming ingestion processes to efficiently read, process, analyze, and publish data for the real-time needs of applications and data science models.
Perform analyses of large structured and unstructured data to solve multiple and complex business problems.
Investigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess and advise.
Understand business use cases to design engineering routines to affect the outcomes.
Review and assess data frameworks and technology platforms with the goal of suggesting and implementing improvements to the existing frameworks and platforms.
Understand the quality of data used in existing use cases to suggest process improvements and implement data quality routines.
 
Preferred Skills:
Proficient in Python, SQL, and REST API.
Ability to work in a fast-paced, dynamic environment with changing priorities.
Excellent communication and interpersonal skills.
Strong problem-solving and critical thinking skills.
Ability to work independently as well as part of a team.
Ability to learn new tools and technologies quickly.
Understanding of Monolithic, SOA, Microservice, Serverless Architectures.
Understanding of authentication methods, data collection, data parsing, data validation, and data presentation.
 
Misc. Information:
Schedule: Monday-Friday | 9 AM - 5 PM.
Interviews will be conducted In-Person or virtually.
Show more "
3576620245,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576620245?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=PyJOhhmyjwgLcJ2fA6mumw%3D%3D&position=14&pageNum=4&trk=public_jobs_jserp-result_search-card,939,"Job Description




Job Role: Data Engineer




Location: Remote




Salary Range: FULL TIME




IOT, AWS, Machine Learning, Python, Jupyter




Description




Experience to code in Python and Jupyter. JSON a plus
Expertise with AWS S3, AWS Athena, and AWS Glue
Expertise with managing fast moving time series data sets
Experience with AWS Lambda
Expertise with Sagemaker/Sagemaker Studio
Secondary: Ability to build step functions
AWS Certified Developer/AWS Machine Learning
Also some dashboarding work is a plus




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3574018227,SQL Developer,Ascendion,2023-04-11,https://www.linkedin.com/jobs/view/sql-developer-at-ascendion-3574018227?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=%2FEHh%2FxYavDymq5X1dq24WQ%3D%3D&position=15&pageNum=4&trk=public_jobs_jserp-result_search-card,1778,"About Ascendion




Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.




Ascendion | Engineering to elevate life




We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:




Build the coolest tech for world’s leading brands




Solve complex problems – and learn new skills




Experience the power of transforming digital engineering for Fortune 500 clients




Master your craft with leading training programs and hands-on experience




Experience a community of change makers!




Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.




About The Role




Title: SQL Developer




Location: On-site hybrid (3 days/week) in Charlotte, NC or Raleigh, NC




Must Haves




3+ years of SQL development experience




2+ years of Snowflake experience




Familiarity with AWS




Experience with a reporting tool - ie. Tableau, PowerBI




Want to change the world? Let us know.




Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more "
3580991189,Data Engineer,Prismagic Solutions Inc.,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-prismagic-solutions-inc-3580991189?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=691TZ9erGgT05EOaMm9tZg%3D%3D&position=16&pageNum=4&trk=public_jobs_jserp-result_search-card,2561,"As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers' digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Client offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development.

Up to 8 years of software development experience in a professional environment and/or comparable experience such as:

Bachelor's or master's degree in computer science, computer engineering, or other technical discipline, or equivalent work experience, is preferred

5+ years of software development experience in big data technologies such as Python, Spark, PySpark, Spark SQL, Shell Scripting & Hive

Preferred – 5+ Years of hands-on experience in Data Ingestion, Data Organization & Data Consumption frame works using AXP Enterprise Data Platform

Good understanding of big data technologies such as Spark, Mapreduce, YARN, Hive, Zookeeper etc. Preferably, with some real-world experience.

Experience in design and development for batch, streaming and real-time big data applications using AXP EDP platform

Experience in hierarchical data structures in json/ xml

Experience with AXP CI/CD frameworks for code management and deployment like GitHub, Jenkins, XLR

Experience with NoSQL technologies (column-family, key-value or document datastores)

Experience (preferably GCP) and exposure to cloud native big data technologies would be a plus

Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores

REST API design and implementation experience

Good understanding of NoSQL technologies such as HBase, Cassandra, Redis, Memcached etc. Preferably, with some real-world experience.

Ability to effectively interpret technical and business objectives and challenges and articulate solutions

Understanding of SOA, microservices and containerized application concepts would be a plus

Strong analytical skills and programming skills, in production environment.

Hands on Experience on GraphQL a plus

Experience with Production Support/Dev-Ops would be a plus

Willingness to learn new technologies and exploit them to their optimal potential
Show more "
3570505211,Data Engineer,Care.com,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3570505211?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=Ne5uhA2ch81dVoRXooGxVA%3D%3D&position=17&pageNum=4&trk=public_jobs_jserp-result_search-card,5586,"About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Care.com offers an array of services that enable families to find, manage and pay for care and provide employment opportunities for caregivers. Our engineering organization is reimagining our tech stacks and consolidating them to a single cloud-native platform and a cloud-based Data Lake/Data Warehouse on Snowflake.

Here, entrepreneurs, self-starters, great teammates, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI and the latest technologies to solve universal problems and connect people in new ways. If you enjoy solving big problems and building new things, and if you're all about using your talent for good, this is the place for you.

Office Locations: (This is a hybrid position)

NY, NY 10011
Austin, TX 78746
Shelton, CT 06484


What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:

Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
Identify data sources used for building out data architecture diagrams/models
Establish engineering practices and setup frameworks for ""Data as a Service""
Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
Solve code level problems quickly and efficiently
Participate in demos and code reviews
Promote software best approach, standards, and processes
Shape development processes to promote a high-quality output while continuing to iterate quickly
Incorporate best practices for security, performance, and data privacy into data pipelines


What You'll Need to Succeed:

BS or MS in Computer Science or relevant engineering experience
4+ years work experience in Data Engineering/ETL
3+ years SQL experience preferred
2+ years traditional RDBMS experience (Oracle & Postgres experience preferred)
1+ years Unix/batch scripting preferred
1+ years Python experience is a plus
1+ years Windows server admin experience is a plus
Experience interfacing with business teams and turning requirements and vision into a technical reality
Ability to drive efforts from start to finish as a self-motivator
Knowledge in Data Warehousing is a MUST
Proven ability to maintain performance level in a fast-paced agile environment
Pragmatic and realistic with solutions


For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please contact talent@care.com.**


____________________________________________________________________________________________________________________________

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: $116,000 to $145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).
Show more "
3576671809,Data Engineer with Python,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-with-python-at-diverse-lynx-3576671809?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=nlzcpXykF%2FbR%2FbJvkcW49g%3D%3D&position=18&pageNum=4&trk=public_jobs_jserp-result_search-card,3546,"Job Description




Job Title: Data Engineer




Location: New York City, NY (1 New York Plaza, 21st Floor | New York, NY 10004)




Duration: FTE role with Genpact/ C2H




Manager Note: We are looking for candidate having practical hands-on experience with python, good database experience with SQL/PL, exposure to Spark/Impala/Hadoop ecosystem and some basic exposure to cloud databases.




Job Description:-




Operations Technology Data and Analytics team is looking for an intermediate level Data Engineer to help define and develop the current and future data strategy for Operations. The team is responsible for providing technology solutions to Brokerage’s global operations functions including Settlements, Payments, Tax, Books and Records, Reconciliations and Regulatory reporting. The data strategy includes various OLTP and OLAP systems, data lakes, and reporting and analytics platforms across various domains, business lines and geographies. The future data strategy involves optimal use of cloud based solutions and phased migration of existing technologies/platforms to the cloud.




Skills Required:-




5 to 10 years of strong experience with relational SQL including Postgres/DB2/SQL Server etc.
Strong experience with scripting languages: Python, Java, Scala, Shell, Perl etc.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets using big data tools: Hadoop, Spark, Kafka, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with data modelling of relational and dimensional data models
Experience in Multi-Parallel Processing (MPP) database or Greenplum 5.X is a big plus.
Good analytical, problem solving, communication and interpersonal skills.
Self-motivated and ability to work consistently and efficiently to achieve end goals.
Interest in learning the business functionality
Experience supporting and working with cross-functional teams in a dynamic environment.
Skilled in identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience in building processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.




Skills Desired




DB2/Oracle to Postgres/Greenplum migration experience.
DB Monitoring and automation.
Experience with NoSQL databases Hive/MongoDB/Cassandra etc.
Knowledge of databases and platforms like Greenplum, Hadoop and MPP Database environment.
Experience with public cloud databases: Azure SQL database, Azure Synapse, Databricks
Knowledge of Financial Services data
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with Data Science and Machine Learning Platforms like Dataiku DSS, Alteryx, DataRobot.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3575186998,Data Engineer,RideCo On-Demand Transit,2023-04-18,https://ca.linkedin.com/jobs/view/data-engineer-at-rideco-on-demand-transit-3575186998?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=mCyk4W4YXN8BC8ozvpzNeA%3D%3D&position=19&pageNum=4&trk=public_jobs_jserp-result_search-card,6355,"This is an opportunity in the exciting and fast-growing transportation technology industry. Public transit is being transformed from a system of static, scheduled fixed-routes, to a dynamic on-demand network, and you’ll be one of the pioneers shaping this transformation.

Data is at the heart of RideCo’s core offering, and drives the decisions we make in both our product and the services offered to our clients. As a Data Engineer, you will be responsible for building tools and processes to collect, transform, and generate insights and reports on the data needed to make these decisions. As a key technical and analytics resource for Project and Operations managers, you’ll have a vital role in shaping the design, performance, and success of our clients’ next-generation on-demand transportation services.

Responsibilities:

Manage and launch new reporting capabilities: spearhead the development of an end-to-end data management and analytics platform and new client-facing reporting dashboard. Work with operations and project managers to transform business requirements into concrete insights. Observe and foster best practices in business process design, implementation, and review.
Use your creative aptitude to extract unique insights: combine your creative thinking with your analytic skills to transform big data into actionable insights. Direct exploratory data initiatives to uncover business values using statistical and analytical techniques to drive project success and support the company reaching its corporate goals.
Generate and analyze operational performance reports: build comprehensive performance analysis reports spanning rider user-experience, driver user-experience, marketing, operating efficiency, and unit economics.
Improve internal processes and tooling: identify ways to further systematize our internal processes, including with new analysis and visualization tools. Share your ideas with team members and collaboratively champion internal process/tooling improvements.
Drive continuous improvement: collaboratively develop weekly/monthly actions to drive continuous improvements, and to achieve the company’s data objectives (e.g. build new reports, database maintenance, etc.). Work with the operations managers, and other stakeholders to execute on initiatives.
Automate: identify areas of automation within the organization. Champion efforts to streamline processes and improve team productivity.


Your playground / what you’ll learn:

At RideCo you’ll get a chance to play, learn and build with the following tools and technologies, and as part of a cross-functional team that is the world’s foremost innovator in on-demand transit software.

Python
Tableau
Microsoft Excel, Access and other Office tools
PostgreSQL, MSSQL, Oracle
Building schemas, data normalization, writing optimal SQL queries, debugging existing queries
Development Processes: Agile, continuous integration, Jenkins


Required Qualifications and Experience:

3-5 years of experience in data analyst or engineering role(s)
Experience working with Tableau
A deep and intuitive understanding of databases and SQL
Education:Undergraduate degree in Engineering / Data Science / Computer Science / Applied Mathematics / Statistics, or related discipline


Compensation and Benefits:

Base Salary: $70K - $95K + performance-based bonus + stock options
Work-Life Balance & Additional Perks: Flex-time work schedules, vacation time, catered lunches, social events, casual dress code
Benefits Plan: Medical, dental, prescription, life/health spending accounts and more
Work Environment: Located in KW's most desirable work space in the heart of Uptown Waterloo
Commuter Program: Complimentary rides to and from work


Who we are:

http://www.rideco.com

RideCo powers on-demand transit. Public transit agencies use RideCo's cloud-based software platform to provide on-demand shared rides in dynamically routed buses and vans. Our clients include some of the world’s largest transportation operators such as Los Angeles Metro, San Antonio Metro, and Calgary Transit.

Have you experienced getting frustrated with transfers and waiting while taking a public bus? Have you seen buses drive around in low-density areas with very few passengers on-board and wondered how inefficient that seems to be? You're likely aware of the first & last mile access challenges faced by transit hubs. We are solving these problems by re-imagining shared mobility. Imagine a world where vehicles have dynamic routes responsive to real-time trip demand. This 'dynamic shuttle' (or van /sedan) would pick you up, on-demand, at or near your doorstep, and take you to your destination or transit hub. Along the way, it may pick-up other passengers going in your direction. Your experience will be seamless: less waiting, less walking, fewer transfers, shorter travel time, and timely pickups and drop-offs. RideCo's 'dynamic shuttle' platform enables this seamless experience and low-cost shared rides for vehicle fleet operators and their passengers. By seamlessly moving more people in fewer vehicles we are catalyzing a generational shift in how people get around cities and towns. This means commuters spend less time in transit and more time doing what they enjoy.

RideCo powers a diverse range of use cases, including residential/ suburban travel; first-mile-last-mile connections for transit hubs; and corporate employee transportation. We are investing to scale up and capture the growing demand for on-demand shared rides solutions.

RideCo is proud to be an equal opportunity employer, we are committed to building and supporting a culture of diversity, inclusion, and accessibility. We hire the best talent regardless of race, color, creed, national origin, ancestry, disability, marital status, age, veteran status, sex, sexual orientation, gender identity, and expression. Building a team that represents a variety of backgrounds, perspectives, and skills benefits our employees, our customers, our products, and our community.

In accordance with the Accessibility for Ontarians with Disabilities Act, accommodations are available upon request for candidates taking part in all aspects of the selection process. If you require special accommodation to complete any portion of the application or interview process, please contact people@rideco.com.
Show more "
3563649404,Data Engineer - Remote | WFH,Get It Recruit - Hospitality,2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-remote-wfh-at-get-it-recruit-hospitality-3563649404?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=fqjzy1aZ3pJXVe28SAttYA%3D%3D&position=20&pageNum=4&trk=public_jobs_jserp-result_search-card,1859,"Looking to join an exciting and dynamic team that is driven by data, not hunches? Our company is currently seeking a skilled and collaborative Data Engineer to join our remote team anywhere in the USA. At our company, we deliver next-generation insights to help drive business decisions in a variety of industries including healthcare, campus, corporate, sports, entertainment, hospitality, and retail.

As a member of our Data Engineering team, you'll have the opportunity to work with a talented and experienced team to build cloud infrastructure and pipelines that deliver data solutions to our clients in the Food, Beverage and Sports industries. Using a tech stack that includes DBT, Snowflake, Python, Spark, Docker, Airflow, and a range of AWS services like SQS, SNS, S3, Redshift, Glue, you'll have the opportunity to make code decisions and bring best practices to the team.

To be considered for this role, you'll need to have at least 2 years of hands-on Python backend or Data Engineering experience, and experience with DBT, SQL, data validation and testing. Additionally, experience with DevOps and CI/CD technologies and methodologies, Snowflake data platform, and AWS Cloud Infrastructure is valued.

We offer a competitive salary range of $85k-$100k plus company benefits, including medical, dental, vision, life insurance/ AD, disability insurance, retirement plan, paid time off, holiday time off, associate shopping program, health and wellness programs, discount marketplace, identity theft protection, pet insurance, commuter benefits, employee assistance program, and flexible spending accounts (FSAs).

If you're a team player who loves working with data and building solutions, apply today and join us at our company where we are the spark that ignites!

Employment Type: Full-Time

Salary: $ 132,000.00 207,000.00 Per Year
Show more "
3572796805,Data Engineer,Encora Inc.,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-encora-inc-3572796805?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=4nKIJjfNa9XzbZRThXb%2FRg%3D%3D&position=21&pageNum=4&trk=public_jobs_jserp-result_search-card,1026,"Minimum of 7 years and maximum of 10 years of working experience in total.

Strong PL/SQL Development Skills.

Proficient in database development utilizing partitions, dbms jobs, packages, procedures, function and Oracle user defined types.

Proficient in working with UI teams/applications and using cursors, XML, JSON etc.

Strong experience in developing end-to-end jobs as part of ETL.

Strong SQL and PL/SQL tuning skills.

2+ years of experience with Python, exposure to DB connect libs and others like requests, json, pandas etc.

Willing to work from EOG Houston Office on all 5 days.

2+ years experience in Shell scripting.

Nice to have:

Familiarity with Oracle Exadata

Familiarity with MemSQL & Fast API is a big plus

Experience with big data tools: Hive, Sqoop, Spark etc.

Familiarity with MemSQL, No-SQL DBs and Big Data technologies is a big plus

Bachelor's degree in Management Information Systems or Computer Science

Oil and Gas Industry experience

Good written and oral communication skills
Show more "
3579881252,DATA ENGINEER,Reverse,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-reverse-3579881252?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=Xr9tHQY8o1ClmI4pQcuo0Q%3D%3D&position=22&pageNum=4&trk=public_jobs_jserp-result_search-card,1575,"Per un’innovativa compagnia internazionale di telecomunicazioni, siamo in cerca di un/una:




Data Engineer




Nello Specifico, Sarai Coinvolto/a Nelle Seguenti Attività




Entrerai a far parte attivamente del team di Software and Data Engineering e sarai responsabile dello sviluppo e della manutenzione di sistemi aziendali.




progettare un'architettura per centralizzare correttamente i dati e renderli disponibili a strumenti di analisi e visualizzazione;
lavorare a stretto contatto con gli altri dipartimenti aziendali per condividere gli stati di avanzamento;
collaborare in modo efficace con i team di BI e Data Science;
utilizzare nuove tecnologie in un processo di apprendimento continuo.




Tra i Requisiti Necessari Per Questa Posizione




almeno 2 anni di esperienza in team di Data Engineering;
Conoscenza di Data Lake e/o Data Warehouse;
Esperienza con database relazionali SQL (Postgres, MySql), NoSQL (Cassandra ElasticSearch, MongoDB) Hadoop/Big Data e concetti di data modelling;
Esperienza con il linguaggio di programmazione Python;
diploma e/o laurea in informatica.




Tra i Requisiti Nice-to-have




Esperienza con sviluppo Agile;
Esperienza con metodologie DevOps;
Esperienza con Docker/Kubernetes;
Conoscenza di machine learning e concetti MLOps;
Conoscenza di framework backend basati su Python (Django, Flask, FastAPI)




L’azienda è riconosciuta per la sua organizzazione informale, l'attenzione all’etica e la modernità degli uffici.




Sede di lavoro: Milano con possibilità di smart working (fino a 8 giorni al mese).




Show more "
3576817439,Data Engineer,Kalderos,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-kalderos-3576817439?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=99C9DAx%2BoHrMUFFuwRHlbg%3D%3D&position=23&pageNum=4&trk=public_jobs_jserp-result_search-card,4388,"About Our Organization

At Kalderos, we are building unifying technologies that bring transparency, trust, and equity to the entire healthcare community with a focus on pharmaceutical pricing. Our success is measured when we can empower all of healthcare to focus more on improving the health of people.

To learn more: https://www.kalderos.com/company/about

That success is driven by Kalderos' greatest asset, our people. Our team thrives on the problems that we solve, is driven to innovate, and thrives on the feedback of their peers. Our team is passionate about what they do and we are looking for people to join our company and our mission.

That's where you come in!

What You'll Do

We are looking for a collaborative Data Engineer. Across all roles, we look for future team members who will live our values of Collaboration, Inspired, Steadfast, Courageous, and Excellence.

Work with product teams to understand and develop data models that can meet requirements and operationalize well
Build out automated ETL jobs that reliably process large amounts of data, and ensure these jobs runs consistently and well
Build tools that enable other data engineers to work more efficiently
Try out new data storage and processing technologies in proof of concepts and make recommendations to the broader team
Tune existing implementations to run more efficiently as they become bottlenecks, or migrate existing implementations to new paradigms as needed
Learn and apply knowledge about the drug discount space, and become a subject matter expert for internal teams to draw upon


What You'll Bring

Bachelor's degree in computer science or similar field or equivalent experience
2+ years work experience as a Data Engineer in a professional full-time role
Experience building ETL pipelines and other services for the healthcare industry


Set Yourself Apart

Experience with dbt and Snowflake
Professional experience in application programming with an object oriented language.
Experience with streaming technologies such as Kafka or Event Hubs
Experience in the healthcare or pharmaceutical industries


Salary Range: $115,000-$140,000

____________________________________________________________________________________________

Highlighted Company Perks And Benefits

Medical, Dental, and Vision benefits
401k with company match
Flexible work schedule and location
Unlimited PTO with a 10 day minimum
24 company holidays, including closures for 1 week in the summer and 1 week in the winter
Opportunity for growth
Annual continuing education stipend
Donation matching for charitable contributions
Bi-annual celebration stipend to spend on celebrating personal or professional wins
Travel reimbursement for healthcare services not available near your home
Doordash DashPass subscription with some company-provided lunches


What It's Like Working Here

We thrive on collaboration because we believe that all voices matter and we can only put our best work into the world when we work together to solve problems.
We empower each other: from our DEI Council to affinity groups for underrepresented populations we believe in ensuring all voices are heard.
We know the importance of feedback in individual and organizational growth and development, which is why we've embedded it into our practice and culture.
We're curious and go deep. Our Slack channel is filled throughout the day with insightful articles, discussions around our industry, healthcare, and our book club is always bursting with questions.


To learn more: https://www.kalderos.com/company/culture

We know that job postings can be intimidating, and research shows that while men apply to jobs when they meet an average of 60% of the criteria, women and other marginalized folks tend to only apply when they check every box. We encourage you to apply if you think you may be a fit and give us both a chance to find out!

Kalderos is proud to be an equal opportunity workplace. We are committed to equal opportunity regardless of race, color, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or veteran status.

We are a remote-first team with offices in Chicago and Milwaukee. Our team members have the option of being fully remote, fully on-site, or a hybrid combination that works best for each individual and their role.

Kalderos participates in E-Verify.
Show more "
3557886754,Data Engineer,SSB,2023-04-05,https://www.linkedin.com/jobs/view/data-engineer-at-ssb-3557886754?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=qY2Quq9WbDTvPhyFRoiLgw%3D%3D&position=24&pageNum=4&trk=public_jobs_jserp-result_search-card,5933,"Description

Affinaquest (AQ) is the market leader in institutional advancement, fan engagement, and nonprofit fundraising through its market-leading data warehouse & analytics, and empowering Constituent Relationship Management (CRM) technology. AQ partners with institutions of higher education, collegiate athletics, and nonprofit organizations to deliver a constituent-centric, personalized experience in support of our clients’ missions.

Our enterprise-class cloud platform and open APIs deliver critical capabilities that support development campaign success by facilitating meaningful engagement based on rich data and keen insights. AQ solutions easily integrate with existing client systems and external partners, enabling an extensive and open ecosystem that leverages existing institutional investment, accelerates innovation, and delivers a frictionless personalized constituent experience.

In 2018, Strattam Capital made a majority investment in Affinaquest, and since then, has added several strategic acquisitions. Strattam Capital invests in founder-led, independent, B2B software and technology companies across North America. For more information, please visit: https://strattam.com/

KEY FACTS

#1 established and/or #1 growing player in its respective verticals.
The preferred next-gen Advancement solution of the “Top 300” fundraising institutions.
Serving over 50% of the Power-5 athletics programs
10+ years in data warehousing, data quality, lifecycle hygiene, analytics, and CRM

The successful candidate will work on our data services product line and perform feature development as we build a new product. As well as provide data services to our clients (NCAA).

You will be the data engineer on our Data Services team which consists of our lead data scientist and our Product Manager. You will play a heavy role in modernizing the current data stack architecture and delivering valued results to the backend of our product. You will have peers in our Product Engineering organization so that you can collaborate and provide input for how our products evolve into the future as a “platform” and serve as a presentation layer to your results. We are currently modernizing our application backend architecture from a single tenant Azure SQL Server tech stack to a more scalable architecture that uses Snowflake and Apache Airflow, so you will play a heavy role in also modernizing our data stack so that we are delivering solutions in a unified approach.

Position Description

You will learn more about our data and clients’ data so that we can provide valued services to them within our processing and modeling. You will work closely with our Product Leaders, our business stakeholders who have insights into data trends and outliers, and our Lead Data Scientist. This role will own the building and delivery of predictive analytics features and services where you are the data expert. You will participate in Scrum ceremonies with your team. You will collaborate with your delivery team peers and the Product Manager to assist creation and refinement of user stories and acceptance criteria. This team performs both, net new feature Product development as well as data science services directly to our clients.

KEY RELATIONSHIPS

Reports to VP of Product Engineering

Other key relationships- Product Leaders, and the Product Delivery Team

Key Responsibilities

Champion and write well-designed, reusable, testable code, based on industry-proven best practices.
Ability to collaborate with software engineers and data scientists to assure data availability and correctness
Experience with software development best practices including unit testing and code review processes
Strong understanding of the full lifecycle of data engineering, through maintenance and monitoring
Ability to discuss technical, modeling practices with technical and non-technical stakeholders.

Requirements

MANDATORY REQUIREMENTS TO APPLY

BS degree in data science, computer science, engineering, or equivalent experience
Ability to collaborate with software engineers and data scientists to assure data availability and correctness
Experience with software development best practices including unit testing and code review processes
3 years+ in Python programming and Python coding best practices using OOP and vectorize operations
2 years+ of professional SQL skills including advanced data manipulation experience with window functions, common table expressions, and the like
1-2 years experience with data warehousing --- ideally Snowflake --- relational data tables, and working with data from disparate sources
1+ years in the building and maintaining data pipelines between SQL databases, API calls, flat files, and external sources
Strong understanding of the full lifecycle of data engineering, through maintenance and monitoring
Pragmatic approach to problem-solving that prioritizes delivering working solutions to real problems
US-Based

Ideal Attributes And Proven Capabilities

Azure Data stack experience and/or Snowflake Snowpark experience
Experience with time series and/or ticketing data, the sports domain, or retail analytics
System-level design and implementation of ETL/ELT pipelines, feature engineering, and data management
Cloud-hosted model development and deployment experience, preferably with Azure
Distributed computation with tools like Spark or Dask
Experience managing modern data tools like dbt, dvc, Fivetran, Airflow, DuckDB
Experience with ticketing data, the sports domain, or retail analytics
Experience with time series data
Tableau dashboards and KPI tracking

POSITION LOCATION

Scottsdale, AZ, or Remote

Affinaquest is an equal-opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.

Salary Description

$165k - $200k
Show more "
3573066130,Data Engineer - Remote,SECU Credit Union,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-remote-at-secu-credit-union-3573066130?refId=o%2FdbI2O0KnS78%2FKMuazFdw%3D%3D&trackingId=otoPCF1%2FZYYxaZxKyqeFfw%3D%3D&position=25&pageNum=4&trk=public_jobs_jserp-result_search-card,5672,"You're the best at what you do. So are we. Imagine what we can achieve together!



We are looking for dynamic professionals with a passion for data innovation to join our SECU team as a Data Engineer.



 



*This position is 100% remote for candidates ideally living in the EST time zone*



**Occasional in-person meetings and commitments throughout the year are required**



 



The Data Engineer designs, develops and maintains data structures and pipelines within Snowflake to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.



 



The Data Engineer plays an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, the data engineer ensures that data sets are of high integrity while in proper alignment with business needs.  Will identify opportunities for efficiency, create recommendations, and enact solutions to automate and streamline existing processes.



 



This position is remote with occasional in-person requirements at SECU’s headquarters in Linthicum, MD. This role reports directly to the AVP, Business Intelligence.



 



Duties and Responsibilities:



Implement data structures and pipelines within Snowflake to facilitate the development of reports, visualizations, data sets and analytical models.
Incorporate additional external data sources with full pipeline integration to support data warehouse initiatives.
Identify opportunities to automate and enact solutions that streamline existing processes.
Utilize version control systems such as Git as part of a CI/CD code pipeline which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.
Perform extensive quality control on all data pipelines through automated processes and manual analysis, including check-ins with subject matter experts as needed.
Maintain a continual understanding of current data engineering technology and methods within the analytics world and how it compares with SECU’s current capabilities.
Interview business users and analysts to understand the use cases that integration solutions must support.
Learn and apply data best practices to ensure data quality, performance and integrity are optimized for SECU’s needs.
Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization.
Sustain a comprehensive knowledge of analytics tools (such as Power BI, SQL, JupyterLab, statistical languages/packages, or other tools to easily analyze and visualize data) and apply knowledge to help build out more dynamic, automated, and easy to consume reporting for key stakeholders.
Solve data questions requiring advanced transforms, custom queries, statistical analyses, or other related tasks outside the realm of possibility for a self-service tool.
Create ad hoc scripts to extract meaning from the underlying data and answer relevant business questions.
Communicate results that address the requester’s core business questions and provide key insights and relevant action items.
Document the process undertaken for auditing and future reproducibility.

 



 



Education and Qualifications



Bachelor’s degree or greater in any of the following or similar areas: Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience may be substituted in lieu of a degree.
3 - 5 years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools
Demonstrated experience with self-directed work on large and complex problems.
Experience working in a highly collaborative, team environment.
Experience communicating among multiple stakeholders and balancing multiple projects.
Snowflake and Power BI a plus
Software/data engineering experience with Python
Advanced SQL Skills
Experience with modern data warehouse frameworks, tools and architectures
Basic understanding of statistical techniques and concepts
Innovative, creative and forward thinking; solutions-driven
Strong interpersonal and communication skills
Ability to work alone or in teams as appropriate

SECU is Maryland's largest Credit Union and our guiding principles define our culture. We are member centered and employee focused, know relationships generate outcomes, choose right over easy, and put the heart in banking.



 



Apply today and be part of our journey!



 



In addition to never being controlled by outside owners, one of the great perks of joining Team SECU is our total rewards package for all employees working 20+ hours per week, which includes:



Market competitive pay
Robust 401(k) retirement savings program
Generous paid leave programs
And more…. SECU 2023 Benefits Guide

SECU is committed to supporting your health and well-being - and maintaining a safe and healthy work environment. 



 



Effective January 10, 2022, SECU requires that all successful applicants be fully vaccinated against COVID-19 as a condition of employment and provide proof of such vaccination prior to commencement of employment.



 



To learn more about what it is like to work at SECU please visit our career portal - secumd.org/careers



 



If you’re interested in a challenging and rewarding career then SECU is for you!



We can’t wait to get to know you!



SECU is an Equal Opportunity Employer



 



Show more "
3567948411,Data Engineer,CVS Health,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-cvs-health-3567948411?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=fredAPjRADhXw3m9nENYEQ%3D%3D&position=1&pageNum=5&trk=public_jobs_jserp-result_search-card,4628,"Job Description




At CVS Health, everything starts with our vision to reimagine health care by putting people at the center of all we do and redefine health care in America. Our strategy is focused on making bold moves to deliver solutions that are personalized, connected and increasingly digital.




To execute on our vision and strategic imperatives, the Enterprise Data Services & Operations (EDSO) team is focused on building a modern, connected data ecosystem that accelerates the use of data, analytics, and advanced AI. We are looking for passionate and motivated Data Engineers to join our growing team of engineers. We are on a mission to transform technology and health care and you will play a pivotal role in executing our strategies and making our vision come true.




Responsibilities:




Develops scalable, repeatable and secure data structures and pipelines to organize, collect, standardize and integrate data that facilitate more complete, accurate and consistent data for insights and reporting
Builds efficient, high quality, integrated and increasingly real-time ETL/ELT processes using tools, programming languages and services
Delivers data capabilities and new data products to accelerate investments in data and democratize data across the enterprise and within the Health Care Business unit
Collaborates with data science team to transform data and integrate algorithms and models into automated processes
Builds data models, data marts and extracts to support Data Science, internal customers and third-party administrators
Analyzes data, understands complex systems and solves challenging data sourcing and integration problems
Promotes engineering excellence by simplifying, optimizing, and automating processes and workflows
Innovates by leveraging and practicing agile techniques to achieve efficiency, boost productivity and improve processes





Pay Range




The typical pay range for this role is:




Minimum: $ 70,000




Maximum: $ 140,000




Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will take into account a wide range of factors, including location.




Required Qualifications




2+ years of hands-on experience building and deploying data pipelines using Python, PySpark or any ETL tool.
Working experience in Hadoop or Relational databases with strong understanding of HQL/SQL.





Preferred Qualifications




Experience building ETL data pipelines in Ab-Initio or equivalent technology
Experience in GCP Cloud services such as Big Query, Composer, Airflow, DataProc.
Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment
Experience with bash shell scripts, UNIX utilities & UNIX Commands, CI/CD
Agile practice experience with methodologies like SAFe.
Healthcare domain experience.





Education




Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline




Master’s degree or PhD preferred




Business Overview




Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
Show more "
3571677695,Data Engineer,CryptoRecruit,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-cryptorecruit-3571677695?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=DpZnaXC7xZOzGKz0JzvPuQ%3D%3D&position=2&pageNum=5&trk=public_jobs_jserp-result_search-card,3009,"Company

The company is looking for a Senior Data Engineer to help build out our suite of

analytics products. As a member of the Data Engineering Team, you’ll work closely with the world’s leading blockchain protocols, developing real-time data pipelines to ingest their data, and identify actionable insights that will help them grow. In this role, you will also play a leading part in continuing to

build out our unique blockchain parsing technology, Chainwalkers.

The engineering team has extensive expertise across data pipelining, distributed databases, at-scale web applications, large-scale front-end applications and data visualisations. They work relentlessly towards our goals, and care a great deal about building quality products with a talented, authentic team.

Responsibilities

Design, build and maintain real-time data pipelines that process blockchain transactions from dozens of different blockchain networks.
Develop data models that translate complex, esoteric blockchain data into standardised formats that are analytics-ready.
Design automated systems that evaluate and parse the results of smart contract calls.
Work alongside the Data Science team to curate and prototype new data-sets to tackle emerging problems.
Lead and scope large technical projects.
Productionalise time-series metrics for our partners and product teams.
Develop systems to monitor the integrity and uptime of data.

Requirements

You possess a strong technical background that includes 5+ years of experience working in a senior engineering position with data infrastructure/distributed systems.
Strong familiarity with blockchain and cryptocurrencies
You have a high bar for the quality of data, the quality of code and ultimately an attention to detail.
You have experience writing, maintaining and debugging ETL jobs that leverage distributed data frameworks such as Spark, Kafka and Airflow.
You are comfortable with the command line, and are not afraid to get your hands dirty with infrastructure and ops when required.
You have extensive experience working with Spark.
You have worked with languages such as Python, Scala and Go.
You have extensive experience working with data warehouses/lakes such as AWS Redshift and Delta Lake.
You are capable of gluing together different services and tools, even if you haven’t previously worked with them.
You have worked in an agile sprint-based manner.
You are relentless when tasked with solving hairy technical challenges.

Remuneration And Benefits

Better than market rate with equity plan

Make sure to follow us here to get our most live jobs https://www.linkedin.com/company/cryptorecruit

Cryptorecruit are the worlds leading specialist recruiter for the blockchain/Cryptocurrency industry. We recruit positions from CEO,CTO, Project Manager, Solidity developer, frontend and Backend Blockchain developers to marketing/sales and customer service roles. Please browse our website and at www.cryptorecruit.com to search all our job vacancies.
Show more "
3564689773,Associate Software Engineer,"REsurety, Inc.",2023-04-10,https://www.linkedin.com/jobs/view/associate-software-engineer-at-resurety-inc-3564689773?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=6I2%2BEfT7wv%2FiVY9999N7Tg%3D%3D&position=3&pageNum=5&trk=public_jobs_jserp-result_search-card,3710,"Position Overview




At REsurety, our focus is on building the analytics engine of the US clean energy economy. As a Associate Software Engineer on our Risk Applications team, you will be building tools to analyze renewable energy markets data and provide insights for sellers and buyers of renewable energy. You will work with other engineers, researchers, and product managers on our team to characterize risk and contribute to the development of renewable energy worldwide.




Key Responsibilities




Design, implement and test applications to analyze decades of time series data
Collaborate with other members of the team to solve challenging engineering tasks on time and with high quality
Build and own applications end to end.
Provide technical support to our internal customers
Maintain and debug our live production systems




Required Experience & Qualifications




Proficiency with Python
Bachelor's degree in Computer Science, Mathematics, Electrical Engineering or related field with equivalent work experience
Experience with git or equivalent source control management tool
Familiarity with software development life cycle
Understanding of SQL query and schema design




Preferred Qualifications




Experience working with database and/or data warehouse technologies (we use Postgresql and Snowflake)
Experience with R
Background in statistics (regression forecasting, conditional probability, population and sampling)
Familiarity with agile development methodologies




Company Overview




REsurety is the leading analytics company empowering the clean energy economy. Operating at the intersection of weather, power markets, and financial modeling, we enable the industry’s decision-makers to thrive through best-in-class value and risk intelligence, and the tools to act on it. Our data and software products offer unprecedented insight into the financial performance and environmental impact of clean energy projects. Our risk-transfer products enable renewable energy buyers, sellers, and financiers to manage the risk inherent to generating power from an intermittent fuel source: the weather. Our clients include clean energy investors, advisors, developers, and buyers.




We are a small team with a big impact! Our culture is open and collaborative. We expect excellence from our team members and reward it with high ownership and flexibility. If you’re a high-achiever with a passion for clean energy, we want to hear from you.




Company Values & Principles




At REsurety, we value the skills of execution, creativity & ownership, commercial focus, and teamwork, and we help and encourage all team members to develop these skills while at REsurety.




Our values also shape our culture and act as the foundation for our principles. Like all great companies, we strive to hire the best and are committed to building a diverse, inclusive company where team members feel engaged, valued, and supported. What is special about REsurety, though, is how much we:




Share information openly, broadly, and deliberately with each other;
Encourage ownership by all team members;
Provide continuous, constructive feedback; and
Empower all team members to bring their full, authentic self to work.




We are in the Boston office every Monday, Tuesday, and Thursday. Learn more about our employee experience and benefits




REsurety, Inc. is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation or any other characteristic protected by law.




Powered by JazzHR




WaeAluIFw2
Show more "
3562827937,Sustainability Data Engineer,Massachusetts Institute of Technology,2023-04-14,https://www.linkedin.com/jobs/view/sustainability-data-engineer-at-massachusetts-institute-of-technology-3562827937?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=cP%2Bte0Mr74qbd3BSkDqTJA%3D%3D&position=4&pageNum=5&trk=public_jobs_jserp-result_search-card,97,Information on MIT’s COVID-19 vaccination requirement can be found at the bottom of this posting.
3573894662,IS Data Engineer,Talent Advisory Group (TAG),2023-03-26,https://www.linkedin.com/jobs/view/is-data-engineer-at-talent-advisory-group-tag-3573894662?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=E%2B7xsK3ACqwiUqDB8jp%2BjQ%3D%3D&position=5&pageNum=5&trk=public_jobs_jserp-result_search-card,6500,"The IS Data Engineer plays a pivotal role in operationalizing and advancing data and analytics for the Company's business initiatives. This involves building, managing and optimizing data pipelines and moving them effectively into production for data and analytics consumers. Consumers include business data analysts, data scientists and other roles that need curated data for data and analytics use cases. The IS Data Engineer ensures compliance with data governance and data security requirements while enabling faster data access, integrated data reuse and acceleration of time-to-solution for the Company's data and analytics initiatives.

Essential Position Functions

Create, maintain and optimize data pipelines as workloads move from development to production for specific use cases.
Manage data pipelines through stages, beginning with ingestion of data sources through integration to consumption for specific use cases.
Utilize innovative tools, techniques and architectures to partially or completely automate tasks in order to minimize manual processes, reduce the potential for error and improve productivity.
Assist with the renovation of data management infrastructure that supports automation in data integration and management.
Partner with other Information Systems teams, business data analysts and other data and analytics consumers to refine their data requirements for initiatives and consumption.
Train data and analytics consumers about data pipelines and preparation techniques to make it easier for them to integrate and consume the data they need for their own use cases.
Apply understanding of data and domains to address emerging data requirements.
Propose innovative data ingestion, preparation, integration and operationalization techniques to optimally address data requirements.
Promote the company's available data and analytics capabilities and expertise to IS staff and department leaders.
Collaborate with and educate staff and leadership on how to leverage data and analytics capabilities to achieve business goals.

Essential Department And Organizational Functions

Propose and implement process improvements.
Meet deadlines for completion of workload.
Maintain agreed upon work schedule.
Demonstrate cooperation and teamwork.
Provide cross-training on specific job responsibilities.
Meet identified business goals that contribute to departmental goals.
Perform other duties as needed.

Knowledge, Skills And Abilities Required

Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management
Strong ability to work with IT and business staff to integrate analytics and data science output into business processes and workflows
Strong ability to partner with data science teams to leverage data science and refine and optimize machine learning models and algorithms
Strong ability to collaborate with data governance, quality and security experts to move data pipelines into production in compliance with applicable standards and certification
In depth knowledge of commonly used database programming languages for relational databases (e.g. SQL)
In depth knowledge of commonly used cloud-based data warehouse platforms (e.g. Snowflake, Redshift, etc.)
Ability to work across multiple deployment environments including cloud, on-premises and hybrid
Ability to work with multiple operating systems and containerization platforms (e.g. Docker, Kubernetes, AWS Elastic Container Service, etc.)
Ability to develop using Microsoft Azure products (e.g. Data Factory, Functions, Databricks, Monitor, etc.)
Ability to work with large, heterogeneous datasets to build and optimize data pipelines, pipeline architectures and integrated datasets
Adept in the use of traditional data integration technologies including ETL/ELT, data replication/CDC and API design and access
Strong ability to work with and optimize existing ETL/ELT processes and data integration, data preparation flows and helping to move them into production
Strong ability to work with analytics tools for object-oriented/object function scripting using R, Python, Java, Scala and/or similar languages
Understanding of business intelligence solutions including working knowledge of commonly used data discovery, analytics and BI software tools for semantic layer-based data discovery (e.g. Tableau, Power BI, etc.)
Strong ability to apply Agile methodologies
Ability to apply DevOps practices and tools and DataOps principles to data pipelines to improve data flows
Knowledge of emerging data ingestion and integration technologies
Possess curiosity and desire for ongoing learning about new data initiatives and how to address them
Possess a high degree of initiative, motivation, self-discipline and good judgment
Knowledge of the basic concepts of managed care preferred
Knowledge of health insurance business entities, relationships and processes preferred

Required

Education and/or Experience

Minimum 5 years' experience in data management required in roles that included all or most of the following functions:
Utilization of data integration, modeling, optimization and data quality improvement processes
Design of data and information architecture that delivers large (multi-terabyte) enterprise data warehouse solutions integrating many heterogeneous data sources
Development utilizing tools such as Microsoft SQL Server, Snowflake and/or similar toolsDevelopment using Microsoft Azure products such as Data Factory, Functions, Databricks, Monitor and/or similar products
Data warehouse technical development that encompasses the data management life cycle and establishes end-to-end data warehousing, data management and analytics architecture

Preferred

Data management experience within the healthcare industry preferred.

Equal Employment Opportunity Policy

Talent Advisory Group provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Show more "
3577818947,Data Engineer - Data Analytics,Costco Wholesale,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-data-analytics-at-costco-wholesale-3577818947?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=ekFhLAceKXGaVlAMElQzJg%3D%3D&position=6&pageNum=5&trk=public_jobs_jserp-result_search-card,5844,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.




The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works in tandem with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality and orchestration.
Designs, develops and implements ETL/ELT processes using IICS (Informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency and quality of data management.
Communicates technical concepts to non-technical audiences both in written and verbal form.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ of hands on experience with Informatica PowerCenter
2+ years’ of hands on experience with Informatica IICS
3+ years’ experience working with Cloud technologies such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB and other big data technologies.
Extensive experience working with various data sources (SQL,Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills required. Solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Scheduling flexibility to meet the needs of the business including weekends, holidays, and 24/7 on call responsibilities on a rotational basis.
Able to work in a fast-paced agile development environment.




Recommended




BA/BS in Computer Science, Engineering, or equivalent software/services experience.
Azure Certifications
Experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
Experience with Git / Azure DevOps
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler




Required Documents




Cover Letter
Resume




California applicants, please click here to review the Costco Applicant Privacy Notice.




Pay Ranges




Level 2 - $100,000 - $135,000




Level 3 - $125,000 - $165,000




Level 4 - $155,000 - 195,000 - Potential Bonus and Restricted Stock Unit (RSU) eligible level




We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States. In some cases, applicants and employees for selected positions will not be sponsored for work authorization, including, but not limited to H1-B visas.
Show more "
3570043798,Data Engineer,Tulip Interfaces,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-tulip-interfaces-3570043798?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=v3aIfpCW%2FSdbm1VpuV2WBw%3D%3D&position=7&pageNum=5&trk=public_jobs_jserp-result_search-card,3489,"Tulip, the leader in frontline operations, is helping companies around the world equip their workforce with connected apps, leading to higher quality work, improved efficiency, and end-to-end traceability across operations. Companies of all sizes and across industries have implemented composable solutions with Tulip's cloud-native, no-code platform to solve some of the most pressing challenges in operations: error-proofing processes and boosting productivity, capturing and analyzing real-time data, and continuous improvement.

A spinoff out of MIT, Tulip is headquartered in Somerville, MA, with offices in Germany and Hungary. Focused on composable, human-centric solutions for industrial environments, Tulip is disrupting the MES category and has been recognized as a World Economic Forum Global Innovator. Tulip has also been named one of Energage's Top Workplaces USA and one of Built In Boston's ""Best Places to Work"" and ""Best Midsize Places to Work"" for 2023.

About You:

You love a good challenge and learning new things.
You have gotten disparate systems working robustly together.
You have delivered complex data engineering solutions.
You're able to own a core part of the product and juggle the different requirements that come along with it.
You are comfortable moving around a large technology stack to understand how those features work and contribute to different parts of the platform.


What skills do I need?

4+ years of experience as a data engineer, fullstack or backend developer
Proficient in python for analytical computing
Highly proficient in SQL distributed data processing pipelines (Spark, Glue, Airflow)
Experience building and maintaining a data warehouse (Snowflake, Redshift)
Works well as a part of a team, with effective communication


Key Responsibilities:

Develop and maintain the data warehouse.
Develop new business metrics and improve existing ones.
Work directly with stakeholders to identify data needs/requirements.
Produce clean, efficient code based on specifications
Test and deploy programs and systems
Work with developers to design algorithms and flowcharts
Integrate software components and third-party libraries
Troubleshoot, debug and upgrade existing software
Gather and evaluate user feedback
Recommend and execute improvements
Create technical documentation for reference and reporting


Key Collaborators:

Engineering, Product Management, Hardware, Commercial Teams

Working At Tulip

We know even great candidates experience imposter syndrome. Even if you don't match every requirement, applying gives you the opportunity to be considered.

We're building a strong, diverse team that values hard work, families, and personal well-being. Benefits of working with us include:

Direct impact on product and culture
Company equity
Competitive benefits package including Health, Dental, Vision, Short-term Disability, Long-term Disability, Life Insurance, AD&D Insurance, Flexible Spending Account (FSA), Commuter Benefits, Parental Leave, and 401(K)
Flexible work schedule and unlimited vacation policy
Virtual company events and happy hours
Fitness subsidies


We are an equal opportunity employer. At Tulip, we celebrate all. Qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Help us build an inclusive community that will transform frontline operations.
Show more "
3563664491,Software Engineer-Early Career,Lockheed Martin,2023-04-13,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3563664491?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=wKDa1ZHQ7blB%2BS2xvOn4Og%3D%3D&position=8&pageNum=5&trk=public_jobs_jserp-result_search-card,7978,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work. With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work. At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Littleton, Colorado is seeking a full-time Early Career Software Engineer.




As a Software Engineer for the Special Programs (SP) Activity Based Intelligence (ABI) organization, you will join a multi-discipline team executing activities required to design and deliver state-of-the-art Intelligence, Surveillance, and Reconnaissance (ISR) mission processing software applications and advanced intelligence producing algorithms. ABI has been accelerating development of new apps to meet critical mission needs of our customers. You will be responsible for the design, development, integration, and testing of new apps. You will also work collaboratively in product teams to successfully deliver mission processing apps to operational software baselines and will directly drive an impact to real-world operations and national security.




This position will support an IRAD that can be worked remote or hybrid until security clearance is in place and then work transitions to program on-site.




Software Engineer Responsibilities Will Include




Designs, develops, documents, tests, and debugs software that contains logical and mathematical solutions to business/mission problems or questions in computer language for solutions by means of data processing equipment
Applies the appropriate standards, processes, procedures, and tools throughout the development life cycle
Applies knowledge of computer hardware and software, subject matter to be programmed in business/mission applications, information processing techniques used, and information gathered from system users to develop software
Corrects program errors, prepares operating instructions, compiles documentation of program development, and analyzes system capabilities to resolve questions of program intent, output requirements, input data acquisition, programming techniques, and controls




Basic Qualifications




Degree in Computer Science, Computer Engineering or other STEM related degree program from an accredited college/university.
C++ and/or Java and/or Python experience
Understanding of Agile Software Development processes
Must be a US Citizen; this position will require obtaining a government security clearance. This position is located at a facility that requires special access.




Desired Skills




Ability to set goals and chase after them with passion and enthusiasm
Excellent written and verbal communication skills, interpersonal skills with Management and Peers
Docker, AWS, and containerization experience
Understanding of Configuration Management processes
Ability to work through difficult technical issues in a collaborative team environment
Active TS/SCI Security Clearance




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Full-time Remote Telework: The employee selected for this position will work remotely full time at a location other than a Lockheed Martin designated office/job site. Employees may travel to a Lockheed Martin office for periodic meetings.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: 4x10 hour day, 3 days off per week




Pay Rate




The annual base salary range for this position in Colorado or Washington is $57,800 - $110,800. Please note that the salary information is a general guideline only. Lockheed Martin considers factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience, education/ training, key skills as well as market and business considerations when extending an offer.




Benefits offered: Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Flexible Spending Accounts, EAP, Education Assistance, Parental Leave, Paid time off, and Holidays.




(Washington state applicants only) Non-represented full time employees: accrue 10 hours per month of Paid Time Off (PTO); receive 40 hours of Granted PTO annually for incidental absences; receive at least 90 hours for holidays. Represented full time employees accrue 6.67 hours of PTO per month; accrue up to 52 hours of sick leave annually; receive at least 96 hours for holidays. PTO is prorated based on hours worked and start date during the calendar year.




This position is incentive plan eligible.




Pay Rate




The annual base salary range for this position in California or New York City is $66,500 - $125,300. Please note that the salary information is a general guideline only. Lockheed Martin considers factors such as (but not limited to) scope and responsibilities of the position, candidate’s work experience, education/ training, key skills as well as market and business considerations when extending an offer.




Benefits offered: Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Flexible Spending Accounts, EAP, Education Assistance, Parental Leave, Paid time off, and Holidays.




This position is incentive plan eligible.




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




At Lockheed Martin, we use our passion for purposeful innovation to help keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel, develop, and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work. We place an emphasis on empowering our employees by fostering an inclusive environment built upon integrity and corporate responsibility.




If this sounds like a culture you connect with, you’re invited to apply for this role. Or, if you are unsure whether your experience aligns with the requirements of this position, we encourage you to search on Lockheed Martin Jobs, and apply for roles that align with your qualifications.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3579881252,DATA ENGINEER,Reverse,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-reverse-3579881252?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=uYFjlEB0A8H3VcI3mU9ftA%3D%3D&position=9&pageNum=5&trk=public_jobs_jserp-result_search-card,1575,"Per un’innovativa compagnia internazionale di telecomunicazioni, siamo in cerca di un/una:




Data Engineer




Nello Specifico, Sarai Coinvolto/a Nelle Seguenti Attività




Entrerai a far parte attivamente del team di Software and Data Engineering e sarai responsabile dello sviluppo e della manutenzione di sistemi aziendali.




progettare un'architettura per centralizzare correttamente i dati e renderli disponibili a strumenti di analisi e visualizzazione;
lavorare a stretto contatto con gli altri dipartimenti aziendali per condividere gli stati di avanzamento;
collaborare in modo efficace con i team di BI e Data Science;
utilizzare nuove tecnologie in un processo di apprendimento continuo.




Tra i Requisiti Necessari Per Questa Posizione




almeno 2 anni di esperienza in team di Data Engineering;
Conoscenza di Data Lake e/o Data Warehouse;
Esperienza con database relazionali SQL (Postgres, MySql), NoSQL (Cassandra ElasticSearch, MongoDB) Hadoop/Big Data e concetti di data modelling;
Esperienza con il linguaggio di programmazione Python;
diploma e/o laurea in informatica.




Tra i Requisiti Nice-to-have




Esperienza con sviluppo Agile;
Esperienza con metodologie DevOps;
Esperienza con Docker/Kubernetes;
Conoscenza di machine learning e concetti MLOps;
Conoscenza di framework backend basati su Python (Django, Flask, FastAPI)




L’azienda è riconosciuta per la sua organizzazione informale, l'attenzione all’etica e la modernità degli uffici.




Sede di lavoro: Milano con possibilità di smart working (fino a 8 giorni al mese).




Show more "
3577530172,Data Engineer,Orange County's Credit Union,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-orange-county-s-credit-union-3577530172?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=sp5dxhHZiBa2SV5ElC5wyw%3D%3D&position=10&pageNum=5&trk=public_jobs_jserp-result_search-card,5315,"Great Opportunity At Orange County's Credit Union

Must reside in the state of CA, AZ, NV or TX.

Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.

Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.

Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.

Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!

More About Orange County's Credit Union

Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.

As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.

Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!

Overview

Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.

Essential Functions

Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).

Technical Must Haves For This Role

5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.
Show more "
3571248038,Data Engineer,Indus Valley Consultants,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-at-indus-valley-consultants-3571248038?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=YhZdjbPaViyVU4Gd8MlNqw%3D%3D&position=11&pageNum=5&trk=public_jobs_jserp-result_search-card,2038,"Req No: 8241

Role: Data Engineer

Location: 100% remote

Duration: 12+ Months

Interview: MS Teams

Note: Online test - Pre Screen consists of 5 questions and a game.

Job Description

Accountable for developing and delivering technological responses to targeted business outcomes. Analyze, design, and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with Product management, Architect team, where needed. Demonstrate the company's core values of respect, honesty, integrity, diversity, inclusion, and safety.

Key Responsibilities

Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with Product management, Architect team and senior leadership
Leverage innovative new technologies and approaches to renovate, extend, and transform the existing Integrations and data platforms
Define high-level migration plans to address the gaps between the current and future state
Contribute to the development of cost/benefit analysis for leadership to shape sound architectural decisions
Experience in design and building batch and real time data integrations using Azure tool stack and Informatica
Analyze technology environments to detect critical deficiencies and recommend solutions for improvement
Draft architectural diagrams, interface specifications and other design documents
Note to Vendors:

Dev Ops /Data Ops, Cloud focused, having strong Data .
Cloud Focused Software Engineer with Databricks, Python and Informatica Experience.
Remote candidates will be considered
Prescreen is 5 questions and a game
Skills: DevOps, Cloud, Databricks, Python, Informatica, Azure.
Show more "
3578557660,BI Data Engineer 100% Remote,ROCKSourceIT Solutions,2023-04-20,https://www.linkedin.com/jobs/view/bi-data-engineer-100%25-remote-at-rocksourceit-solutions-3578557660?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=0hnfClrDRbldIfqqN1xHKA%3D%3D&position=12&pageNum=5&trk=public_jobs_jserp-result_search-card,3555,"Position Overview




The Business Intelligence Data Engineer will be a key member of the IT organization. The Business Intelligence Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, support cross functional teams in generating timely insights. The ideal candidate is a data specialist experienced in designing, developing, and deploying complex data pipelines in Azure Cloud platform




The Business Intelligence Data Engineer will support our software developers, database architects, data analysts, dashboard developers and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The role requires solid technical skills in designing and delivering large-scale enterprise data platforms on Azure Cloud combined with very strong communication skills..




Position Responsibilities include, but are not limited to: Deploy new solutions and configurations to meet business and compliance requirements. Participate in 24x7 on call rotations. Discover current technical standards and best practices (R&D). Deploy security patches, updates, and configuration changes. Manage consultants to ensure compliance with Barnes & Noble engineering and business standards.




Position Requirements: Work with multiple business stakeholders in defining the right data requirements to fulfill growing analytics / insights needs across the enterprise Create and maintain optimal data pipeline architecture Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, optimal cost and performance. Design right infrastructure / compute configuration for optimal extraction, transformation, and loading of data from a wide variety of data sources into ADLS, Databricks and Synapse. Develop data pipelines using PySpark, Python and DB SQL in Databricks in Lakehouse architecture 5+ years of experience in a Data Engineering environment with hands on experience developing ADF (Azure Data Factory) pipelines for an enterprise solution. 3+ years of experience in writing code in Databricks using Python to transform, manipulate (ETL/ELT) data, along with managing objects in Notebooks, Data Lake, ADLS, Azure Synapse. Experience with writing complex SQL Queries, User Defined Function, Stored procedures and Materialized views. Someone who comes from database development background and have transitioned to Azure Cloud/Data Lake/Synapse. Working experience with Azure DevOps and Source controls. Experience working in a large Retail enterprise and understanding of Retail based data and reporting models. Experience with reporting tools like PowerBI/Tableau/MicroStrategy Strong analytic skills related to working with different types of datasets from wide variety of data sources Strong project management and organizational skills Experience supporting and working with cross-functional teams in a dynamic environment Understanding of ELT and ETL patterns and when to use each. Position Qualifications:




Undergraduate degree required (Graduate degree preferred) in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Experience using the following software/tools/services: Azure Data Factory, Azure Data Lake Storage, Azure Databricks, Azure Synapse, SQL, PySpark Experience with relational SQL and NoSQL databases Experience with data pipeline and workflow management tools
Show more "
3571964978,Data Engineer,TEKFORTUNE INC,2023-04-15,https://www.linkedin.com/jobs/view/data-engineer-at-tekfortune-inc-3571964978?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=sC84%2FaVAgQw6Kn%2FVUoTC9g%3D%3D&position=13&pageNum=5&trk=public_jobs_jserp-result_search-card,2447,"Tekfortune is a fast-growing consulting firm specialized in permanent, contract & project-based staffing services for worlds leading organizations in a broad range of industries. In this quickly changing economic landscape, virtual recruiting and remote work are critical for the future of work. To support the active project demands and skills gaps, our staffing experts can help you find the best job for you.

Role: Data Engineer

Location: New York, NY (Onsite 3 days in a week)

Duration: 6+ Months Contract

Job Description

Data Engineer: *

A person that is technical in nature and can work with Celonis back-end system to:

Setup data extractions from other systems by working closely with the IT team to establish connections between Celonis and transaction systems

Ability to understand the transactional system raw data structures and use that information to code transformation scripts in Celonis' Vertical SQL to create the data model

Ability to setup the relevant schedules for data extraction, transformation & data model loading to support the data refresh frequency as dictated by use cases

Work closely with the Data Analyst to update the data model as needed by the use case

Data Analyst

Required skills - SQL. Celonis knowledge

A person that can build analysis views and dashboards in the Celonis front end using the drag n drop feature & Celonis PQL coding language

This person is also the PoC with the business stakeholders thru the Business Liason to understand their exact needs (views, KPIs etc.) and work collaborately to build relevant components.

Conduct data validation sessions to ensure the team sees in Celonis is meaningful and reflects the raw data in accurate transformed manner. These validations are done by collaborating with business stakeholders.

Ability to wireframe a view / dashboard before building it to gain alignment from the business stakeholders but questioning he exact value story it will and how it will be helpful to accomplish business team goals

Collaborate with a Value Architect to conduct value framing sessions where insights for improvements are found and help build operational elements in Celonis that will help the business with day-to-day work.

Skills Required - Celonis PQL Coding Platform.

For more information and other jobs available please contact our recruitment team at . To view all the jobs available in the USA and Asia please visit our website at .
Show more "
3548929829,Data Engineer,Aroma360,2023-03-27,https://www.linkedin.com/jobs/view/data-engineer-at-aroma360-3548929829?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=lpFVpECjStMOWgwKFhCw3Q%3D%3D&position=14&pageNum=5&trk=public_jobs_jserp-result_search-card,3147,"Responsibilities

We are looking for a talented and passionate IT professional with a broad range of experience and specialized skills in database management and data analytics to join our IT and software development team. Here's what you can expect from this role:

Managing databases and data warehouses

Extracting, standardizing, and joining data from a variety of software platforms, including Salesforce CRM and SQL databases

Designing and maintaining ETL scripts

Working with stakeholders throughout the company to analyze requirements for reporting and business analytics

Constructing reports and dashboards to convey relevant business metrics

Contributing to process automation projects

Assisting IT team with overflow support requests

Desired Skills

Experience with any or all of the following languages: Python, JavaScript (inc. React, Angular, and Node.js), PHP, Apex

Strong ability to model business logic and rapidly develop feasible technical solutions in a changing business environment

Knowledge of Unix/Linux shell scripting

Familiarity with Grafana or equivalent data visualization tools

Strong knowledge of SQL; hands-on experience with PostgreSQL is a plus

ETL processes and reporting automation solutions; Talend experience is a plus

Jira or other Kanban-style project management tools

Familiarity with MS Excel or comparable spreadsheets

Experience with ViciDial-based contact center platforms is a plus

Experience with Salesforce and/or NetSuite is a plus

Experience with Google Cloud Platform is a plus

Desired Qualities

Trustworthiness and dependability

Orientation toward quality and results

Willingness to take initiative

Strong expertise in data, project, and time management

Strategic and creative problem solving

Clear and effective communication skills

Consistent accuracy and attention to detail

Comprehensive technical and business knowledge

Desired Education

A bachelor's degree in Computer Science, Data Science, Information Systems, or an equivalent field of study is preferred.

Joining our team comes with a range of exciting benefits to support your health, well-being, and professional growth, including:

Comprehensive health coverage, including dental and vision insurance, to ensure you and your family are taken care of.

Life insurance provides peace of mind for you and your loved ones.

Paid time off, allowing you to recharge and enjoy life outside of work.

Access to a 401(k) plan to help you plan for a secure financial future.

Employee discount to take advantage of great deals on our products and services.

Opportunities for paid training to develop your skills and advance your career.

Fun and exciting company events.

Our organization is an equal opportunity employer and does not discriminate against any candidate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or any other legally protected characteristics. We are committed to promoting diversity, equity, and inclusion in our workplace and welcome candidates from all backgrounds to apply for any open positions.
Show more "
3561437972,Data Engineer,Pyx Health,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-at-pyx-health-3561437972?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=iyD9g6%2BzQdjZ2ZoEdwx8Sw%3D%3D&position=15&pageNum=5&trk=public_jobs_jserp-result_search-card,3574,"Loneliness is a devastating and pervasive problem, fraught with steep social, personal, and economic costs, and Pyx Health is looking for a

Senior Data Engineer with the ability to hit the ground running to help solve it. We're changing the landscape of care, resulting in improved

health. To do so requires ingesting, processing, and transforming large amounts of information. The ideal candidate will be dedicated and willing

to take ownership of the data processing systems, learn quickly, and work in a team environment with humility and compassion.

ONLY CANDIDATES RESIDING IN THE USA MAY APPLY.

Responsibilities

Help make architectural standards a reality and always look for ways to improve.
Manage the hydration and data hygiene of a large data warehouse.
Eliminate technical debt across Database infrastructure; Build and optimize data ETL processes, and flat file ingestion mechanisms.
Maintain and improve existing stored procedures, views, and table schemas.
Administer large, high-volume production database environments.
Work closely with data analytics and development partners to create robust data solutions.
Strive to write code that is high quality and easily maintainable.
Work in a fast-paced, Agile environment where the requirements and needs are constantly evolving.
Keep up with new technologies when sensible and make improvements where possible.
Develop highly available database systems that can sustain operational and disaster scenarios specifically in multi-regional cloud

Required Skills

Is self-motivated, needs very little oversight, can take ownership of the product, and can deliver features on time.
Is humble, approachable, easy to work with and enjoys tackling complex problems.
Is constantly seeking challenges and takes pride in creating solutions that clients love.
Has experience ingesting files with large amounts of data efficiently and expediently using ETL pipelines.
Has experience with Cloud Computing, preferably with Azure using tools like Azure Data Factory, Azure Databricks, and Azure DevOps.
Possesses strong knowledge of Python and relevant data transformation libraries such as Pandas and PySpark.
Has strong SQL development skills including stored procedures, writing performance-optimized views, and query analysis.
Familiarity with version control systems, preferably Git or BitBucket.
Understanding of Data Architecture and how it affects different environments such as Application, Analytics, and Processing.

The Ideal Candidate Will Also Have

History of working in a high speed, aglie environment, using tools like JIRA, Conflucene, or other ticketing systems
Experience performing schema migrations across multiple environments.
Experience securing databases for compliance and security frameworks (e.g. HIPPA, HITECH, HITRUST, PCI, SOC).
Has experience administering RDBMS like SQL Server / MySQL / Oracle.
Interest or experience in developing AI models.
Knowledge of common health care file exchange formats.
Experience working in or with the healthcare industry or healthcare-related products that require compliance with HIPAA.

About Pyx Health

We solve some of the biggest problems in healthcare - loneliness and social isolation.

Pyx Health is a mobile solution that reduces loneliness and social isolation by connecting with your members outside of the traditional care

setting. By providing critical and timely interventions and addressing social determinants of health, we take care of health plan members when

they are most vulnerable, especially after a transition of care.
Show more "
3576705900,Data Engineer,ComResource,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-comresource-3576705900?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=Sz6HGQIHbw5ybZysPKU%2B9g%3D%3D&position=16&pageNum=5&trk=public_jobs_jserp-result_search-card,1849,"ComResource is looking for a Data Engineer.

We need someone to be responsible for understanding, preparing, processing, and analyzing data to make data valuable and useful for operations decision support.

Responsibilities:


Demonstrates problem-solving ability that allows for effective and timely resolution of system issues, including but not limited to production outages
Develops and supports standard processes to harvest data from various sources and perform data blending to develop advanced data sets and analytical cubes and data exploration
Designing and implementing data transformation, ingestion, and curation functions on the GCP cloud using GCP native or custom programming
Designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, etc.
Performing detailed assessments of current state data platforms and creating an appropriate transition path to GCP cloud
Analyzing, re-architecting, and re-platforming on-premise data warehouses to data platforms on GCP cloud using GCP/3rd party services
Optimizing data pipelines for performance and cost for large-scale data lakes


Essentials:


Queries, data exploration and transformation, basic statistical methods
Python scripts
Microsoft SQL Server Integration Services Workflows
Microsoft SQL Server Analysis Services Tabular Models
Focuses on SQL database work with a blend of strong technical and communication skills
Demonstrates ability to learn and navigate in large complex environments
Excel acumen to develop complex spreadsheets and formulas, create macros, and understand VBA code within the modules
GCP stack- Big query, cloud functions, pubsub
CI/CD automation
GitHub
Terraform for infrastructure development and deployment
Data modeling experience
Python
Degree in Computer Science
Show more "
3561912160,"Data Engineer, Power BI",Petrossian Inc,2023-04-11,https://www.linkedin.com/jobs/view/data-engineer-power-bi-at-petrossian-inc-3561912160?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=dlhtunMYMuGR91qbmboTaw%3D%3D&position=17&pageNum=5&trk=public_jobs_jserp-result_search-card,5807,"Description




This position is hybrid, working in Manhattan or Brooklyn, NY or Tampa or Boca Raton, FL.




Job Summary




The ETL (extract, transform, load) Power BI Data Engineer will design, develop and support new and existing BI solutions for users and client reporting. The primary role is to transform data into meaningful and accurate information which the business can easily consume.




Duties And Responsibilities




Support implementation and ongoing maintenance, including data mapping, data gap analysis and remediation, configuration and changes as a result of data source updates.
Identify data requirement and data quality issues from data providers, account services teams and end-users.
Create robust reporting and issue tracking, such as data quality scorecards and dashboards based on key metrics and indicators to monitor progress.
Analyze requirements at sufficient level of detail to allow ETL solution to be developed.
Develop ETL job flows according to company standards for naming, performance, reliability.
Support testing and remediation of defects in newly-developed/modified workflows.
Create Power BI Datasets to support the Analytic Delivery team.
Stay current with industry trends and emerging technologies in data exploration and BI tools, and recommend tools and methodologies for data management, governance and PHI security.
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses, data marts and data lakes).
Tune and troubleshoot processes under development and in production; monitor data integration jobs and correct failures.
Work with Data Architects to augment ERD’s as changes are developed.
Develop, maintain, and extend reusable data components.
Provide timely project and task updates.
Monitor production data integration jobs and correct failures create and manage incident reports as they pertain to data integration processes.
Reverse engineer existing reports and rebuild using Power BI.
Write SQL queries for data extraction.
Execute unit tests, validate to expected results and ensure data quality and accuracy.
Coordinate and perform unit, system and user acceptance testing as well as code deployment / configuration changes to environments.
Follow change management stipulations.




Requirements




Required Knowledge, Skills, and Abilities:




Bachelor’s degree in computer science, engineering or equivalent work experience
2+ years Azure exposure (Any Resources: databases, data factory, Synapse Studio, Storage Account, Power Platform) and ANSI SQL
1+ year data modeling
Preferred:




Experience with Azure connectivity/authentication (service principals, managed identities, certificates;




Power BI Dataset creation/maintenance;




Azure resources: DevOps, Logic Apps, Gen 2 Storace, Purview;




SQL Server, PostGre SQL




Solid working knowledge of standard computer applications including MS Word, Excel, Outlook and PowerPoint
Ability using a computer which includes expert keyboard and navigation skills and learning new programs
Communicate clearly and professionally with internal and external customers
Work effectively as part of a team to achieve established outcomes. Understand other’s roles and empower one another to take responsibility to be successful. Demonstrate a collaborative interaction with peers to reach a common goal.
Demonstrate a collaborative interaction with peers to reach a common goal as well as be a resource to team members and internal/external customers
Pay close attention to detail in all aspects of the job
Make decisions using available resources and sound judgment
Maintain confidentiality and discretion
Identify and resolve problems in a timely manner, gather and analyzes information skillfully
Share knowledge with associates by effectively communicating and providing follow-up
Open to other’s ideas and exhibits a willingness to try new things.
Demonstrate accuracy and thoroughness; monitor work to ensure quality.
Prioritize and plan work activities to use time efficiently.
Adapt to changes in the work environment, manage competing demands and is able to deal with frequent change, delays, or unexpected events.
Follows instructions, responds to direction, and solicits feedback to improve.
Act in such a way to instill trust from management, other associates, as well as customers.




Physical Demands - The physical demands described here are representative of those necessary for an employee to successfully perform the essential functions of this job. Reasonable accommodation can be made to enable individuals with disabilities to perform the essential functions.




While performing the duties of this job, the employee is required to talk and hear. The employee will have prolonged periods of sitting at a desk and working on a computer. The employee may be required to lift/carry objects of up to 20 pounds. The employee is also required to have better than average dexterity to use hands to, handle, or feel; and reach with hands and arms. Specific vision abilities required by this job include close vision, distance vision, color vision, depth perception, and ability to adjust focus.




Work Environment - The work environment described is representative of what must be met by an employee successfully perform the essential functions of this job.




The physical environment is indoors in a controlled climate, office setting. The noise level may be low to moderate.




The duties described are representative, but not restrictive of tasks that may be assigned or of the abilities required to do the job. The description is subject to change at any time. Other related duties may be assigned. This description does not alter the at-will status of employment.
Show more "
3574407938,Data engineer,Diverse Lynx,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3574407938?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=XGolzL61zx8PMpSGOXiOnQ%3D%3D&position=18&pageNum=5&trk=public_jobs_jserp-result_search-card,439,"Job Description




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3552297556,Python Data Engineer,NR Consulting,2023-04-03,https://www.linkedin.com/jobs/view/python-data-engineer-at-nr-consulting-3552297556?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=WbAvD2DMHcCusC5H%2B3yyDw%3D%3D&position=19&pageNum=5&trk=public_jobs_jserp-result_search-card,2101,"Job Description

Minimum Qualifications:

Bachelor's Degree in Computer Engineering or related field
7&plus; years' experience in a data engineering
10&plus; years' experience in data programming languages such as java or python
4&plus; years' experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc.
3&plus; years' experience working in a production cloud infrastructure
Preferred Qualifications:

Proven track record of success directing the efforts of data engineers and business analysts within a deadline-driven and fast-paced environment
Hands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment
Demonstrated experience working in an Agile environment as a Data Engineer
Hands on work with Amazon Web Services, including creating Redshift data structures, accessing them with Spectrum and storing data in S3
Knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval.
Proven results using an analytical perspective to identify engineering patterns within complex strategies and ideas, and break them down into engineered code components
Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry
Experience developing, prototyping, and testing engineered processes, products or services
Proven ability to work in distributed systems
Proficiency with relational, graph and noSQL databases required; expertise in SQL
Must be able to develop creative solutions to problems
Demonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes
Excellent interpersonal skills with proven ability to influence with impact across functions and disciplines
Ability to work independently and as part of a team
Ability to manage multiple projects/deadlines, identifying the necessary steps and moving forward through completion
Skilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio
Show more "
3554717567,Cloud (Azure) Data engineer,Accord Technologies Inc,2023-04-04,https://www.linkedin.com/jobs/view/cloud-azure-data-engineer-at-accord-technologies-inc-3554717567?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=x88rUt%2Bqh9%2Fob6tb3k%2Bxiw%3D%3D&position=20&pageNum=5&trk=public_jobs_jserp-result_search-card,389,"Title: Cloud (Azure) Data Engineer.

Location: Warren, NJ

Duration: 6 months

Position type: Contract/Day one onsite.

Cloud Data Engineer -

Looking for a mid-senior level cloud engineer with the below skill sets.

This position is a onsite position.

Databricks (Python, sql),
Azure Data Factory,
Azure ML,
Databricks ML,
Azure SDK,
Synapse Analytics,
Parque and Delta tables
Show more "
3575215508,Data Engineer - Entry Level,Octo,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-entry-level-at-octo-3575215508?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=RcbPvVMlWpRG8vlBhQrSUA%3D%3D&position=21&pageNum=5&trk=public_jobs_jserp-result_search-card,3764,"You…

As a Data Engineer - Entry Level, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.

Us…

We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.

Program Mission…

The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.

Responsibilities...

Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:

Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team

What We’d Like To See…

0 to 1 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Working knowledge of CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs,
Understanding of foundational ETL concepts
General understanding of implementing data ignorations with in the IC DoD Enterprise.
Experience analyzing and benchmarking data using SQL and Python

Desired Skills:

Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills

Years of Experience: Junior Level (0-4 years)

Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)

Location: Chantilly, VA

Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)
Show more "
3577061897,azure Data Engineer,Arthur Grand Technologies,2023-04-24,https://www.linkedin.com/jobs/view/azure-data-engineer-at-arthur-grand-technologies-3577061897?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=VcvsmgCh0t6ddeMtlSbkCw%3D%3D&position=22&pageNum=5&trk=public_jobs_jserp-result_search-card,2111," Arthur Grand Technologies is currently seeking a highly motivated and skilled Azure Cloud Data Engineer for one of our clients.




Job Title: Azure Cloud Data Engineer - Onsite day 1

Location: Mount Laurel, NJ / Charlotte, NC / Raleigh, NC / Wilmington, DE

Duration: Long Term

Employment: Full Time

 

Job Description :

 

Keys Skills: Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre or SQL Server SSIS or DataStage

 

Must Have

More than 5-12 years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail

Nice to Have

·        Any relevant certifications

 

About Arthur Grand Technologies:

 

Arthur Grand Technologies is a leading provider of staffing and technology consulting services. Our company is managed by a team of professionals who have worked for big 5 consulting firms for 20+ years. We are a minority-owned staff augmentation and technology consulting company.

At Arthur Grand Technologies, we value our employees & contractors and strive to provide them with challenging, interesting work, market-relevant benefits, and opportunities for professional growth. If you have the necessary qualifications, and are excited to join a dynamic team, please send your resume to cassindra.esther@arthurgrand.com for immediate consideration.

Thank you for considering Arthur Grand Technologies. We look forward to hearing from you soon.

Best regards,

Cassindra

cassindra.esther@arthurgrand.com

Arthur Grand Technologies Inc

www.arthurgrand.com

 

Show more "
3580141356,Data Engineer,Oshi Health,2023-04-16,https://www.linkedin.com/jobs/view/data-engineer-at-oshi-health-3580141356?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=Tl0NgD8VedVT6%2FmJF5x0Eg%3D%3D&position=23&pageNum=5&trk=public_jobs_jserp-result_search-card,6365,"Do you love to work with data, finding ways to make it reportable, and building models that will add clinical and commercial value for the future?




Do you want to bring your skills and experience to a growth stage engineering team, and help set us up for smart expansion?




Are you excited by the prospect of having a high-visibility high-impact role in a fast-moving startup?




Are you passionate about healthcare, and looking to create a revolutionary new approach to digestive healthcare with a radically better patient experience?




If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company.




Oshi Health is revolutionizing GI care with a virtual clinic that provides easy, convenient access to a multidisciplinary care team including a GI Physician, Registered Dietician, Mental Health Professional, and Health Coach that takes a whole-person approach to diagnosing, managing and treating digestive health conditions. Our care is built on the latest evidence-based protocols and is delivered virtually through an app, secure messaging and telehealth visits with the care team.




NOTE: Oshi is a fully remote company, with team members all over the US.




What You’ll Do




This role will be a perfect fit if you enjoy learning the entire stack and taking on interdisciplinary challenges. A primary focus will be building out a data engineering program, including ETL, data governance, sanitization, and data operations. This part of your responsibilities will be a balance of executing data operations, and building automation to make those operations scalable.




You will also have the opportunity to join engineers on the frontend end (React Native and React.js), backend (Node.js Lambdas) and Salesforce to help build the Oshi platform. Experience with any of these technologies is a plus, but more important is an enthusiasm to adapt and learn the ones that are new to you.




What you’ll do: build the Oshi data program




Implement and maintain data pipelines using Stitch, Databricks, and other scripting as needed to feed a PostgreSQL schema supporting Tableau reporting
Support users of Tableau by updating data sources or modifying inbound inputs as needed deliver critical BI reports
Own the Oshi data model, ensuring that new features built and new technologies adopted serve the needs of the clinical, commercial, product, and engineering teams
Manage ETL of client eligibility files and other data, to make them available for Oshi use in a secure and timely manner. Wherever possible, replace bespoke processes with automation
Once you have a understanding of Oshi’s requirements, design and implement a data strategy, (with your recommendation of approach and products,) to meet the needs of Oshi’s analytics, commercial, and clinical business lines




Your Work Will Also Include




AWS maintenance and administration
Writing technical documentation to outline designs for forthcoming features, outlining the implementation across all technology layers
Meeting with colleagues in Strategy, Product, and Clinical to support their needs from the Engineering group.
Production support responsibilities (shared with the entire engineering team) responding to alerts in Datadog, reviewing and troubleshooting issues




Our Tech Stack




Mobile Platforms Supported: iOS & Android
Cross-Platform Mobile Language: React Native
Other Languages: React-js, HTML, CSS, Java (Salesforce Apex), Node.js (Lambda)
Systems: Salesforce, AWS Amplify / Cognito / Lambda




Your Profile




A minimum of 3+ years of professional experience
Bachelor's Degree or equivalent experience
Good interpersonal and relationship skills that include a positive attitude
Self-starter who can find a way forward even when the path is unclear.
Team player AND a leader simultaneously.




What You’ll Bring To The Team




Passionate about creating value that changes people's lives
Make low-level decisions quickly while being patient and methodical with high-level ones
Are curious and passionate about digging into new technologies with a knack for picking them up quickly
Adept at prioritizing value and shipping complex products while coordinating across multiple teams
Love working with a diverse set of engineers, product managers, designers, and business partners
Strive to excel, innovate and take pride in your work
Work well with other leaders
Are a positive culture driver
Excited about working in a fast-paced, startup culture
Experience in a regulated industry (healthcare, finance, etc.) a plus




And Perks




We’re revolutionizing GI care — and our employees are driving the change. We’re a hard-working and fun-loving team, committed to always learning and improving, and dedicated to doing the right thing for our members. To achieve our mission, we invest in our people:




We Make Healthcare More Equitable And Accessible




Mission-driven organization focused on innovative digestive care
Thrive on diversity with monthly DEIB discussions, activities, and more
Virtual-first culture: Work from home anywhere in the US
Live our core values: Own the outcome, Do the right thing, Be direct and open, Learn and improve, Team, Thrive on diversity




We Take Care Of Our People




Competitive compensation and meaningful equity
Employer-sponsored medical, dental and vision plans
Access to a “Life Concierge” through Overalls, because we know life happens
Tailored professional development opportunities to learn and grow




We Rest, Recharge And Re-energize




Unlimited paid time off — take what you need, when you need it
13 paid company holidays to power down
Team events, such as virtual cooking classes, games, and more
Recognition of professional and personal accomplishments




Oshi Health’s Core Values




Go For It
Do the Right Thing
Be Direct & Open
Learn & Improve
TEAM - Together Everyone Achieves More




Oshi Health is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.




Powered by JazzHR
Show more "
3580123876,Data Engineer (8267),"Patricio Enterprises, Inc.",2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-8267-at-patricio-enterprises-inc-3580123876?refId=4xGYV5bROGM5jdzRk0W5Ig%3D%3D&trackingId=DJOyNcjCc0%2Fz%2FdjpxxHnVw%3D%3D&position=24&pageNum=5&trk=public_jobs_jserp-result_search-card,3425,"Description




LOCATION: Aberdeen Proving Grounds, MD




STATUS: Contingent




TRAVEL: 10%




CLEARANCE: TS/SCI




BENEFITS: 401K, Life/Health/ Dental/Disability Insurance, Flexible Paid Leave, Student Debt Relief Program and Tuition Reimbursement




DESCRIPTION:.MSK LLC is looking to fill a Data Engineer position in support of Army programs at Aberdeen Proving Grounds.




Primary Duties Will Include (but Not Limited To)




Serves as Data Engineer Rep to Army Data Scientist and Knowledge Managers
Engages with customer to:
Scope new data integrations
Design data models
Perform data cleansing and transformation
Engages with data scientists and users, accessing/leveraging the data foundation to:
Perform data integration
Application troubleshooting
Configuration to support and improve workflows
Engages, discovers, learns and operates within the C5ISR system




Knowledge And Skills




Proficiency with one or more programming languages/technologies such as:
Java
Python (Pandas, PySpark)
GitHub
Hadoop
TypeScript/JavaScript
Analytical mindset and eagerness to solve technical problems
Strong communication skills
Must take ownership and accountability of role/responsibility within team
Active DoD TS/SCI Clearance




Education / Experience




BS in Computer Science / Engineering (or related career field)
Strong military background (preferred Army intel experience)




PHYSICAL REQUIREMENTS AND WORK ENVIRONMENT: This is a normal office working environment.




TRAVEL REQUIREMENTS: %




Patricio Enterprises is a federal contractor subject to the Executive Order on Ensuring Adequate Covid Safety Protocols for Federal Contractors requiring all employees to be fully vaccinated.




By applying for this position, you acknowledge that you will be required to provide proof that you are fully vaccinated upon hire, or to verify that you cannot be vaccinated due to a legally recognized exception to the vaccine mandate set forth in the Executive Order.




Note: An individual is not considered to be fully vaccinated until two weeks after receiving the last vaccine dosage in a vaccine regimen, either single shot or multiple vaccine cycle.




EOE. Protected Veterans/Individuals with Disabilities.Patricio Enterprises Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.




Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities




The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c)
Show more "
3577824107,Data Engineer - Data Science & Analytics,Costco Wholesale,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-data-science-analytics-at-costco-wholesale-3577824107?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=Fx992r79TpPa4DGxEtdo%2Fg%3D%3D&position=1&pageNum=6&trk=public_jobs_jserp-result_search-card,5919,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers”.




The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both written and verbal.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ of hands on experience with Informatica PowerCenter.
2+ years’ of hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
3+ years’ hands on experience with Git / Azure DevOps
Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.
Able to work in a fast-paced agile development environment.




Recommended




Microsoft Azure/similar certifications.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
BA/BS in Computer Science, Engineering, or equivalent software/services experience.




Required Documents




Cover Letter
Resume




California applicants, please click here to review the Costco Applicant Privacy Notice.




Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.




Pay Ranges




Level 2 - $100,000 - $135,000,




Level 3 - $125,000 - $165,000




Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible




We offer a comprehensive package of benefits including paid time off, health benefits — medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States.
Show more "
3569703907,"Data Engineer (AWS, Python, SQL)",Cerotid Inc,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-aws-python-sql-at-cerotid-inc-3569703907?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=AKEno7T0F2TE1nnGXSWB9Q%3D%3D&position=2&pageNum=6&trk=public_jobs_jserp-result_search-card,1111,"Title: Data Engineer

Industry: Banking & Financial

Duration: 12 Months- Long term

Location: NC

Top Skills: Oracle/ DynamoDB Python Looking for SR level candidates 7+ years of experience

Nice to Have Skills: Snowflake API Development Skills Big Data Skills

The Expertise And Skills You Bring

6-9 years experience in Oracle development (PL/SQL)
1+ years of Informatica/ETL experience
At least one year of data modeling experience
3+ years of solutioning experience with one or more of the following:
AWS Cloud environments (development, deployment, and support)
Oracle RDS/Aurora Postgres and Dynamo Databases
Containerization technologies
Application web server technology concepts
Python/PySpark
Experience working in an Agile team setting (Kanban and/or SCRUM)Experience working in a DevOps and CI/CD environmentAWS certification a strong plusSnowflake experience a strong plusA Bachelor's or Master's degree in Computer Science, Information Technology, or equivalent experience

Required Skills

Basic Qualification :

Additional Skills : Only Durham, NC (Preference is for local candidates)
Show more "
3574150329,Data Engineer,Libsys Inc,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-libsys-inc-3574150329?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=chRDL%2Blq5gf4i31RIWOV0g%3D%3D&position=3&pageNum=6&trk=public_jobs_jserp-result_search-card,842,"Hope you are doing great!

This is Reshma, from Libsys. We have a requirement of Data engineer with one of our client. Please go through the job description and share your updated resume if you are comfortable with the JD

4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
Possess a solid understanding of end-to-end Data ingestion, ETL, data analysis, Reporting processes.
Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
Experienced in using dbt for data transformation.
Expertise on Tableau/Metabase for building reports/dashboards
Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems.
Excellent communicator with the ability to work effectively with distributed teams.
Show more "
3576714930,data engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576714930?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=HDcgP8Gp1DWsGogA0ogHsA%3D%3D&position=4&pageNum=6&trk=public_jobs_jserp-result_search-card,2027,"Job Description




Job Title Senior Data Engineer




Relevant Experience & Experience Required (in Yrs) 8+ years




Technical/Functional Skills




MUST HAVE SKILLS
Bachelor or Masters in Software Engineering or Computer Science
8&plus; years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.




Roles & Responsibilities




Hands-on design and develop streaming and IoT data pipelines.




Developing streaming pipeline using MQTT, Kafka, Spark Structure Streaming




Orchestrate and monitor pipelines using Prometheus and Kubernetes




Deploy and maintain streaming jobs in CI/CD and relevant tools.




Python scripting for automation and application development




Design and implement Apache Airflow and other dependency enforcement and scheduling tools.




Hands-on data modeling and data warehousing




Deploy solution using AWS, S3, Redshift and Docker/Kubernetes




Develop storage and retrieval system using Presto and Parquet/ORC




Scripting with Apache Spark and data frame.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3547890757,"SQL Database Development Engineer in Shelton, CT (Preferred) or Remote",Maania Consultancy Services,2023-04-03,https://www.linkedin.com/jobs/view/sql-database-development-engineer-in-shelton-ct-preferred-or-remote-at-maania-consultancy-services-3547890757?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=1YoucOcD7gnpsabzWFdX0w%3D%3D&position=5&pageNum=6&trk=public_jobs_jserp-result_search-card,681,"We requiredSQL Database Development Engineer in Shelton, CT (Preferred) or Remote



If you are interested please send me the updated resume along with expected salary range.



Required Qualifications:



Experience in T-SQL programming under SQL Server 2012 or higher is required.
Experience in PL/SQL programming under Oracle 12 or higher is require.
Experience Azure Data Tools (Data Factory, Databricks, Synapse) is a plus.
Excellent skills in writing, maintaining, testing and debugging code in applicable programming languages.
Experience with enterprise software applications or with reporting tools that require strong database programming support is preferred.
Show more "
3576952894,Data Engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576952894?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=lZGrGjrQmzgwgG1Yz2Cvew%3D%3D&position=6&pageNum=6&trk=public_jobs_jserp-result_search-card,683,"Job Description




Job Description: Data Engineer




Execute on the strategy laid out by the data architect to Build, Manage, Monitor data pipeline (AWS / Azure --> Snowflake/Data Warehouse --> Power BI / Other tools)
Data Pre-processing and post processing




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3572003592,Software Engineer - Entry Level,Lockheed Martin,2023-04-12,https://www.linkedin.com/jobs/view/software-engineer-entry-level-at-lockheed-martin-3572003592?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=sMxyLRLyPjvmkvvQAI4BFA%3D%3D&position=7&pageNum=6&trk=public_jobs_jserp-result_search-card,1759,"By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Kings Bay, GA is seeking a full-time Early Career Software Engineer. In this role, you will be involved in all phases of the software engineering process including capability/requirements definition, design, development, test, and demonstration of applications utilized for short, mid-range, and long-range program planning. In addition to working development activities, the Software Engineer will also be required to support the program planning team in troubleshooting issues, debug software, and employ continuous improvement methodology. As a member of the Program Planning team the candidate will be responsible for supporting all aspects of the Program Planning team’s mission including; supporting the Missile Integrated Support Facility (MISF). This support will include planning all production work and providing metrics of MISF's performance to the schedule and requirements. Typically, employees in this level position will have 2+ years of related experience.
Show more "
3576740780,Data Engineer I,"Toray Plastics (America), Inc.",2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-i-at-toray-plastics-america-inc-3576740780?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=cAk%2BHH5%2B8Gq9H%2BOcJUEFJw%3D%3D&position=8&pageNum=6&trk=public_jobs_jserp-result_search-card,5484,"Job Details




Description




Toray Plastics (America), Inc., is a subsidiary of the Japan-based Toray Group , which manufactures synthetic fibers, carbon fibers, plastics, and chemicals and employs more than 45,000 people in 26 countries. Annual sales exceed US$21 billion.




TPA’s films businesses in Rhode Island and foams division in Virginia are global leaders and provide innovative products that are used worldwide for consumer and industrial applications. TPA is committed to environmental, social, and corporate governance. Learn more by visiting www.toraytpa.com .




We currently have an opportunity available for a Data Engineer l, based at our Rhode Island headquarters.




Primary Function: Analyze and organize raw data from different sources. Build data systems and pipelines from different sources. Evaluate business needs and objectives as it relates to data analytics. Interpret trends and patterns. Conduct complex data analysis and report on results. Prepare data for prescriptive and predictive modeling. Build algorithms and prototypes. Combine raw information from different sources. Explore ways to enhance data quality and reliability. Identify opportunities for data acquisition. Develop analytical tools and programs like Spotfire, Power Bi and Power Apps. Collaborate with data analysts to provide insightful data on several projects. Understand data relationships between systems and create and publish data models. Advise the Analytics team on best practices for modeling data within Power BI. Assist in day-to-day system maintenance for the Business Applications group.




Essential Duties




Develops and maintains a professional relationship with the members of the supported organization providing advice, assistance, training, and guidance in their use of data analytics. Utilizes an in-depth understanding of the supported business to facilitate the integration of the system into the supported organization’s daily activities
Troubleshoots system (application and other) issues identifying and communicating solutions. Conducts required research utilizing appropriate tools to resolve issues in an accurate, timely manner.
Translates high level business requirements into detailed functional specifications and manages any changes to the specifications, if applicable. Assists the business in determining the business requirements and how they should be integrated into the rest of the business landscape (systems, processes, etc.).
Through a developed understanding of the supported business and system capabilities, recommends the adoption of system solutions that enhance operating efficiency by enabling and configuring the system’s functionality to support the development of strategies and standards.
Designs, creates, tests and supports complex system integrations (SSIS, SSAS, Visual Studio, stored procedures, special tables, reporting, etc.) that supports the business environment and analytics.
Develops and maintains documentation detailing functional specifications, conversions, upgrades, interfaces, reports, forms and workflow. Ensures end-user documentation is maintained in accordance with established standards.
Develops and maintains data systems and pipelines development using (SQL Server Views and Stored Procedures, SQL Server Integration Services (SSIS), Visual Studio C# or VB .Net, etc.)
Develop and maintain data models within Power BI and SQL Server Analysis Server (SSAS)
Perform basic system functions to support day-to-day operations with the Business Applications group
Understand how systems relate to create complete data models for analytics




Skills & Qualifications Required




Knowledge of programming, troubleshooting, knowledge of ERP Systems, Manufacturing Execution Systems (MES), Production Scheduling System, etc. as they relate to data integrations
Strong diagnostic, analytical and troubleshooting skills required
Must have excellent organizational and time management skills
Must have excellent interpersonal and communication skills
Must be able to support a 24x7, 365 day work environment, thus be available for supporting users off-hours when necessary




Education & Experience Required




Bachelor Degree in a relevant field required
0-2 years experience
High degree of proficiency in a variety of computer programs and applications, software, data processing, communications, database, and systems analysis




Toray Plastics (America), Inc. is committed to the principles of equal employment opportunity and prohibits discrimination based on any protected status, workplace harassment/bullying and retaliation for filing a complaint or providing information related to a complaint. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, pregnancy (including childbirth, lactation and medical related conditions), age (40 and over), national origin or ancestry, physical or mental disability, genetic information (including testing and characteristics), veteran status or any other consideration protected by federal, state or local laws.




It is the policy of Toray Plastics (America) Inc. to maintain a work environment that is safe for all persons, including the community, and conducive to attaining high work standards. To achieve these objectives, we are committed to maintaining a drug, tobacco and alcohol-free workplace and perform pre-employment testing.
Show more "
3576259125,Data Engineer,D1 Brands,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-at-d1-brands-3576259125?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=sL4BQAK%2Bc0%2FXhif%2FFauVCQ%3D%3D&position=9&pageNum=6&trk=public_jobs_jserp-result_search-card,4412,"A Little Bit About D1




D1 Brands is a global operator and acquirer of 3rd-party consumer products brands. Starting from a small group of Amazon-native sellers, we’ve built D1 into a global consumer brands platform with a vision to define the future of 21st century retail. Since our founding in late 2020, we’ve expanded the D1 Brands family into more than 110 talented professionals located around the globe; we’ve raised more capital in our first year of business than most start-ups raise in a lifetime; and we’ve partnered with dozens of exceptional entrepreneurs to help protect and grow the brands they’ve built. But, as our namesake indicates, we believe it is always “Day 1”, so if you’re interested in helping us build the future of digital commerce, we’d love to meet you.




Here Are a Few




The D1 team comes from a wide variety of professional backgrounds and countries, but we all share a few core beliefs.




We are fanatical about creating value for our brand sellers, customers, and D1 team




members




We’d rather write the rules than follow someone else’s – there is always a better way




to do things




When it comes to building a world-class business, we shouldn’t compromise between




hard work and enjoying the journey – if we’re not having fun, we’re failing




About The Role




At D1 our Data Engineer role is end-to-end, from unearthing data at its source to insights and automation for the team. Data is at the center of everything we do, and you’ll be a key partner to teams across the business. This role involves integrating data from a variety of sources, designing and building scalable pipelines, transforming data for use, and generating insights and tools that power the business. We’re entrepreneurial and enjoy experimenting with and leveraging the industry’s latest capabilities.




Responsibilities




Build and maintain scalable infrastructure to provide teams seamless access to data and insights
Extract, prepare and ingest data from various sources, set up ETLs to ready data for efficient access
Monitor and Audit data flows regularly to ensure accuracy
Understand and document underlying data from multiple sources
Engineer the most efficient data housing, transformation and querying
Manage self-service tools and model data for end users across the company to explore a wide variety of data with little assistance
Leverage emerging technologies to automate insights, workflows and tasks
Partner with teams to automate actions based on data, engineer creative solutions for passing data between tools




Qualifications




3+ years of experience in data roles at high growth companies
Bachelor's or Master's degree in Computer Science, Data Science, or a related field preferred
Experienced in SQL, ETL/ELT, data modeling / transformation, data collection and instrumentation
Experience with cloud-based data storage and processing technologies, snowflake, fivetran, and dbt experience preferred
System monitoring and alerting, and dashboarding experience
Self starter comfortable in fast past environments passionate about learning, experimenting and having an impact




What else does D1 have to offer?




Do you offer any benefits to your employees?




We do offer medical, vision, dental, short term disability and life insurance benefits for U.S. employees. We offer medical stipends for our international staff. We also offer unlimited PTO, a generous parental leave, and remote work options.




Some Other Fun Perks




Monthly company socials to get to know your fellow D1ers
An in-house resource library
Opportunities for professional development




What if I don't see a job description that fits my background?




If this position isn't a fit for your background, but you are still interested in D1 Brands please consider dropping your resume to our General Consideration posting. Our company is growing fast and our Talent Acquisition Team does review all resumes.




If you are at least a little intrigued at the potential of working at D1, drop your resume!




We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Show more "
3579845932,Junior Software Engineer,Test Yantra,2023-04-21,https://www.linkedin.com/jobs/view/junior-software-engineer-at-test-yantra-3579845932?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=fizRbHoJcYCOKMSykcKupw%3D%3D&position=10&pageNum=6&trk=public_jobs_jserp-result_search-card,1339,"Job Title: Junior Software Engineer Location: Chicago/ Aurora/ Joliet




Experience: 0-1 Yrs. Job Overview: We are seeking a talented fresher software engineer to join our team. As a software engineer, you will collaborate with a team of developers and engineers to design, develop, and maintain software solutions. You should have a strong foundation in computer science and programming concepts, and be able to work in a fast-paced, collaborative environment. Responsibilities:




Collaborate with a team of developers and engineers to design and develop software solutions
Write clean, maintainable, and efficient code
Test and debug software solutions to ensure they meet quality standards
Participate in code reviews to maintain code quality and ensure adherence to coding standards
Contribute to the development of technical documentation, user manuals, and other materials as needed Requirements:
Basic knowledge in one or more programming languages such as Java, C++, Python
Basic understanding of data structures and algorithms
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills Preferred Qualifications:
Bachelor's degree in Computer Science or related field If you are a passionate software engineer who is eager to learn, we encourage you to apply for this role.
Show more "
3576700571,"Data engineer with Python coding experience(Richmond, VA)",Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-with-python-coding-experience-richmond-va-at-diverse-lynx-3576700571?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=nu84pLuqpakGXxFaID%2B1%2Fg%3D%3D&position=11&pageNum=6&trk=public_jobs_jserp-result_search-card,1962,"Job Description




Title: Data engineer with Python coding experience




Location: Richmond, VA




Type : Contract




Must Have Skills




Python coding, Spark, Scala,AWS, Snowflake




Required Technologies




Strong Programming experience with object-oriented/object function scripting languages: Python.
Experience with big data tools: Hadoop, Apache Spark etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc. (Nice to have)
Experience with relational SQL, Snowflake and NoSQL databases




Job Description




Candidate with 5&plus; years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have working experience using the following software/tools:




3&plus; years of experience (Mid-level) Strong Programming experience with object-oriented/object function scripting languages: Python
3&plus; years of experience (Mid-level) Experience with big data tools: Hadoop, Apache Spark, Kafka, etc
1&plus; years of experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc. (Nice to have)
1&plus; Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.




Preferred Skills




Python, Hadoop, Apache Spark, AWS, Snowflake/SQL knowledge




Years Of Experience Required




5&plus;




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3557821014,Data Engineer,"APCO Holdings, LLC",2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-at-apco-holdings-llc-3557821014?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=vOwu2%2B9%2F83EmriJ%2FxmpTxA%3D%3D&position=12&pageNum=6&trk=public_jobs_jserp-result_search-card,4172,"Overview




Data is transforming the way businesses think, decide and act. APCO values collaboration and the ability to communicate insights that drive decisions. A successful candidate will have a strong desire to make an outsized contribution to organizational transformation. Candidate must reside in GA, FL or PA.




As a Data Engineer, you will be making the firm's data ubiquitous and accessible. You have to be passionate about technology and driving results. Moving data and calling API that validate and cleanse data are important elements of the role.




Essential Duties And Responsibilities




As a professional, you will be responsible for ingesting, cleansing, and representing data for the Analytics team. Duties include:




Analyze business and application needs to assist with determining and evaluating potential solutions.
Help develop specifications and requirements.
Motivated to deliver on committed timeframes and communicate and address issues as they arise.
Ensure that all production systems are supported and maintained according to departmental standards.
Willingness to learn new technologies and concepts.
Enforce change control processes and controls.
Willingness to learn database management tools currently used to support APCO's database environment.
Assist with the implementing database replication strategies, including database mirroring/Always-On, peer-to-peer, merge and transactional replication.




Education And Experience




BS Degree in Computer Science or other related technical discipline.
At least 2 years of experience with RDBMS databases and familiar with database structures and objects.
At least 2 years of Microsoft SQL Server T-SQL experience, preferably on versions 2008 and up.
At least 1 year experience with SSIS packages.
Azure experience is desired.
Replication experience is desired.
Working experience with SSAS and SSRS is strongly desired.




Skills




A data engineer's job is to provide the data used the data architecture and data science teams. The following lists common skills and abilities:




Problem Solving - Ability to work through problems methodically to arrive at optimal solutions. Independent thinking and a comfort with sometimes ambiguous requirements is a must.
Programming Skills - knowledge of scripting languages like JavaScript and Python as well as database query languages like SQL, Hive, and Pig.
Data Cleansing - proficiency in handling imperfections in data such. Collaborate with team regarding when to stage raw data versus methods for handling noisy or inconsistent data.
Tools - ability to recommend the right tools for any given task and guiding organizational thinking on best practices.
Communication - ability to effectively communicate highly complicated, technical, and detailed findings to end users in all areas of our business.




Physical Demands




While performing the duties of this job, the employee is regularly required to type and look at a computer screen for long periods of the day. The employee must be able to sit for long periods of time.




Qualifications




To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations will be made to enable individuals with disabilities to perform the essential functions.




We provide full-time comprehensive benefits packages to all of our employees including: Medical, dental, vision, paid company holidays, paid time off, paid community service day, wellness program, 401K with company match, referral bonuses, discounted gym membership and much more.




APCO Holdings, LLC is a Drug Free Workplace as well as an Equal Opportunity Employer. APCO Holdings, LLC does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.




Job Posted by ApplicantPro
Show more "
3576711062,Data Engineer - Hadoop/ Databricks,Compunnel Inc.,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-hadoop-databricks-at-compunnel-inc-3576711062?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=AkWlVxh5BXfAB1s8pUKjFg%3D%3D&position=13&pageNum=6&trk=public_jobs_jserp-result_search-card,3810,"Description-




Skills




Strong demonstrated skill working with SQL programming




Strong experience in Hadoop and Spark and/or Databricks




Experience in MS SQL Server OR Hana (as source data)




Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts




Demonstrated skills in data modeling, SQL Stored Procedures.




Experience with troubleshooting/ production support (please make sure candidates are aware this is an important responsibility of this position)




Job Description




Our Data Engineering team is looking for innovative data and software engineers to build and evolve our industry leading data products and applications that empower our Data Driven Operating Model and next generation customer experiences.




What You’ll Do




Troubleshoot, support, and tune data products, applications and integrations on large scale data platforms (SQL server, HANA, Hadoop, Databricks etc) with an emphasis on performance, reliability and scalability and most of all quality.




Analyze the business needs, profile large data sets and build custom data models and applications to drive business decision making and customers experience




Develop and extend design patterns, processes, standards, frameworks and reusable components for various data engineering functions/areas.




Collaborate with key stakeholders including business team, engineering leads, architects, BSA's & program managers.




Integrate data from MS-Dynamics 365 application and build reporting solution.




You will have the opportunity to extend your network and collaborate with engineers, architects, and leaders across data management, product engineering, and business leadership teams.




The Ideal Candidate Will Have




MS in Computer Science / related technical field with 7 to 10 years of strong hands-on experience in enterprise data warehousing / big data implementations & complex data solutions and frameworks




Strong demonstrated skill working with SQL programming




Strong experience in Hadoop/ Databricks and Spark




Experience in MS SQL Server OR Hana (as source data)




Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts




Demonstrated skills in data modeling, SQL Stored Procedures.




SQL Server ETL Development experience using SSIS; strong ETL, data warehouse, T-SQL skills.




Extensive experience in implementing large scale data warehouse and data mart architecture and implementation.




Dynamics 365 integration and reporting solution enablement experience is a plus.




Experience on MSBI Stack SQL Server DBMS, SQL Server Analysis Services (SSAS) and SQL Server Integration Services (SSIS) is preferred.




Demonstrated ability to clearly form and communicate ideas to both technical and non-technical audiences.




Strong problem-solving skills with an ability to isolate, deconstruct and resolve complex data / engineering challenges




Results driven with attention to detail, strong sense of ownership, and a commitment to up-leveling the broader engineering team through mentoring, innovation and thought leadership




Demonstrated skill designing, developing and supporting database applications.




Marketing, Sales, Bookings, and finance Domain expertise is a plus




Has agile scrum work experience for executing day to day activities and reporting to scrum team.




Good communication skills across distributed team environment




Must be self-motivated, responsive, professional and dedicated to customer success




Familiarity with streaming applications desired




Additional Skills & Qualifications




Must have experience diagnosing and trouble-shooting production issues




Must have excellent communication and follow-up skills




Education: Bachelors Degree
Show more "
3576883387,Data Science Engineer (All Levels),"Roberts Recruiting, LLC",2023-03-28,https://www.linkedin.com/jobs/view/data-science-engineer-all-levels-at-roberts-recruiting-llc-3576883387?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=QAQ75qaziBcrBOXOxNVBaw%3D%3D&position=14&pageNum=6&trk=public_jobs_jserp-result_search-card,2276,"We love programming and the excitement that comes with building something people use. We are the kind of people that love talking to users and can find the balance between solving a problem quickly and thinking about how your code will work in the future. We love to move fast, keep learning and get stuff done.

Our Data Science team is analyzing large data sets (we’re collecting billions of individual actions every month) and building products that enable our customers to grow faster and communicate more effectively with their customers. We’re looking for data science software engineers to build production features, models and pipelines.

Our data science team is still in its early days and you’ll have a big impact on our direction and how we operate. You’ll be central to shipping products that help our customers learn and grow from their data. We’re looking for people who could play any number of different engineering roles in a cloud-scale SaaS company and are specifically excited by working in data science.

You
Are a strong software engineer with a passion for data science.
Can write scalable and robust production code.
Know how to achieve cloud-scale data processing through parallel, elastic, streaming and similar techniques.
Have experience working on features from conception to deployment to ongoing enhancements.
Are skilled at working with data science packages and tools.
Have experience with or a strong desire to learn statistical, modeling and machine learning techniques.
Aspire to correctness (e.g. in your code, in drawing conclusions from data)
Have a bachelor’s or advanced degree in computer science or other relevant quantitative discipline, or equivalent industry experience.

Technologies we use (not comprehensive!):
Python
Numpy, Scipy, Pandas
Aurora, Cassandra, Kafka
HTML, JavaScript, React
SageMaker

About Us

We are a team of people who are crazy motivated by growth.

It’s what we help our customers do: grow their businesses by making it possible and easy for them to use their data to power better marketing.

It’s how we behave as individuals: we’re all deeply passionate about learning.

It’s how we manage our business: we have thousands of paying customers, we’re profitable, and we’re growing insanely fast.
Show more "
3576575856,Software Engineer Early Career,Lockheed Martin,2023-03-28,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3576575856?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=TGZEVOE0Q34h4bSuutkZNw%3D%3D&position=15&pageNum=6&trk=public_jobs_jserp-result_search-card,708,"Lockheed Martin Space in Littleton, CO, is seeking a full-time Software Engineer. In this role, the candidate will join a team of software and algorithm developers within the Special Programs line of business to develop innovative mission data processing applications. The candidate will collaborate with team members to design, develop, test, and analyze software implementation of algorithms with hardware interfaces.




The successful candidate will have experience and/or knowledge of programming, software, mathematics, and/or related field. Must be a US Citizen; this position will require a government security clearance. This position is located at a facility that requires special access
Show more "
3572797779,Data Engineer,Atlas Health,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-atlas-health-3572797779?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=i5SsJ%2FtAkYW6feKPm9s0GA%3D%3D&position=16&pageNum=6&trk=public_jobs_jserp-result_search-card,3902,"No Patient Left Behind

Some people would rather die than leave their family with medical debt.

This includes people with cancer not showing up to their chemotherapy appointments due to cost, or people stopping treatment because they don't want their spouse to be forced to sell their home in bankruptcy.

Beyond our goal of building a multibillion-dollar business, there's a real mission and soul behind Atlas and that's why we continue to attract incredible people to join us on our journey, whether they be colleagues, customers, or partners.

Patients owe $500B every year in out-of-pocket medical expenses, and there's only two ways for medical providers to collect – either the patient pays or someone else does. We focus on the latter and call it medical financial aid.

Through intelligent matching and enrollment to 20,000 programs, we help healthcare organizations improve access, affordability, outcomes, and health equity for vulnerable populations.

Data Engineers on our data platform team develop systems that manage data flow throughout the Atlas infrastructure and support downstream data applications. These systems are responsible for handling sensitive patient data, and members of the team are technical champions of ensuring HIPAA standards for confidentiality and compliance. Broadly, this role focuses on all elements of data engineering, such as ingestion, transformation, and distribution of data, and works alongside security engineering and business implementation & operations teams as a custodian of patient and company data.

Responsibilities:

Engineer scalable, reliable, and performant systems that manage data
Consume data from a variety of sources and formats, such as flat files, streaming systems, or APIs
Leverage cloud infrastructure to develop scalable data solutions.
Collaborate with platform engineers on creating reliable and secure systems
Collaborate closely with team members and product stakeholders
Create trustworthy, secure, governable, and standardized data components for consumption
Develop readable, well-tested applications, APIs, and libraries
Implement application observability in the form of metrics, logging, and monitoring


Requirements:

Professional experience with cloud-based systems
Experience with data architectures and tools in support of streaming and batch driven data processing
Solid understanding of distributed task orchestration
Demonstrated ability in developing and testing systems that manage data reliability, efficiency, and quality
Understanding of multiple software development paradigms
First-nature comfort in working with containerization tools
Computer Science or related technical degree in a related field or equivalent technical experience


Bonus points:

Depth of knowledge in Google Cloud PlatformExperience working in an ePHI environment
Familiarity with domain driven design concepts


Preferred Qualifications:

Fluency in Python
Experience using Apache Airflow or similar orchestration systems
Experience with IaC tools


Salary: Salary range for this position is $120,000-$165,000 per year

Why Join Our Team:

Because you're motivated by a combination of success, working alongside incredible people, and have a passion for helping clients and patients. Atlas helps people access essential medical treatment, and avoid financial ruin from medical debt. You care about being a part of the journey and wish to play a key role in our organization's success.

Benefits:

We offer a comprehensive benefit plan for our U.S. based employees which includes:

Health, dental and vision insurance
401K
Flexible time off
Paid holidays


Atlas values diversity of all kinds, and we're committed to building a diverse and inclusive workplace where we learn from each other. We are an equal opportunity employer and welcome people of all different backgrounds, experiences, abilities, and perspectives.
Show more "
3580991189,Data Engineer,Prismagic Solutions Inc.,2023-04-22,https://www.linkedin.com/jobs/view/data-engineer-at-prismagic-solutions-inc-3580991189?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=ka%2B6yMO7eklFVmrJ5hQvLA%3D%3D&position=17&pageNum=6&trk=public_jobs_jserp-result_search-card,2561,"As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers' digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Client offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development.

Up to 8 years of software development experience in a professional environment and/or comparable experience such as:

Bachelor's or master's degree in computer science, computer engineering, or other technical discipline, or equivalent work experience, is preferred

5+ years of software development experience in big data technologies such as Python, Spark, PySpark, Spark SQL, Shell Scripting & Hive

Preferred – 5+ Years of hands-on experience in Data Ingestion, Data Organization & Data Consumption frame works using AXP Enterprise Data Platform

Good understanding of big data technologies such as Spark, Mapreduce, YARN, Hive, Zookeeper etc. Preferably, with some real-world experience.

Experience in design and development for batch, streaming and real-time big data applications using AXP EDP platform

Experience in hierarchical data structures in json/ xml

Experience with AXP CI/CD frameworks for code management and deployment like GitHub, Jenkins, XLR

Experience with NoSQL technologies (column-family, key-value or document datastores)

Experience (preferably GCP) and exposure to cloud native big data technologies would be a plus

Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores

REST API design and implementation experience

Good understanding of NoSQL technologies such as HBase, Cassandra, Redis, Memcached etc. Preferably, with some real-world experience.

Ability to effectively interpret technical and business objectives and challenges and articulate solutions

Understanding of SOA, microservices and containerized application concepts would be a plus

Strong analytical skills and programming skills, in production environment.

Hands on Experience on GraphQL a plus

Experience with Production Support/Dev-Ops would be a plus

Willingness to learn new technologies and exploit them to their optimal potential
Show more "
3573066130,Data Engineer - Remote,SECU Credit Union,2023-04-21,https://www.linkedin.com/jobs/view/data-engineer-remote-at-secu-credit-union-3573066130?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=RgBffObJUNtmttjyZ4LNHQ%3D%3D&position=18&pageNum=6&trk=public_jobs_jserp-result_search-card,5672,"You're the best at what you do. So are we. Imagine what we can achieve together!



We are looking for dynamic professionals with a passion for data innovation to join our SECU team as a Data Engineer.



 



*This position is 100% remote for candidates ideally living in the EST time zone*



**Occasional in-person meetings and commitments throughout the year are required**



 



The Data Engineer designs, develops and maintains data structures and pipelines within Snowflake to facilitate the development of reports, visualizations, data sets and analytical models used by analysts, data scientists and other business stakeholders across the organization.



 



The Data Engineer plays an important role in the continued evolution of SECU’s modern data architecture. By collaborating with other data engineers, data scientists, analysts, technical consultants and other business users, the data engineer ensures that data sets are of high integrity while in proper alignment with business needs.  Will identify opportunities for efficiency, create recommendations, and enact solutions to automate and streamline existing processes.



 



This position is remote with occasional in-person requirements at SECU’s headquarters in Linthicum, MD. This role reports directly to the AVP, Business Intelligence.



 



Duties and Responsibilities:



Implement data structures and pipelines within Snowflake to facilitate the development of reports, visualizations, data sets and analytical models.
Incorporate additional external data sources with full pipeline integration to support data warehouse initiatives.
Identify opportunities to automate and enact solutions that streamline existing processes.
Utilize version control systems such as Git as part of a CI/CD code pipeline which includes data ingestion, transformation, orchestration, anomaly detection and metadata management.
Perform extensive quality control on all data pipelines through automated processes and manual analysis, including check-ins with subject matter experts as needed.
Maintain a continual understanding of current data engineering technology and methods within the analytics world and how it compares with SECU’s current capabilities.
Interview business users and analysts to understand the use cases that integration solutions must support.
Learn and apply data best practices to ensure data quality, performance and integrity are optimized for SECU’s needs.
Provide SQL, Python and other programming-specific technical guidance to other engineers and analysts to support continued learning and development throughout the organization.
Sustain a comprehensive knowledge of analytics tools (such as Power BI, SQL, JupyterLab, statistical languages/packages, or other tools to easily analyze and visualize data) and apply knowledge to help build out more dynamic, automated, and easy to consume reporting for key stakeholders.
Solve data questions requiring advanced transforms, custom queries, statistical analyses, or other related tasks outside the realm of possibility for a self-service tool.
Create ad hoc scripts to extract meaning from the underlying data and answer relevant business questions.
Communicate results that address the requester’s core business questions and provide key insights and relevant action items.
Document the process undertaken for auditing and future reproducibility.

 



 



Education and Qualifications



Bachelor’s degree or greater in any of the following or similar areas: Data Science, Computer Science, Management Information Science, etc. Equivalent professional experience may be substituted in lieu of a degree.
3 - 5 years of data-centric programming experience as a software/data engineer, including at least one year of experience implementing data pipelines using modern tools
Demonstrated experience with self-directed work on large and complex problems.
Experience working in a highly collaborative, team environment.
Experience communicating among multiple stakeholders and balancing multiple projects.
Snowflake and Power BI a plus
Software/data engineering experience with Python
Advanced SQL Skills
Experience with modern data warehouse frameworks, tools and architectures
Basic understanding of statistical techniques and concepts
Innovative, creative and forward thinking; solutions-driven
Strong interpersonal and communication skills
Ability to work alone or in teams as appropriate

SECU is Maryland's largest Credit Union and our guiding principles define our culture. We are member centered and employee focused, know relationships generate outcomes, choose right over easy, and put the heart in banking.



 



Apply today and be part of our journey!



 



In addition to never being controlled by outside owners, one of the great perks of joining Team SECU is our total rewards package for all employees working 20+ hours per week, which includes:



Market competitive pay
Robust 401(k) retirement savings program
Generous paid leave programs
And more…. SECU 2023 Benefits Guide

SECU is committed to supporting your health and well-being - and maintaining a safe and healthy work environment. 



 



Effective January 10, 2022, SECU requires that all successful applicants be fully vaccinated against COVID-19 as a condition of employment and provide proof of such vaccination prior to commencement of employment.



 



To learn more about what it is like to work at SECU please visit our career portal - secumd.org/careers



 



If you’re interested in a challenging and rewarding career then SECU is for you!



We can’t wait to get to know you!



SECU is an Equal Opportunity Employer



 



Show more "
3567587480,Data Engineer,GalaxE.Solutions,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-at-galaxe-solutions-3567587480?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=hezgUva0R7fgf122nWjvsQ%3D%3D&position=19&pageNum=6&trk=public_jobs_jserp-result_search-card,4375,"Work with business and technical leadership to understand requirements.
Design to the requirements and document the designs.
Ability to write product-grade performant code for data extraction, transformations and loading using Spark, PySpark
Do data modeling as needed for the requirements.
Write performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive
Implementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS or Hadoop Edge Nodes
Implement Hadoop job orchestration using Shell scripting, Apache Oozie, CA7 Enterprise Scheduler and Airflow
Troubleshooting the issues, providing effective solutions and jobs monitoring in the production environment
Participate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.
Strong development experience in Spark, Py-Spark, Shell scripting, Teradata, Hive and Hadoop
Experience of Ab Initio is a bonus.
Strong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures
Excellent work experience on Hadoop as data warehouse/Data Lake implementations
Experience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)
Unix/Linux Shell scripting (KSH) and basic administration of Unix servers
CA7 Enterprise Scheduler
Experience with AWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)
Databricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration)
Experience in Jira and Confluence
Exercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.
Health care domain knowledge is a plus.




GalaxE is a professional IT services firm that specializes in platform-driven solutions and the use of automation to achieve enterprise business transformation and mission-critical change for some of the largest companies in the world. Using our proprietary solution set, GxFource®, we apply machine learning techniques and predictive analytics tools as part of a broad artificial intelligence strategy that provides effective impact and data-driven business transformation.




Since its founding, GalaxE has been dedicated to advancing the benefits of technology. As we continue that legacy and look to the future, a focus on business enablement through agile, cost-efficient, and effective integration of people, process, and technology anchors our success. We revolutionize change in the costs of doing business that transform companies and their ability to leap beyond the competition.




At GalaxE we value people and are committed to diversity and inclusion where our employees are made to feel comfortable and are encouraged to be authentic. We focus on cultivating both traditional IT and non-traditional, new collar, workers through our Outsource to America®, program.




We are always looking for passionate, entrepreneurial-minded innovators and disrupters; game-changers that take ownership of the work they produce and bring it each and every day. Working with like-minded team members you will get a chance to discover, develop, and use cutting-edge technologies to transform the way we deliver creative business solutions.




Sound like you? Join us and find out for yourself what it means for you, and your career, to be part of the GalaxE team. Let’s build something, together. #WeAreGalaxE




Equal Opportunity Employer/Veterans/Disabled




Physical Requirements




Prolonged periods of remaining stationary at a desk and working on a computer
Must be able to lift to 15 lbs., as needed
Must be able to work on-site (corporate/client offices), as needed (not applicable for 100% remote roles)
Occasionally required to bend, kneel, crouch, and reach overhead.
Hand-eye coordination necessary to operate computers and various pieces of office equipment.
Specific vision abilities required include close vision, the ability to tolerate fluorescent lighting, and the ability to adjust focus.




Employees must be able to perform the physical requirements of the position satisfactorily and, if requested, reasonable accommodations will be made to enable employees requiring accommodations to perform the essential functions of their jobs, absent undue hardship.




For more information, please visit https//www.galaxe.com/
Show more "
3538313665,GCP Data Engineer,ClearChoice Dental Implant Centers,2023-04-03,https://www.linkedin.com/jobs/view/gcp-data-engineer-at-clearchoice-dental-implant-centers-3538313665?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=jbOQ1e%2B3wd%2FUm8DqWc7HnA%3D%3D&position=20&pageNum=6&trk=public_jobs_jserp-result_search-card,4395,"Company Overview




ClearChoice was founded in 2005 to bring an innovative and patient-focused approach to solve gaps within the dental industry. We’ve experienced strong growth and today, we’re the leader in dental implant treatments. Driven by a collective desire to improve the lives of our prospective patients, we help them reclaim their health and confidence. Beyond restoring teeth, this is about getting their lives back.




This mission-focused work has enabled us to achieve four straight years of double-digit company growth, yet we’ve only reached 1% of the population that needs our services. We are searching for individuals who can help us continue pursuing our goal of reaching prospective patients and transforming their lives. When you join ClearChoice, you are joining a team of individuals with passion, conviction, and integrity whose mission is to be the platform of hope for our patients. Come help us write the next chapter of our story!




Data Engineer Summary




We are seeking a Data Engineer who will partner with business, analytics, and engineering teams to build data structures to facilitate reporting, models, and monitoring key performance metrics. Collaborating across disciplines, you will identify internal/external data sources to design data assets, define a data pipeline strategy & automated testing and implement scalable data solutions. We expect you to be aware of best practices and open to working with experienced mentors.




Now is a great time to join us to design and build the future of the ADMI Data Platform. Be a part of a new team building cloud-native solutions with open-source tools and technologies. You will partner with the business to build self-service data assets and modern data science/ analytics solutions. Work with big data and modernize our technology standards to benefit millions of our patients and thousands of employees.




Responsibilities




Work with IT partners to educate and advance adoption of modern data engineering techniques.
Contribute to an evolving cloud-hosted data platform using modern data engineer techniques.
Partner cross-functionally to understand data, reporting, and data asset requirements
Work with engineering teams to collect required data from internal and external systems
Build ETL strategy to build performant data solutions that are reliable and scalable in a fast-growing cloud-native data ecosystem
Rebuild and automate legacy reporting pipelines to a new platform
Contribute toward evolving company’s analytical self-service/ ad hoc reporting/Data Analytics/Analytical Modeling/Data Visualization process and/or product
Develop and conduct automated testing
Author, schedule, and monitor workflows using orchestration tools
Document and publish Metadata and table designs to facilitate data adoption.
Perform pipeline tuning as necessary




Qualifications/Requirements




Bachelor's Degree or equivalent combination of education, training, and experience
1+ yrs. of IT experience
Strong SQL writing skills.
Experience with SQLServer and BigQuery
Hands-on experience with Python and/or other programming languages
Experience with GCP, the various data-related resource types, how to organize, secure, and deploy them.
Experience interacting with REST APIs
Experienced with GIT, CI/CD, and the overall SDLC process.
Experienced with best practices in automated testing and automated pipelines
Familiarity building out and maintaining metrics, alarms, and monitoring of services




Desired Characteristics




Strong desire to work with analytics data and partners
Experience in MS SQL, SQL Server Integration Services (SSIS), and would be a plus
GCP certifications (Solution Architect, Engineering, Big Data Specialty, etc.) would be a plus
Experience/Exposure in Data Science and/or Analytics in cloud would be a plus
Experience in Machine Learning/Deep Learning use cases and development would be a plus
Experience in Data Cleansing/Transformation would be a plus
Experience in Container, Kubernetes, and related technology ecosystems would be a plus
Experience in Data Modeling would be a plus
Experience in Event-Driven Architecture, Streaming processing would be plus
Experience in sourcing and processing structured, semi-structured, and unstructured data would be a plus
Outstanding written and verbal communication skills




Salary Range: 80,000 - 100,000
Show more "
3580138094,Data Engineer,"Double Line, Inc.",2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-at-double-line-inc-3580138094?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=hfAHGPjPFz60Wn0cEBf5%2FA%3D%3D&position=21&pageNum=6&trk=public_jobs_jserp-result_search-card,2970,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)




Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane , not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?




If so, we're looking for a motivated and driven person like you who has:




Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time




Bonus points if you're bringing knowledge of or really want to learn the following:




Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools




In Return, We Offer




A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture




We Do Not Want You To Make The Leap Without Knowing What We Need, So Here Is How We Define Success For This Position




Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days




We need to know - can you make this happen? If so, we definitely need to talk to you.




Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.




Double Line does not currently offer relocation assistance.




Powered by JazzHR
Show more "
3536674451,Data Engineer - Data Analytics,Costco Wholesale,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-data-analytics-at-costco-wholesale-3536674451?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=oknetmYOGBMKrCBDOOGEMw%3D%3D&position=22&pageNum=6&trk=public_jobs_jserp-result_search-card,7177,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale , the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers” .




Responsible for developing and operationalizing data pipelines to make data available for consumption (reports and advanced analytics). This includes data ingestion, data transformation, data validation / quality, data pipeline optimization, orchestration; and engaging with DevOps Engineer during CI / CD. The role requires a grounding in programming and SQL, followed by expertise in data storage, modeling, cloud, data warehousing, and data lakes. The Data Engineer works closely with Data Architects, Data Scientists and BI Engineers to design and maintain scalable data models and pipelines.




The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Supports development of Data Dictionaries and Data Taxonomy for product solutions.
Demonstrates strong understanding with coding and programming concepts to build data pipelines (e.g. data transformation, data quality, data integration, etc.).
Builds data models with Data Architect and develops data pipelines to store data in defined data models and structures.
Conducts ad-hoc data retrieval for business reports and dashboards.
Assesses the integrity of data from multiple sources.
Manages database configuration including installing and upgrading software and maintaining relevant documentation.
Monitors database activity and resource usage.
Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works in tandem with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality and orchestration.
Designs, develops and implements ETL/ELT processes using IICS (Informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency and quality of data management.
Communicates technical concepts to non-technical audiences both in written and verbal form.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ hands on experience with Informatica PowerCenter.
2+ years’ hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies such as ADLS, Azure Data Factory, Azure Databricks, Databricks Live Table, Spark, Azure Synapse, Cosmos DB and other big data technologies.
Extensive experience working with various data sources (SQL, Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills required. Solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Scheduling flexibility to meet the needs of the business including weekends, holidays, and 24/7 on call responsibilities on a rotational basis.
Able to work in a fast-paced agile development environment.




Recommended




BA/BS in Computer Science, Engineering, or equivalent software/services experience.
Azure Certifications.
Experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
Experience with Git/Azure DevOps.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
Excellent verbal and written communication skills.




Required Documents




Cover Letter
Resume




California applicants, please click here to review the Costco Applicant Privacy Notice.




Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.




Pay Range




$155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible




We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States
Show more "
3568541417,Data Engineer,"Torch Technologies, Inc.",2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-torch-technologies-inc-3568541417?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=OttAjVOxuZGOz0wl6SIuQA%3D%3D&position=23&pageNum=6&trk=public_jobs_jserp-result_search-card,5506,"Thank you for your interest in employment with Torch Technologies. We are a 100% employee-owned, Certified Great Place To Work headquartered in Huntsville, AL with over 1200 employee-owners. Our team provides superior research, development, and engineering services to the Federal Government and Department of Defense. As one of the nation\'s top 100 defense companies, the services we provide directly support the men and women who serve our country. Our corporate mission sums up the pride our employee-owners take in the work we do: \""Lighting the Pathway of Freedom\"". And, as a Certified Evergreen ESOP, we have made the commitment to grow and sustain our company for the next 100 years! Come grow with us!




Job Title Data Engineer




Location Colorado Springs, CO 80840 US (Primary)




Job Description




Salary Range: \$121,600 - \$168,040 Please note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, internal equity, as well as candidate qualifications such as skills, education, and experience. Top Reasons to work with us Our team is sought after by leadership and peers to work their most ambiguous ideas and challenges due to our high degree of out of the box thinking, willingness to try something new, and no fear of failure. Our success is driven by every member of the team government and contractors alike striving to do the best they can. We work across the entire MDA Engineering Enterprise working to strengthen and enable every element and component. What you will be doing This position provides data engineering support to enhance the Missile Defense Agency (MDA) Engineering Enterprise program office looking to employ modern digital engineering and data science approaches across the lifecycle and connecting engineering, test, business, contract, and finance data of different types together. The Data Engineer works with MDA engineers to capture and understand both authoritative data and non-authoritative data, its meaning, and relation to other data and how to meaningfully use it to develop data stories that can be automated. This position will identify potential improvements to data sources themselves, collection methods, processes and supporting systems. The position participates in the development of an enterprise-wide system-of-systems engineering effort between the element weapon system program offices and the MDS system offices ensuring that the program offices are integrated and aligned working with weapon system prime contractors to deliver capability. Analysis of engineering and program office activities to help identify ways to apply digital transformation practices. Facilitating adoption of data engineering processes across MDA Engineering Enterprise in the Digital Engineering and Software Development DevSecOps functional areas. Responsible for building data pipelines to pull together data from different source systems. Works closely with data managers, systems engineers, and IT Services to understand architectural path between data sources and sinks across the organization. Assist in making data easily accessible and optimized for organizational use. In addition, the data engineer will assist Product and Data Owners to refine their data usage and provide organizational implementation strategy, training, and metrics.




Job Requirements




What you need Degree in engineering, computer science, or related technical data intensive discipline plus 10-15 years of directly related experience. Additional relevant experience will be considered in lieu of a degree Practical experience as a data engineer or working engineering data intensive analysis of systems who understands the needs between software engineers, data scientist, and other data workers Experience with accessing and structuring data (SQL, Python) and pr duction database development in a Business Unit or larger scope (Relational SQL, Non-Relational DBs) Experiencing creating fully automated end to end Data Pipelines and tools involved (e.g., Airflow, NiFi) Knowledge of Cloud technologies - experience with hybrid cloud data provisioning a plus Knowledge of and experience in the MDA Systems engineering lifecycle processes Software development experience across the development lifecycle Desire to have experience documenting and standardizing processes into commonly accessible digital methods Excellent problem-solving skills Ability to work independently and as part of a group Ability to effectively participate and collaborate with functionally and geographically diverse teams of engineering professionals with differing perspectives, skills, and backgrounds Ability to effectively communicate complex ideas to non-technical stakeholders and non-agile data trained people, both in written and spoken forms Ability to provide significant contribution in the areas of creative thinking, problem solving, analytical thinking, strategic thinking and out-of-the-box thinking Energetic, motivated, self-starting What would be helpful Understanding of the MDS system implementation. Demonstrated ability to analyze complex data sets utilizing software tools. Experience in test analysis of missile defense in the areas of Link-16, Aegis, THAAD, GMD, C2BMC, AN/TPY-2, Overhead Sensors, and/or Patriot desired Experience in evaluating system performance, enhancing system operations and data processing
Show more "
3576140175,Software Engineer I - Growth,Gopuff,2023-04-22,https://www.linkedin.com/jobs/view/software-engineer-i-growth-at-gopuff-3576140175?refId=LCWxKJC96YqpMHZYedlUhg%3D%3D&trackingId=acapsWQJBLITnuxn0Fnjpg%3D%3D&position=24&pageNum=6&trk=public_jobs_jserp-result_search-card,3792,"Gopuff’s engineering team is building solutions to dramatically change how people purchase their daily goods. We provide the modern-day solution to meet customers’ immediate everyday needs with products ranging from snacks and ice cream to household goods and beer, at the click of a button.




We are seeking a Software Engineer who is excited about creating innovative solutions to make life effortless for our customers! The kind of people we are looking for want to build optimized routing systems to deliver to our customers efficiently, create an end-to-end shopping experience that will delight our customers, devise warehouse management systems that enable us to always fulfil our customers’ needs, or design mobile and web applications that are joyful to use. In short, we are looking for people eager to help create the future of Gopuff!




The right candidate will be able to write code in a team environment where good work will improve our customers’ lives by giving them precious time back in their day. The balance between delivering on commitments and collaboration among peers will be crucial to success in this role. We are biased towards action and your actions will matter. We are looking for candidates who are passionate about delivering consistently great experiences within our growing engineering team.




Responsibilities:




Promote and support Gopuff Engineering’s culture of inclusion and diversity
Participate in cross-functional projects in an agile environment
Build, deploy, and maintain your own code
Support standard development practices including idiomatic syntax, design patterns, and Test driven development
Implement and Monitor analytics to ensure the correctness of the business process




Minimum Qualifications:




1-2 years of industry experience beyond internship
Bachelor's degree in Computer Science (or related field)
Understand modern web, frontend and server, and/or cloud applications
Familiarity with data structures, algorithms and databases (ex: SQL, CosmosDB)




Preferred Qualifications:




Experience using Java in a production setting
Understanding of message-based, async processing
Experience with a variety of web services (ex: REST and HTTP cache-semantics)
Experience with SPA technology in a production setting (ex: Vue, React)
Public cloud experience in a production setting (Azure preferred)




Benefits




We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*




We also offer*:




Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.




At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.




And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.




Like what you’re hearing? Then join us on Team Blue.




Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show more "
3567587480,Data Engineer,GalaxE.Solutions,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-at-galaxe-solutions-3567587480?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=yuNOhOOMZ7Z9ympK8ZiVLA%3D%3D&position=1&pageNum=7&trk=public_jobs_jserp-result_search-card,4375,"Work with business and technical leadership to understand requirements.
Design to the requirements and document the designs.
Ability to write product-grade performant code for data extraction, transformations and loading using Spark, PySpark
Do data modeling as needed for the requirements.
Write performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive
Implementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS or Hadoop Edge Nodes
Implement Hadoop job orchestration using Shell scripting, Apache Oozie, CA7 Enterprise Scheduler and Airflow
Troubleshooting the issues, providing effective solutions and jobs monitoring in the production environment
Participate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.
Strong development experience in Spark, Py-Spark, Shell scripting, Teradata, Hive and Hadoop
Experience of Ab Initio is a bonus.
Strong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures
Excellent work experience on Hadoop as data warehouse/Data Lake implementations
Experience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)
Unix/Linux Shell scripting (KSH) and basic administration of Unix servers
CA7 Enterprise Scheduler
Experience with AWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)
Databricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration)
Experience in Jira and Confluence
Exercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.
Health care domain knowledge is a plus.




GalaxE is a professional IT services firm that specializes in platform-driven solutions and the use of automation to achieve enterprise business transformation and mission-critical change for some of the largest companies in the world. Using our proprietary solution set, GxFource®, we apply machine learning techniques and predictive analytics tools as part of a broad artificial intelligence strategy that provides effective impact and data-driven business transformation.




Since its founding, GalaxE has been dedicated to advancing the benefits of technology. As we continue that legacy and look to the future, a focus on business enablement through agile, cost-efficient, and effective integration of people, process, and technology anchors our success. We revolutionize change in the costs of doing business that transform companies and their ability to leap beyond the competition.




At GalaxE we value people and are committed to diversity and inclusion where our employees are made to feel comfortable and are encouraged to be authentic. We focus on cultivating both traditional IT and non-traditional, new collar, workers through our Outsource to America®, program.




We are always looking for passionate, entrepreneurial-minded innovators and disrupters; game-changers that take ownership of the work they produce and bring it each and every day. Working with like-minded team members you will get a chance to discover, develop, and use cutting-edge technologies to transform the way we deliver creative business solutions.




Sound like you? Join us and find out for yourself what it means for you, and your career, to be part of the GalaxE team. Let’s build something, together. #WeAreGalaxE




Equal Opportunity Employer/Veterans/Disabled




Physical Requirements




Prolonged periods of remaining stationary at a desk and working on a computer
Must be able to lift to 15 lbs., as needed
Must be able to work on-site (corporate/client offices), as needed (not applicable for 100% remote roles)
Occasionally required to bend, kneel, crouch, and reach overhead.
Hand-eye coordination necessary to operate computers and various pieces of office equipment.
Specific vision abilities required include close vision, the ability to tolerate fluorescent lighting, and the ability to adjust focus.




Employees must be able to perform the physical requirements of the position satisfactorily and, if requested, reasonable accommodations will be made to enable employees requiring accommodations to perform the essential functions of their jobs, absent undue hardship.




For more information, please visit https//www.galaxe.com/
Show more "
3538313665,GCP Data Engineer,ClearChoice Dental Implant Centers,2023-04-03,https://www.linkedin.com/jobs/view/gcp-data-engineer-at-clearchoice-dental-implant-centers-3538313665?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=aiSs%2ByJgUv8lCBNewZcllQ%3D%3D&position=2&pageNum=7&trk=public_jobs_jserp-result_search-card,4395,"Company Overview




ClearChoice was founded in 2005 to bring an innovative and patient-focused approach to solve gaps within the dental industry. We’ve experienced strong growth and today, we’re the leader in dental implant treatments. Driven by a collective desire to improve the lives of our prospective patients, we help them reclaim their health and confidence. Beyond restoring teeth, this is about getting their lives back.




This mission-focused work has enabled us to achieve four straight years of double-digit company growth, yet we’ve only reached 1% of the population that needs our services. We are searching for individuals who can help us continue pursuing our goal of reaching prospective patients and transforming their lives. When you join ClearChoice, you are joining a team of individuals with passion, conviction, and integrity whose mission is to be the platform of hope for our patients. Come help us write the next chapter of our story!




Data Engineer Summary




We are seeking a Data Engineer who will partner with business, analytics, and engineering teams to build data structures to facilitate reporting, models, and monitoring key performance metrics. Collaborating across disciplines, you will identify internal/external data sources to design data assets, define a data pipeline strategy & automated testing and implement scalable data solutions. We expect you to be aware of best practices and open to working with experienced mentors.




Now is a great time to join us to design and build the future of the ADMI Data Platform. Be a part of a new team building cloud-native solutions with open-source tools and technologies. You will partner with the business to build self-service data assets and modern data science/ analytics solutions. Work with big data and modernize our technology standards to benefit millions of our patients and thousands of employees.




Responsibilities




Work with IT partners to educate and advance adoption of modern data engineering techniques.
Contribute to an evolving cloud-hosted data platform using modern data engineer techniques.
Partner cross-functionally to understand data, reporting, and data asset requirements
Work with engineering teams to collect required data from internal and external systems
Build ETL strategy to build performant data solutions that are reliable and scalable in a fast-growing cloud-native data ecosystem
Rebuild and automate legacy reporting pipelines to a new platform
Contribute toward evolving company’s analytical self-service/ ad hoc reporting/Data Analytics/Analytical Modeling/Data Visualization process and/or product
Develop and conduct automated testing
Author, schedule, and monitor workflows using orchestration tools
Document and publish Metadata and table designs to facilitate data adoption.
Perform pipeline tuning as necessary




Qualifications/Requirements




Bachelor's Degree or equivalent combination of education, training, and experience
1+ yrs. of IT experience
Strong SQL writing skills.
Experience with SQLServer and BigQuery
Hands-on experience with Python and/or other programming languages
Experience with GCP, the various data-related resource types, how to organize, secure, and deploy them.
Experience interacting with REST APIs
Experienced with GIT, CI/CD, and the overall SDLC process.
Experienced with best practices in automated testing and automated pipelines
Familiarity building out and maintaining metrics, alarms, and monitoring of services




Desired Characteristics




Strong desire to work with analytics data and partners
Experience in MS SQL, SQL Server Integration Services (SSIS), and would be a plus
GCP certifications (Solution Architect, Engineering, Big Data Specialty, etc.) would be a plus
Experience/Exposure in Data Science and/or Analytics in cloud would be a plus
Experience in Machine Learning/Deep Learning use cases and development would be a plus
Experience in Data Cleansing/Transformation would be a plus
Experience in Container, Kubernetes, and related technology ecosystems would be a plus
Experience in Data Modeling would be a plus
Experience in Event-Driven Architecture, Streaming processing would be plus
Experience in sourcing and processing structured, semi-structured, and unstructured data would be a plus
Outstanding written and verbal communication skills




Salary Range: 80,000 - 100,000
Show more "
3580138094,Data Engineer,"Double Line, Inc.",2023-03-30,https://www.linkedin.com/jobs/view/data-engineer-at-double-line-inc-3580138094?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=cFvRtOCgEkR5ZdgOLpq8Fw%3D%3D&position=3&pageNum=7&trk=public_jobs_jserp-result_search-card,2970,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)




Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane , not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?




If so, we're looking for a motivated and driven person like you who has:




Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time




Bonus points if you're bringing knowledge of or really want to learn the following:




Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools




In Return, We Offer




A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture




We Do Not Want You To Make The Leap Without Knowing What We Need, So Here Is How We Define Success For This Position




Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days




We need to know - can you make this happen? If so, we definitely need to talk to you.




Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.




Double Line does not currently offer relocation assistance.




Powered by JazzHR
Show more "
3536674451,Data Engineer - Data Analytics,Costco Wholesale,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-data-analytics-at-costco-wholesale-3536674451?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=LoeqKL3SSJRirFF6nutMGA%3D%3D&position=4&pageNum=7&trk=public_jobs_jserp-result_search-card,7177,"This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.




Costco IT is responsible for the technical future of Costco Wholesale , the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes “World’s Best Employers” .




Responsible for developing and operationalizing data pipelines to make data available for consumption (reports and advanced analytics). This includes data ingestion, data transformation, data validation / quality, data pipeline optimization, orchestration; and engaging with DevOps Engineer during CI / CD. The role requires a grounding in programming and SQL, followed by expertise in data storage, modeling, cloud, data warehousing, and data lakes. The Data Engineer works closely with Data Architects, Data Scientists and BI Engineers to design and maintain scalable data models and pipelines.




The Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test and automate data pipelines that are relied upon across the company as the single source of truth.




If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.




ROLE




Supports development of Data Dictionaries and Data Taxonomy for product solutions.
Demonstrates strong understanding with coding and programming concepts to build data pipelines (e.g. data transformation, data quality, data integration, etc.).
Builds data models with Data Architect and develops data pipelines to store data in defined data models and structures.
Conducts ad-hoc data retrieval for business reports and dashboards.
Assesses the integrity of data from multiple sources.
Manages database configuration including installing and upgrading software and maintaining relevant documentation.
Monitors database activity and resource usage.
Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).
Works in tandem with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality and orchestration.
Designs, develops and implements ETL/ELT processes using IICS (Informatica cloud).
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency and quality of data management.
Communicates technical concepts to non-technical audiences both in written and verbal form.
Performs peer reviews for other data engineer’s work.




Required




5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
5+ years’ hands on experience with Informatica PowerCenter.
2+ years’ hands on experience with Informatica IICS.
3+ years’ experience working with Cloud technologies such as ADLS, Azure Data Factory, Azure Databricks, Databricks Live Table, Spark, Azure Synapse, Cosmos DB and other big data technologies.
Extensive experience working with various data sources (SQL, Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills required. Solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
5+ years’ experience with Data Modeling, ETL, and Data Warehousing.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Scheduling flexibility to meet the needs of the business including weekends, holidays, and 24/7 on call responsibilities on a rotational basis.
Able to work in a fast-paced agile development environment.




Recommended




BA/BS in Computer Science, Engineering, or equivalent software/services experience.
Azure Certifications.
Experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
Experience with Git/Azure DevOps.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Experience working with SAP integration tools including BODS.
Experience with UC4 Job Scheduler.
Excellent verbal and written communication skills.




Required Documents




Cover Letter
Resume




California applicants, please click here to review the Costco Applicant Privacy Notice.




Apart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.




Pay Range




$155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible




We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.




Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com




If hired, you will be required to provide proof of authorization to work in the United States
Show more "
3568541417,Data Engineer,"Torch Technologies, Inc.",2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-torch-technologies-inc-3568541417?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=0Wc2rw1P6OO%2BuoyPZH5CxQ%3D%3D&position=5&pageNum=7&trk=public_jobs_jserp-result_search-card,5506,"Thank you for your interest in employment with Torch Technologies. We are a 100% employee-owned, Certified Great Place To Work headquartered in Huntsville, AL with over 1200 employee-owners. Our team provides superior research, development, and engineering services to the Federal Government and Department of Defense. As one of the nation\'s top 100 defense companies, the services we provide directly support the men and women who serve our country. Our corporate mission sums up the pride our employee-owners take in the work we do: \""Lighting the Pathway of Freedom\"". And, as a Certified Evergreen ESOP, we have made the commitment to grow and sustain our company for the next 100 years! Come grow with us!




Job Title Data Engineer




Location Colorado Springs, CO 80840 US (Primary)




Job Description




Salary Range: \$121,600 - \$168,040 Please note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, internal equity, as well as candidate qualifications such as skills, education, and experience. Top Reasons to work with us Our team is sought after by leadership and peers to work their most ambiguous ideas and challenges due to our high degree of out of the box thinking, willingness to try something new, and no fear of failure. Our success is driven by every member of the team government and contractors alike striving to do the best they can. We work across the entire MDA Engineering Enterprise working to strengthen and enable every element and component. What you will be doing This position provides data engineering support to enhance the Missile Defense Agency (MDA) Engineering Enterprise program office looking to employ modern digital engineering and data science approaches across the lifecycle and connecting engineering, test, business, contract, and finance data of different types together. The Data Engineer works with MDA engineers to capture and understand both authoritative data and non-authoritative data, its meaning, and relation to other data and how to meaningfully use it to develop data stories that can be automated. This position will identify potential improvements to data sources themselves, collection methods, processes and supporting systems. The position participates in the development of an enterprise-wide system-of-systems engineering effort between the element weapon system program offices and the MDS system offices ensuring that the program offices are integrated and aligned working with weapon system prime contractors to deliver capability. Analysis of engineering and program office activities to help identify ways to apply digital transformation practices. Facilitating adoption of data engineering processes across MDA Engineering Enterprise in the Digital Engineering and Software Development DevSecOps functional areas. Responsible for building data pipelines to pull together data from different source systems. Works closely with data managers, systems engineers, and IT Services to understand architectural path between data sources and sinks across the organization. Assist in making data easily accessible and optimized for organizational use. In addition, the data engineer will assist Product and Data Owners to refine their data usage and provide organizational implementation strategy, training, and metrics.




Job Requirements




What you need Degree in engineering, computer science, or related technical data intensive discipline plus 10-15 years of directly related experience. Additional relevant experience will be considered in lieu of a degree Practical experience as a data engineer or working engineering data intensive analysis of systems who understands the needs between software engineers, data scientist, and other data workers Experience with accessing and structuring data (SQL, Python) and pr duction database development in a Business Unit or larger scope (Relational SQL, Non-Relational DBs) Experiencing creating fully automated end to end Data Pipelines and tools involved (e.g., Airflow, NiFi) Knowledge of Cloud technologies - experience with hybrid cloud data provisioning a plus Knowledge of and experience in the MDA Systems engineering lifecycle processes Software development experience across the development lifecycle Desire to have experience documenting and standardizing processes into commonly accessible digital methods Excellent problem-solving skills Ability to work independently and as part of a group Ability to effectively participate and collaborate with functionally and geographically diverse teams of engineering professionals with differing perspectives, skills, and backgrounds Ability to effectively communicate complex ideas to non-technical stakeholders and non-agile data trained people, both in written and spoken forms Ability to provide significant contribution in the areas of creative thinking, problem solving, analytical thinking, strategic thinking and out-of-the-box thinking Energetic, motivated, self-starting What would be helpful Understanding of the MDS system implementation. Demonstrated ability to analyze complex data sets utilizing software tools. Experience in test analysis of missile defense in the areas of Link-16, Aegis, THAAD, GMD, C2BMC, AN/TPY-2, Overhead Sensors, and/or Patriot desired Experience in evaluating system performance, enhancing system operations and data processing
Show more "
3570628332,Data Engineer,Compunnel Inc.,2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3570628332?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=nly69qNzUPGHtQDLJxy4gg%3D%3D&position=6&pageNum=7&trk=public_jobs_jserp-result_search-card,2557,"Description




Requirements




Design, Develop and maintain optimal data pipeline to assemble large & complex data sets that meets analytics and business requirements for various Client Lines of Business




Identify, design, and implement engineering process improvements through automating manual processes, optimizing data pipelines, re-designing services for greater scalability.




Build the frameworks required for optimal extraction, transformation, and loading of data from a wide variety of data sources using GCP technologies.




Build analytics patterns and tools that utilize the data pipeline to provide actionable insights and integrate insights with various consumer touchpoints.




Create model features, data tools for data science team members that assist them in model building and deployment




Work with analytic teams to support their data and technology needs.




Operationalize and automate the data pipelines on GCP cloud environments.




Collaborate with Lead Engineer and Architect in team to build Data Pipelines.




Required Qualifications




Hands on experience in designing and building data engineering solutions in cloud environments (preferably GCP).




Strong hands-on experience with languages like Python, PySpark, SQL, UNIX/Linux scripting to access, extract, manipulate and summarize data.




Strong understanding of query optimization, data structures, transformation, metadata, dependency, and workload management




Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.




Experience with Jenkins, Git, CI/CD pipeline, and other DevOps principles/best practices




Experience working in multi-developer environment, using version control (i.e. Git).




Preferred Qualifications




Certified Cloud Engineer in Data / Analytic engineering track: GCP / AWS/Azure preferred




Good articulation in communicating complex technical subjects to non-technical audience




Working experience with automation and orchestration tools




Knowledge of storage, networking, and IAM services on the cloud (GCP preferred)




Experience with modern API and microservice patterns




Knowledge and working experience in agile frameworks like Scrum. Kanban etc.




Familiarity in Healthcare / Insurance industry




Education




Bachelor’s Degree in Computer Sciences, Data Analytics, Informatics, Information Systems, or related quantitative field




Education: Bachelors Degree
Show more "
3580060956,Data Engineer (remote),Renuity,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-remote-at-renuity-3580060956?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=XmXMOyOn0MsoTbvlYTSs4A%3D%3D&position=7&pageNum=7&trk=public_jobs_jserp-result_search-card,3541,"Renuity is seeking a Data Engineer to design and implement data solutions using industry best practices in Azure.




Working at Renuity




“It’s a lifestyle”, our employees say. It means working in an environment of collaboration, respect, diversity, shared values and a passion for growth. That’s why Titan Holdings, our Parent Company, was recently featured in the Financial Times 2021 list of Americas’ Fastest Growing Companies. Whether you work in the field, a distribution center, a division location, or at the Coral Gables, Florida “Home Base” location, you work with team members who embrace a collaborative spirit to propel all Renuity Divisions to achieve faster growth, greater profitability, and become THE most trusted name in home improvement.




We, at Renuity strive to instill and maintain these core values, by being




Collaborative – We get farther, together. We pride ourselves on having the most talented people in our industry, and we expand what is possible through cohesive teamwork




Innovative – We challenge industry norms and take intelligent risks to discover better ways to serve our customers




Principled – We do the right thing – no matter what. We go to great lengths to ensure our customers, employees and partners have a world-class experience and are treated fairly




Enthusiastic – We love what we do and the bonds we create with the people around us. Our passion positively influences our customers, colleagues, and partners




Value-Driven – We have an unrelenting focus on creating value for our stakeholders. We reward performance that increases the value of our company, and we live a culture where everyone thinks and acts like an owner




We hope this information is helpful to you in making the right decision when choosing your next employer!




What We Offer




Competitive base salary between $120,000 - $150,000 per year commensurate on experience
Full benefits package including health, vision, dental and 401k match
Paid time off and holidays 
Work with a team of talented, professional, and fun individuals who enjoy what they do




What You'll Do




Design and implement data solutions using industry best practices in Azure
Understanding of Data warehouse and Big Data Concepts
Perform ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards
Create scripts and programs to automate data operations
Automate tasks and deploy production standard code (with unit testing, continuous integration, versioning etc.)
Write complex T-SQLs, Functions and Stored Procedures
Monitor and maintain data pipelines proactively to ensure high service availability
Troubleshoot production data pipelines and provide critical fixes to keep production processes running efficiently
Performance tuning Data pipelines, SQL Scripts, Store Procedures to optimize cost and execution times




What You'll Bring




5+ years’ experience in building data pipelines in Azure Synapse
5+ years’ experience in writing complex T-SQL, Stored Procs in MS SQL Server
8+ years’ experience in Building Data Warehouse and Big Data solutions
Experience in supporting MDM solutions
Experience in CI/CD processes is an added advantage




Renuity, LLC provides the following inclusive hiring information We are an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, or disability status.
Show more "
3576140175,Software Engineer I - Growth,Gopuff,2023-04-22,https://www.linkedin.com/jobs/view/software-engineer-i-growth-at-gopuff-3576140175?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=m8zYtyHzTJ5TCAhmRNkxYQ%3D%3D&position=8&pageNum=7&trk=public_jobs_jserp-result_search-card,3792,"Gopuff’s engineering team is building solutions to dramatically change how people purchase their daily goods. We provide the modern-day solution to meet customers’ immediate everyday needs with products ranging from snacks and ice cream to household goods and beer, at the click of a button.




We are seeking a Software Engineer who is excited about creating innovative solutions to make life effortless for our customers! The kind of people we are looking for want to build optimized routing systems to deliver to our customers efficiently, create an end-to-end shopping experience that will delight our customers, devise warehouse management systems that enable us to always fulfil our customers’ needs, or design mobile and web applications that are joyful to use. In short, we are looking for people eager to help create the future of Gopuff!




The right candidate will be able to write code in a team environment where good work will improve our customers’ lives by giving them precious time back in their day. The balance between delivering on commitments and collaboration among peers will be crucial to success in this role. We are biased towards action and your actions will matter. We are looking for candidates who are passionate about delivering consistently great experiences within our growing engineering team.




Responsibilities:




Promote and support Gopuff Engineering’s culture of inclusion and diversity
Participate in cross-functional projects in an agile environment
Build, deploy, and maintain your own code
Support standard development practices including idiomatic syntax, design patterns, and Test driven development
Implement and Monitor analytics to ensure the correctness of the business process




Minimum Qualifications:




1-2 years of industry experience beyond internship
Bachelor's degree in Computer Science (or related field)
Understand modern web, frontend and server, and/or cloud applications
Familiarity with data structures, algorithms and databases (ex: SQL, CosmosDB)




Preferred Qualifications:




Experience using Java in a production setting
Understanding of message-based, async processing
Experience with a variety of web services (ex: REST and HTTP cache-semantics)
Experience with SPA technology in a production setting (ex: Vue, React)
Public cloud experience in a production setting (Azure preferred)




Benefits




We want to help our employees stay safe and healthy! We offer comprehensive medical, dental, and vision insurance, optional FSAs and HSA plans, 401k, commuter benefits, supplemental employee, spouse and child life insurance to all eligible employees.*




We also offer*:




Gopuff employee discount
Career growth opportunities
Internal rewards programs
Annual performance appraisal and bonus
Equity program
Not applicable for contractors or temporary employees.




At Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get it—stuff happens. But that’s where we come in, delivering all your wants and needs in just minutes.




And now, we’re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.




Like what you’re hearing? Then join us on Team Blue.




Gopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.
Show more "
3563815263,Data Engineer,Serif Health,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-at-serif-health-3563815263?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=jdF6QbTVoGnicAC7viMXKQ%3D%3D&position=9&pageNum=7&trk=public_jobs_jserp-result_search-card,3042,"About Serif Health

At Serif Health, we are taking the complexity out of the cost of healthcare. Our mission is to make healthcare pricing and contracting more transparent, easier to understand, fairer, and ultimately more affordable for all. New price transparency regulations have made a wealth of healthcare pricing and reimbursement data publicly available for the first time, but it is still extremely hard to ingest, organize, and actually use this data to understand the cost of care. That’s where we come in. Serif Health is building healthcare price intelligence data products and APIs to power the future of more transparent healthcare.

We are an early-stage, venture-backed startup with a small, collaborative team of healthcare experts, engineers, and designers. We are excited to add new team members motivated by our mission, passionate about making a difference in healthcare, and looking forward to learning and growing from the challenges we will face along the way!

What You’ll Do: - Help build and sustain our data ingest, inventory, aggregation, and delivery pipelines - Design, run, and test novel analyses and presentations for healthcare price transparency data - Develop and maintain novel fill rate and data quality measurements for healthcare pricing data - Be responsible for estimating feasibility, cost, and timeframe for delivering custom analyses requested by our most important customers and partners - Identify new opportunities for automation and areas where software can optimize otherwise manual processes - Collaboratively define the data architecture and delivery processes for our company, and ensure our customer pipeline can scale and grow profitably relative to the size of our team. - Keep abreast of industry developments and best practices in data operations engineering

What we expect: - At least 3 years of commercial development experience in a big data or at-scale systems engineering role - Familiarity with Amazon AWS serving infrastructure and data pipeline tools (EC2 / EKS / Lambda / S3 / Cloudfront / Glue / Athena) - Strong knowledge of NodeJS and/or GoLang, specifically with streams, pipelines, and emitter patterns - High attention to detail, strong written and verbal communication skills, and professionalism - An ability to work independently, move quickly, and self-manage for high impact in a small team environment. - Passion for increasing affordability & transparency in the US healthcare market

What we'd love to see (Bonus): - Prior experience in a startup environment - Experience working with any of the following data: electronic medical records, medical claims and explanation of -benefits, healthcare billing / coding / pricing - Familiarity with Golang

Job Location: Anywhere in the U.S. - Serif Health is a remote-first company with team members across California, Ohio, Washington, and New York. We hold company gatherings at least quarterly in rotating locations across the U.S. for team members to get to know each other and collaborate in-person.
Show more "
3576634127,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576634127?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=OQl14uxc88LXcLmcuK5PSA%3D%3D&position=10&pageNum=7&trk=public_jobs_jserp-result_search-card,3143,"Job Description




Title: Data Engineer




Project Location : Louisville KY




Type: FullTime




Job Description




Part of a diverse engineering team consisting of electrical engineers, bio-engineers, AI engineers, mechanical engineers where you will participate in ideation sessions to provide inputs into new projects and applications
Developing machine learning and Artificial Intelligence solutions to products across the business
Research Emerging and Futuristic Technologies and current market solutions for the business needs and technical requirements
Collaborate with various advanced product development teams of providing solutions related to futuristic technologies like Artificial Intelligence, Augmented reality and IOT etc.
Perform market research, technical analysis and building proof of concepts and prototypes based on business needs and requirement
Keep current with technological developments through the attendance of trade shows, professional affiliations, online technical networks, university relationships and training
Build relationships and trust with various stakeholders of different product line leaders by understanding their needs for enabling innovation and transformation
Presentation of innovation concepts and ideas including general knowledge awareness and insights in appliance industry to different business leaders and stakeholders
Developing strategies and evaluation objectives for various advanced technologies




Key Qualifications




Practical knowledge and experience machine learning/deep learning, computer vision especially in the home appliances industry
Proficiency in training models with large scale data using modern machine learning frameworks like Pytorch and TensorFlow
Strong experience in building early stage products with Artificial Intelligence capabilities
Deep understanding of probability and statistics especially related to experimental analysis
Effective communication and strong technical skills in order to transform abstract business needs into technology solutions
Ability to independently learn and demonstrate new technologies by building proof of concepts and prototypes in a fast-paced environment
Exceptional analytical and critical thinking skills to analyze proposed solutions and effectively evaluate them
Excellent organizational skills with the ability to manage multiple priorities and meet established deadlines Skill requirements:
Proficient in using Python for various machine learning and AI applications
Basic skills in using CAD modelling software like CREO and SolidWorks




Desired Skills




App development in Android or JavaScript
Building and Developing Augmented Reality apps
Working with Arduino and Raspberry Pi




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3559643633,Data Engineer,Cocolevio LLC,2023-04-10,https://www.linkedin.com/jobs/view/data-engineer-at-cocolevio-llc-3559643633?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=%2B%2FO2ySKQhxqdriOcrvf5vA%3D%3D&position=11&pageNum=7&trk=public_jobs_jserp-result_search-card,1516,"Description




Who we are...




Cocolevio is a modern technology solutions company, and our vision is to enable a future where all businesses have the modern technology they need to compete in their market. Our mission is to provide platform-agnostic solutions to our clients, so they can leverage modern technologies—Cloud, AI, IoT, Big Data—to grow, increase profitability, and improve operational efficiency. Visit our website at cocolevio.com to learn more about what we do.




Who We're Looking For...




We are looking for a ""Data Scientist"" to join our team. This role will start off directly supporting one of our largest clients in the Finance/Mortgage industry (BFS), with the ability to work with other clients that Cocolevio supports. This is a hybrid role and the Data Scientist must be able to report On-Site to the client's office in the DMV area.




Requirements




5+ years of experience working in a PySpark / AWS EMR environment
Proven proficiency with multiple programming languages: Python, PySpark, and Java.
Experience in writing complex SQL Queries involving multiple tables and inner and outer joins.
Experience working within an Agile / Kanban environment and helped prioritize and scope feature requests.
Experience with AWS S3
Highly proficient with EMR, stepfunctions
Hands on experience with AWS GLue/Batch , ECS Fargate, Dynamo DB, Lambada functions, Redshift
Strong SQL experience
Knowledge of AWS cloud watch
Exposure to gitlab
Knowledge of IaC terraform (preferred)
Show more "
3570642241,Data Engineer,Compunnel Inc.,2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3570642241?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=MUw13Z2gXTlhqnQSbmo%2BiA%3D%3D&position=12&pageNum=7&trk=public_jobs_jserp-result_search-card,4000,"Description-




Top Skills - Must Haves:




SQL




Hadoop




Data bricks




Troubleshooting




Production Support




Cloud de




Cloud apps




Top Skills' Details




Strong demonstrated skill working with SQL programming




Strong experience in Hadoop and Spark and/or Data bricks




Experience in MS SQL Server OR Hana (as source data)




Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts




Demonstrated skills in data modeling, SQL Stored Procedures.




Experience with troubleshooting/ production support (please make sure candidates are aware this is an important responsibility of this position)




Job Description




Secondary Skills - Nice to Haves:




Our Data Engineering team is looking for innovative data and software engineers to build and evolve our industry leading data products and applications that empower our Data Driven Operating Model and next generation customer experiences.




What You’ll Do




Troubleshoot, support, and tune data products, applications and integrations on large scale data platforms (SQL server, HANA, Hadoop, Data bricks etc) with an emphasis on performance, reliability and scalability and most of all quality.




Analyze the business needs, profile large data sets and build custom data models and applications to drive business decision making and customers experience




Develop and extend design patterns, processes, standards, frameworks and reusable components for various data engineering functions/areas.




Collaborate with key stakeholders including business team, engineering leads, architects, BSA's & program managers.




Integrate data from MS-Dynamics 365 application and build reporting solution.




You will have the opportunity to extend your network and collaborate with engineers, architects, and leaders across data management, product engineering, and business leadership teams.




The Ideal Candidate Will Have




MS in Computer Science / related technical field with 7 to 10 years of strong hands-on experience in enterprise data warehousing / big data implementations & complex data solutions and frameworks




Strong demonstrated skill working with SQL programming




Strong experience in Hadoop/ Data bricks and Spark




Experience in MS SQL Server OR Hana (as source data)




Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts




Demonstrated skills in data modeling, SQL Stored Procedures.




SQL Server ETL Development experience using SSIS; strong ETL, data warehouse, T-SQL skills.




Extensive experience in implementing large scale data warehouse and data mart architecture and implementation.




Dynamics 365 integration and reporting solution enablement experience is a plus.




Experience on MSBI Stack SQL Server DBMS, SQL Server Analysis Services (SSAS) and SQL Server Integration Services (SSIS) is preferred.




Demonstrated ability to clearly form and communicate ideas to both technical and non-technical audiences.




Strong problem-solving skills with an ability to isolate, deconstruct and resolve complex data / engineering challenges




Results driven with attention to detail, strong sense of ownership, and a commitment to up-leveling the broader engineering team through mentoring, innovation and thought leadership




Demonstrated skill designing, developing, and supporting database applications.




Marketing, Sales, Bookings, and finance Domain expertise is a plus




Has agile scrum work experience for executing day to day activities and reporting to scrum team.




Good communication skills across distributed team environment




Must be self-motivated, responsive, professional and dedicated to customer success




Familiarity with streaming applications desired




Additional Skills & Qualifications




Must have experience diagnosing and trouble-shooting production issues




Must have excellent communication and follow-up skills




Education: Bachelors Degree
Show more "
3577127285,Analytics Engineer - Remote Work,BairesDev,2023-03-28,https://www.linkedin.com/jobs/view/analytics-engineer-remote-work-at-bairesdev-3577127285?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=as9STtAKC44r7egmlMTKNg%3D%3D&position=13&pageNum=7&trk=public_jobs_jserp-result_search-card,2681,"Who We are

BairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.

Analytics Engineer at BairesDev

We are looking for an Analytics Engineer to join our Data Team. You will be responsible for performing data transformation, modeling, and validation activities to support building data pipelines, models, and dashboards for C-Level and Managers while optimizing the performance of the queries and data models connected to a large dataset of an operation that has data as a core. This is an excellent opportunity for professionals looking to advance their careers at one of the industry's fastest-growing companies!

What You Will Do

Building data models using SparkSQL or PySpark.
In the Data Lake, transform and catalog new datasets to answer business questions.
Orchestrate and monitor data pipelines.
Collaborate with data product managers to build data products.
Build and maintain corporate data visualization dashboards.

Here’s What We Are Looking For

Bachelor’s degree in computer science, engineering, information systems, or closely related fields.
3 years of relevant experience as a data engineer, analytics engineer, or data analyst is required.
Strong SQL skills.
Advanced English level.

Desirable

Knowledge of data bricks.
Experience with data visualization and BI tools (ex: Tableau)
Knowledge of Apache Spark, Hive and Impala, Hadoop, or other big data technologies.
Previous DBT experience.
Solid understanding of Git and code version control.

How we do make your work (and your life) easier:

100% remote work.
Hardware setup for you to work from home.
Flexible hours - make your schedule.
Paid parental leave, vacation & holidays.
Diverse and multicultural work environment.
An innovative environment with the structure and resources of a leading multinational.
Excellent compensation — well above the market average.
Here you can grow at the speed of your learning curve.

Our people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.

Every BairesDev team member brings something unique to our company.

We want to hear your story. Apply now!
Show more "
3576568987,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576568987?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=A46K6Oqo%2F9SAJiBCPJcM7A%3D%3D&position=14&pageNum=7&trk=public_jobs_jserp-result_search-card,312,"Job Description




10&plus; years of Data Engineering experience.
Very good SQL skills,
Experience in Unix/Python Scripting , Spark
Good understanding of Data Warehousing & Bigdata Concepts.
ETL & Reporting Knowledge
Working Experience on AWS Cloud Platform/Services, Redshift, Glue, IICS Informatica
Show more "
3571624710,Data Engineer,eTeam,2023-04-19,https://ca.linkedin.com/jobs/view/data-engineer-at-eteam-3571624710?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=LdmCP%2B9yGnMb7fuzBi1TOQ%3D%3D&position=15&pageNum=7&trk=public_jobs_jserp-result_search-card,277,"Experience with relational SQL and NoSQL databases

Experience with data pipeline and workflow management tools.

Preferred Experience With AWS Cloud Services

Preferred experience with object-oriented and function scripting languages: Java, Scala, Python or similar
Show more "
3548475956,Data Engineer,"AEEC, LLC",2023-04-03,https://www.linkedin.com/jobs/view/data-engineer-at-aeec-llc-3548475956?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=3s3WmIJk9CGX4SGEYV6pCQ%3D%3D&position=16&pageNum=7&trk=public_jobs_jserp-result_search-card,4937,"AEEC is looking for a Data Engineer which will be a part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.

Responsibilities

Architects, designs, implements and maintains reliable and scalable data infrastructure.
Writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data.
Adheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation)
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI/CD pipeline
Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.

You Will Have

BS degree in computer science
Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)
Advanced SQL knowledge
Experience designing and implementing large-scale distributed systems
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Strong stakeholder management and ability to lead large organizations through influence
Continuous learning and improvement mindset
No prior experience in the energy industry required

Physical Demands : While performing duties of the job, incumbent is occasionally required to stand, walk, sit, use hands and fingers, handle or feel objects, tools, or controls, reach with hands and arms, talk and hear. Employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by job include close vision, distance vision, color vision, peripheral vision, depth perception and the ability to adjust and focus.

Work Environment : The noise level in the work environment is usually moderate.

About AEEC

AEEC is an award winning CMMI Level 3 and ISO accredited professional services organization with a proven track record of providing technology and engineering solutions to the commercial and federal market since 1995. AEEC provides leading edge of innovative technology solutions to solve customer’s complex business problems. We build long lasting business relationships based upon integrity, resourcefulness, ingenuity, and fully delivering commitments. AEEC possesses the fundamental IT and engineering skills needed to objectively evaluate problems and develop technically sound, cost effective solutions. AEEC has been featured in a Harvard Business School Case Study on the topic of innovation.

AEEC Values : Our customers and their missions come first. We want to add value to every engagement. We will demonstrate integrity and responsiveness in all of our business practices. Teamwork and respect for everyone.

AEEC Vision : To be the best partner to our customers and a great place to work and grow.

AEEC Mission : Our mission is our customers’ success. We strive to responsively solve our customers’ needs. Our current skills are IT and Engineering services.

Benefits : AEEC offers competitive wages with benefits (Medical, Dental and Vision Insurance, Life Insurance, Short term/Long term disability dismemberment, HSA, Flex Spending, 401k, and a 529 college savings plan). Medical, dental and vision benefits start the first of the month following start of full time employment.

AEEC is a Federal Contractor and agrees to comply with all provisions set forth in Equal Employment Opportunity, Executive Order 11246, as amended, Section 503 of the Rehabilitation Act of 1973, as amended, 38 U.S.C. 4212 of the Vietnam Veterans’ Readjustment Assistance Act of 1974, as amended, 29 CFR Part 471, Appendix A to Subpart A (Executive Order 13496), and Executive Order 11246. If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Kim Hartley at 703-766-4300

AEEC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

AEEC invites any applicant and/or employee to review the Company’s written Affirmative Action Plan. This plan is available for inspection upon request by contacting Kim Hartley at 703-766-4300
Show more "
3576650048,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576650048?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=93Ne0w9J9Qip1Zrz4L%2Batg%3D%3D&position=17&pageNum=7&trk=public_jobs_jserp-result_search-card,1082,"Job Description




Job Title: Data Engineer




Location: 100% Remote




Position type: Fulltime




Job Description




Design and Develop scalable Big Data Warehouse solutions using cloud technologies.




Create and review technical and user-focused documentation for data solutions (data models, data dictionaries, business glossaries, process and data flows, architecture diagrams, etc.).




Extend and enhance the business Data Lake




Collaborate with management, business partners, analysts, developers, architects, and engineers to support data quality efforts.




Work closely with the Data Science and Business team to improve actionable data




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3561986799,Analytics Engineer,Flywheel Software,2023-04-11,https://www.linkedin.com/jobs/view/analytics-engineer-at-flywheel-software-3561986799?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=ek6H4AupVJFl1vW2h3FB2Q%3D%3D&position=18&pageNum=7&trk=public_jobs_jserp-result_search-card,3383,"Flywheel

Flywheel Software is a high-growth technology startup offering a customer segmentation platform on Snowflake and BigQuery that is changing the way businesses acquire, retain, and win back their customers. We are a completely bootstrapped and profitable startup with clients like Indeed, Google, and Uber.

Our Mission 🧭

Unlock the value of data in the cloud data warehouse to make it drive massive, real-world impact.

How we work 💼

A collaborative team that strongly believes in taking the learner's mindset
We encourage exploration of new technologies
We believe in empowering every person on our team to do their best work
Dedication to building a product people love
We build slowly but surely for the long term. We are transparent about the challenges of building a great company. We are humble in facing those challenges. But, we know that if we keep improving every day the Flywheel turns.


About You

In this role, you will be working with clients to understand the nuances of their business and translate them into efficient and accessible data models. You're passionate about the organization and optimization of data architecture, you enjoy taking a complicated problem and breaking it down into manageable pieces, and you're not afraid to spend a bit more time building out tooling or processes in a repeatable way to make your life easier down the line.

If you are someone who is oriented around growth, excited about a fast-paced environment packed with opportunities this might be interesting to you.

Responsibilities

Engineer and optimize the ingestion of massive amounts of data into cloud data warehouses
Work directly with clients to understand what their data looks like, how they want to use it, and what's getting in their way
Design and build data models that support and enable the needs of the client
Iterate on our workflow to make it more efficient


Qualifications

2+ years experience in SQL
2+ years experience in Python
Hands-on experience with the design, development, and maintenance of cloud data warehouses
Strong knowledge of relational database and data warehousing fundamentals
Strong analytical and problem solving skills
Insatiable intellectual curiosity to learn new (cloud) technologies
Ability to self-manage


Nice to have

Experience with transformation orchestration tools such as DBT
Experience with Airflow
Experience with Machine Learning model training and productionization


We are a collaborative team that strongly believes in taking the learner's mindset to everything we do. This role will have the opportunity to learn how to apply machine learning / artificial intelligence models to some of the most important problems businesses face.

By joining our team, we hope you will change the trajectory of your professional career and that of our business.

Benefits

Meritocracy💰

Spot bonuses for major milestones
Grow into leadership roles as we scale this rocket-ship
Startup equity for star performers


Generous Time-off 🏝

Take as much vacation as you like!
Flexible remote work policies


Platinum Benefits 🧑🏻‍💻

Free Platinum Health Insurance with Aetna
401(k) Program with Generous Company Match
Flexible PTO and WFH Policies


Learn and Grow ✍️

Education Stipend towards your professional development
Work directly with Founders (ex-Googlers)
Learners' mindset culture of 'Friendly Geniuses'
Show more "
3580138491,Data Engineer,Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-ag-3580138491?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=yRchGcyl1zcqA01a8FB8gw%3D%3D&position=19&pageNum=7&trk=public_jobs_jserp-result_search-card,4893,"Hybrid | Crystal City, VA | 2-3 Days a week Onsite




Secret Clearance Required




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.




Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering




Requirements




Mid-level proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active Secret clearance is required.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3714




External Company URL: http://precision-solutions.com/
Show more "
3563649404,Data Engineer - Remote | WFH,Get It Recruit - Hospitality,2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-remote-wfh-at-get-it-recruit-hospitality-3563649404?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=zpqnrmNPVqRDj7d%2BLNjMog%3D%3D&position=20&pageNum=7&trk=public_jobs_jserp-result_search-card,1859,"Looking to join an exciting and dynamic team that is driven by data, not hunches? Our company is currently seeking a skilled and collaborative Data Engineer to join our remote team anywhere in the USA. At our company, we deliver next-generation insights to help drive business decisions in a variety of industries including healthcare, campus, corporate, sports, entertainment, hospitality, and retail.

As a member of our Data Engineering team, you'll have the opportunity to work with a talented and experienced team to build cloud infrastructure and pipelines that deliver data solutions to our clients in the Food, Beverage and Sports industries. Using a tech stack that includes DBT, Snowflake, Python, Spark, Docker, Airflow, and a range of AWS services like SQS, SNS, S3, Redshift, Glue, you'll have the opportunity to make code decisions and bring best practices to the team.

To be considered for this role, you'll need to have at least 2 years of hands-on Python backend or Data Engineering experience, and experience with DBT, SQL, data validation and testing. Additionally, experience with DevOps and CI/CD technologies and methodologies, Snowflake data platform, and AWS Cloud Infrastructure is valued.

We offer a competitive salary range of $85k-$100k plus company benefits, including medical, dental, vision, life insurance/ AD, disability insurance, retirement plan, paid time off, holiday time off, associate shopping program, health and wellness programs, discount marketplace, identity theft protection, pet insurance, commuter benefits, employee assistance program, and flexible spending accounts (FSAs).

If you're a team player who loves working with data and building solutions, apply today and join us at our company where we are the spark that ignites!

Employment Type: Full-Time

Salary: $ 132,000.00 207,000.00 Per Year
Show more "
3580138489,Data Engineer,Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-ag-3580138489?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=zECTr%2FfgX%2FLnooIk%2BYvV2w%3D%3D&position=21&pageNum=7&trk=public_jobs_jserp-result_search-card,4905,"Onsite | Transcom (Illinois) | 5 Days a week Onsite




Interim Secret Clearance Required




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.




Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools Next
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering




Requirements




Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an Interim Secret clearance is required.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3712




External Company URL: http://precision-solutions.com/
Show more "
3576567622,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576567622?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=BTSn0rM1W3BupRIg%2FicZmg%3D%3D&position=22&pageNum=7&trk=public_jobs_jserp-result_search-card,1633,"Job Description




The job description




Data Engineer

Irvine CA / Remote

Full time

JOB Description

Solid working experience with SQ L to include complex queries and joins and automation of manual or non-integrated workflows
Solid analytical and reasoning skills for query design and optimization
Great communication skills with technical and non-technical teams
Experience in popular hyperscaler platforms GCP, AWS, Azure and underlying Data pipeline, Data Storage, Access and Governance services
Experience in software development in both SQL and NO SQL Databases
Databases: SQL DBs - Snowflake, Google Big Query, NoSql DBs - Couchbase, Cassandra, MongoDb, Graph DBs – Neo4J, OrientDB, In-memory – Elastic, VoltDB, Apache Solr, Redis, Snowflake / Postgres Databases
Experience in ETL, Data and Event Streaming, CDC technologies
Experience in popular Dev Ops tools
Experience in Data cleansing, transformation, validation and monitoring Data flows for data ingestion and staging using traditional ETL solutions like Airflow, SSIS, Talend, or similar Familiarity with data quality and MDM tools
Basic familiarity with visualizing data with Tableau, Business Objects, Quicksight , PowerBI and similar tools

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3573884257,Data Engineer,Navanti Group,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-at-navanti-group-3573884257?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=ElXh8ZKg%2BFEhmekx64JUWg%3D%3D&position=23&pageNum=7&trk=public_jobs_jserp-result_search-card,5561,"Project/Team: NATO




Location : Client Site (NATO HQ SACT) – Norfolk, VA




Employment Type: Full-Time




Number of Vacancies: 1




POSITION NOT YET FUNDED. SOLICITING RESUMES FROM INTERESTED CANDIDATES FOR ANTICIPATED POSITION.




Background




Navanti is seeking a Data Engineer to assist a prospective contract with the North Atlantic Treaty Organization (NATO). Navanti is a dynamic small business focused on providing subject matter expertise and data collection capabilities for a range of private and public clients; this position is part of NATO’s Capability Development Management Support (CDMS) and would be stationed in-person, full time in Norfolk, Virginia.




Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.




Tasks




Contribute to the development and implementation of an enabling data science and AI capability at HQ SACT and for the NATO.
Contribute to ML/AI initiatives across HQ SACT and the NATO Enterprise with a particular focus on the data engineering side.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, proposals on how to re-design infrastructure for greater scalability.
Develop, construct, test, and maintain data pipelines and architectures such as databases and large-scale processing systems, within the constraints of existing but evolving processes and technologies.
Transform data into formats that can be easily analyzed by developing, maintaining, and testing infrastructures for data generation.
Prepare data for prescriptive and predictive modelling.
Provide subject matter expertise to (military and civilian) staff within HQ SACT or the NATO Enterprise and develop proofs of concept, as directed.
Work in tandem with data scientists and software engineers.
Select from existing data sources and prepare data to be used by data science models.
Improve data quality and efficiency.
Support evaluation of operational requirements and objectives.
Interpret trends and patterns and support building of algorithms and prototypes.
Support educational efforts and training development related to data, AI or digital literacy.
Remain up-to-date with new developments in data engineering and data architectures to bring innovative ideas into implementation.
Support building a data-driven culture that uses data and analytics to generate insights, improve decision making at all levels, inform strategy and policy decisions, and improve performance.
Perform additional tasks as required by the COTR related to the LABOR category.




Qualifications




A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service.
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake.
Ability to work with large data sets is required.
A Master’s degree or higher from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Knowledge of NoSQL databases such as MongoDB, CosmoDB recommended but not mandatory.
Ability to work in cloud environments to develop scalable data pipelines highly recommended. Skills in Cloud infrastructure and technologies such as Google Cloud Compute, AWS, Azure Data Factory, distributed computing will be highly advantageous.
Working experience with geospatial data structures such as raster and vector-based data is recommended.
Ability to collect and document project requirements, and to translate the requirements to technical solutions, including working in an agile environment to implement complex database projects is highly desirable.
Working experience in an international environment with both military and civilian elements.
Understanding of the NATO organization and its functions.
A valid NATO SECRET-level security clearance or an active SECRET security clearance issued by a national authority.
Show more "
3555268545,Data Engineer,Homebase,2023-04-05,https://www.linkedin.com/jobs/view/data-engineer-at-homebase-3555268545?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=sCNc%2BGK4oestkwJQ110QHQ%3D%3D&position=24&pageNum=7&trk=public_jobs_jserp-result_search-card,4450,"About Us

Our mission is to make hourly work easier for local businesses and hourly workers. Homebase currently serves more than 100,000 small (but mighty) businesses with everything they need to manage their hourly teams: employee scheduling, time clocks, payroll, team communication, hiring, onboarding, and compliance. Just don't call us ""Human Capital Management."" We have built tools for the busiest businesses, so owners and employees can spend less time on bullsh*t and more time on what matters. The Homebase team brings small business expertise from Intuit, Square, OpenTable, Yelp, Gusto, and First Data. Homebase is backed by leading venture investors Bain Capital Ventures, Baseline Ventures, Cowboy Ventures, Khosla Ventures, Plus Capital, and GGV Capital.

Diversity, Equity, and Inclusion at Homebase

At Homebase, we take pride in fostering a welcoming space where every Homie of every gender, age, orientation, culture and walk of life can be their full selves. Diverse perspectives empower us to build the best-in-class platform for small businesses and hourly shift workers. We recognize that experience comes in many forms, so if you think you're close to what we're looking for (even if you don't meet 100% of the qualifications), we encourage you to apply!

We are currently hiring in Toronto, CA, and Houston, TX.

The Job

The Data Engineer will be responsible for developing and maintaining data pipelines, building data models, and creating reports and dashboards that support business decisions. The ideal candidate will have a strong background in Python and SQL, experience with tools like DBT, Looker, Databricks, and expertise in cloud analytics databases like Snowflake or Redshift.

Responsibilities

Develop, maintain, and optimize data pipelines that process large volumes of structured and unstructured data from various sources.
Build and maintain data models using dimensional data modeling techniques
Design and develop reports and dashboards that provide insights into business performance
Work closely with cross-functional teams to understand business requirements and translate them into technical specifications
Troubleshoot data issues and provide timely resolutions
Optimize query performance and implement best practices for database design and development
Collaborate with data scientists and analysts to support their data needs
Provide technical guidance and mentorship to junior members of the team
Continuously improve the quality of our data and analytics processes through automation and innovation
Show high ownership over projects and take the initiative to drive them to completion


Qualifications

3+ years of experience in data engineering or related field
Expertise in SQL and proficiency in data modeling and database design
Working knowledge of a scripting language like Python, Shell, or Ruby
Previous experience with data modeling, ETL/ELT development principles, and data warehousing concepts
Strong experience with data transformation tools like DBT or Databricks
Ability to break down complex business problems into small constituent units
Excellent problem-solving skills and attention to detail
Strong communication skills and ability to collaborate effectively with cross-functional teams
Ability to organize and work on multiple tasks at the same time
Ability to work independently as well as effectively within a team environment
Working knowledge of Git, Jira, and testing frameworks like great expectations.


What We Offer You

Stock Options - everyone is an owner
Comprehensive Insurance Plans
401(k) program +4% company match
Remote, hybrid, and in-office work options
Top-of-the-line equipment and home office $$
Annual Holidays and Accrued PTO
A dynamic, well-connected, productive team
Fun company activities


At Homebase, we value our differences, and we encourage all to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, national origin, citizenship, age, marital status, veteran status, disability status, or any other characteristic protected by law. Homebase is proud to be an equal opportunity employer and participant in the U.S. Federal E-Verify program. Accommodations will be provided during the hiring process if needed. Please advise us of any accommodations needed within your application to ensure fair and equitable access throughout the recruitment and selection process.
Show more "
3576714651,Data Engineer and Visualization Developer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/data-engineer-and-visualization-developer-at-diverse-lynx-3576714651?refId=w5lSssdv5DinlgR%2BvplB5A%3D%3D&trackingId=2%2FQNE3ddvROQpv03xcatsg%3D%3D&position=25&pageNum=7&trk=public_jobs_jserp-result_search-card,811,"Job Description




Job Details:




Title : Data Engineer and Visualization Developer




Client : Qualcomm




Location: San Diego,CA




Job Description




Strong Python development experience
Experience working on Splunk
Tableau development experience
Excellent SQL knowledge
Airflow and PySpark experience
Excellent communication skills
Ability to work in agile project environment




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3564686073,Junior Software Engineer,Patterned Learning AI,2023-04-10,https://www.linkedin.com/jobs/view/junior-software-engineer-at-patterned-learning-ai-3564686073?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=NZpank5G9kiqxE%2FymPywhQ%3D%3D&position=1&pageNum=8&trk=public_jobs_jserp-result_search-card,1723,"REMOTE (US/Canada Residing people only, with work permit)




Patterned Learning – Junior Software Engineer , FULL-TIME, Salary $100K - $130K a year.




About us: The Future of AI is Patterned, a stealth-mode technology startup. Top investors include Sequoia and Anderson Horowitz, founders from Google, DeepMind, and NASA and we’re hiring for almost everything!




Responsibilities




About the job requirements:




Collaborate on development of new data-driven application features and behaviors
Provide first level support for a high volume of support requests
Integrate with and maintain existing code base using industry and firm best practices, including source control (git)
Participate directly in the complete SDLC, working closely with our product, QA, and legal teams
Build smart internal tools to streamline platform development
Learn and work with independence, merge with an established dev team with decades of collective experience building best-in-class enterprise software




Minimum Requirements




Bachelor's degree in Computer Science or related field
1-2 years of experience in full-stack Software Engineering
C#/.NET with solid understanding of OO & SOA
Experience using RDBMS, like SQL Server or MySQL
Familiarity with abstract and data-driven models is a major plus
Extremely self- motivated




Special Benefits You Will Love




Flexible vacation, paid holidays, and paid sick days
401(k) with up to 2% employer match (no match)
Health, vision, and dental insurance
Optional supplemental insurance policies for things like accidents, injuries, hospitalizations, or cancer diagnosis and treatment.




Schedule: 8 hour shift, Monday to Friday , Time: Flexible, Job Type: Full-time
Show more "
3574150323,Data Engineer - Hadoop /Cloudera,Laksan Technologies,2023-04-06,https://www.linkedin.com/jobs/view/data-engineer-hadoop-cloudera-at-laksan-technologies-3574150323?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=hoZIz8EFidOnwLpUv0Xpug%3D%3D&position=2&pageNum=8&trk=public_jobs_jserp-result_search-card,591,"Role : Hadoop Data Engineer Years of Experience : 8+ yrs Strong experience needed in building end to end pipelines, SQL and Pyspark along with large scale migrations. Requirement :

Need Senior Hadoop developer with focus on cloudera Hadoop.
Should be able to demonstrate work experience in hive/Impala and Spark, pyspark.
Strong spark hands on skill a must. Very Strong SQL skills.
Working knowledge of Python, shell scripting and bash.
Must have work experience in Hadoop as data warehouse/data lake implementations.
Must be able to work from onsite santa clara ( no exceptions)
Show more "
3576702853,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576702853?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=eVgjSvHfqE286LNpze39jQ%3D%3D&position=3&pageNum=8&trk=public_jobs_jserp-result_search-card,2110,"Job Description




Job Title: Data Engineer




Location: Franklin, TN




Type: Contract




Professional Experience




5&plus; years of experience in data, consulting, analytics or a related function involving quantitative data analysis to solve problems




Python Programs, R scripting with exposure to Big data and data analysis




Technical expertise regarding data models, database design development, data mining and segmentation techniques




Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.




Design Data models, Dashboards and Reports, using different data sources.




Convert the business requirements to technical code using SQL




Enable advanced new technologies in the big data and analytics space that will lead to improved sales, marketing strategies




Responsibilities




Interact with Business users and understand their requirements from a data analytics perspective.




Work closely with data scientists to create advanced analytical & predictive analytical models




Configuring linked services, Dataset and integrating Pipelines




Data-oriented personality, great communication skills, and an excellent eye for details.




Acquire data from primary or secondary data sources and maintain databases/data systems.




Interpret data, analyze results using statistical techniques and provide ongoing reports.




Strong SQL skills in at least one DB technology




Must To Have Technical Skills / Education Background




Master in Business Analytics




R




Python




SQL




Experience in ETL




Good To Have Skills




Snowflake




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3575523402,Data Engineer,Kuona,2023-04-12,https://mx.linkedin.com/jobs/view/data-engineer-at-kuona-3575523402?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=yeTeE8uT8LN86OUMW%2FwuBA%3D%3D&position=4&pageNum=8&trk=public_jobs_jserp-result_search-card,4829,"Were Kuona (kuona.ai), an AI technology company that empowers consumer products and retailers to live up to their revenue goals and empowers our clients to maximize their sales and profitability through the dynamic optimization of prices, promotions, and inventories. Our AI-powered platform helps top brands like Coca-Cola, Heineken, and Nutrioli to maximize their profits and optimize inventories. We are looking for people that are world-class, curious, innovative, bright, and work to be better every single day!




We are currently looking for a solution-oriented Data Engineer to help us build and maintain complex data pipelines and analytical solutions that deepen our collective understanding of customers. You will encounter a wide range of problem-solving situations, strategic to real-time, requiring extensive use of data collection and analysis. As a Data Engineer, you will work with the Data Science team to drive and evolve our analytics solutions and data systems. You will greatly contribute to analytics decision-making, analysis, and data integration to enable Kuona to become a world-class data-driven organization. This position is fully remote.




Key Responsibilities:




Gain a deep understanding and become a subject matter expert for the current database and related functions and systems
Creation and maintenance of architecture to obtain optimal pipelines of information.
Assemble large and complex datasets that meet the functional requirements of the collection.
Identify, design, and implement internal processes to optimize scalability and information delivery as well as the automation of manual processes.
Construction of analytical tools that prevent errors in the data, thus ensuring a high quality of the information collected.
Assist in requirements gathering, design, and development of complex data systems that collect, analyze, and measure data throughout all business units
Build the infrastructure required for optimal extraction, transformation, and loading of data from various data sources using diverse technologies.





Requirements:




2+ years of experience working in data engineering, business intelligence, or engineering
Experience analyzing and integrating data using Python and SQL to extract and transform data according to business rules and requirements
1+ years of experience working with Pandas, Relational Databases and NoSQL, and AWS.
2+ years of experience working with large-scale data warehouses, web APIs, and database platforms to integrate internal and external data sources
Attention to detail and the ability to think critically and solve problems using analytical and quantitative methodologies
Analytical and Problem-Solving skills
Teamwork and Leadership skills
English spoken and written
Spanish spoken and written





Your experience with Kuona:




Creativity: We like that all team members can propose and create new features
Self-management: Since we are a very horizontal company, we require that team members can decide for themselves what to work on and define priorities
Opportunities to learn: We offer all our employees a wide opportunity to learn things you probably wouldn't be exposed to in a corporate environment.
High Impact: You will be involved in the growth and evolution of the company, and all your contributions will be of high impact on the overall results.
We value our culture: We are fully committed to prioritizing great results for our clients and an amazing employee experience for our people.
Ability to work anywhere / Flexibility: We provide everyone the opportunity to design your day and execute your projects with flexibility and focus on your well-being.





Benefits:




Competitive base compensation
Health insurance
Life insurance
Flexible time off and 12 holidays
Work from home policy including a laptop and support for your home office needs
Opportunity to join a diverse, passionate, and fun team





Our process:




This is our basic process, we may add or remove steps based on the information that we gather during the process.

Introductory screening call with someone from our recruiting team.
Interview loop with additional team members, with the following panel:
Technical interview for the role + Test Project
Industry knowledge and management experience
Culture fit and behavioral interview with our CEO

Reference Checks
Successful candidates will subsequently be made an offer.





Kuona provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more "
3576533528,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576533528?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=6gs9d%2BMj4L%2Ffr%2F9iQSDZdQ%3D%3D&position=5&pageNum=8&trk=public_jobs_jserp-result_search-card,828,"Job Description




Primary skills:




AWS Services (S3, Athena, Glue, Lambda, Step Functions, SQS, Redshift), SQL




5&plus; years of experience as Data engineer
Experience developing business applications using SQL databases.
Experience working with AWS Cloud
Should have good experience with AWS Services – S3, Athena, Glue, Lambda, Step Functions, SQS, Redshift.
Plus to have knowledge on Snowflake




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3570628332,Data Engineer,Compunnel Inc.,2023-04-14,https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3570628332?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=Q3deIxYE5fKm0CSZQDYmZQ%3D%3D&position=6&pageNum=8&trk=public_jobs_jserp-result_search-card,2557,"Description




Requirements




Design, Develop and maintain optimal data pipeline to assemble large & complex data sets that meets analytics and business requirements for various Client Lines of Business




Identify, design, and implement engineering process improvements through automating manual processes, optimizing data pipelines, re-designing services for greater scalability.




Build the frameworks required for optimal extraction, transformation, and loading of data from a wide variety of data sources using GCP technologies.




Build analytics patterns and tools that utilize the data pipeline to provide actionable insights and integrate insights with various consumer touchpoints.




Create model features, data tools for data science team members that assist them in model building and deployment




Work with analytic teams to support their data and technology needs.




Operationalize and automate the data pipelines on GCP cloud environments.




Collaborate with Lead Engineer and Architect in team to build Data Pipelines.




Required Qualifications




Hands on experience in designing and building data engineering solutions in cloud environments (preferably GCP).




Strong hands-on experience with languages like Python, PySpark, SQL, UNIX/Linux scripting to access, extract, manipulate and summarize data.




Strong understanding of query optimization, data structures, transformation, metadata, dependency, and workload management




Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.




Experience with Jenkins, Git, CI/CD pipeline, and other DevOps principles/best practices




Experience working in multi-developer environment, using version control (i.e. Git).




Preferred Qualifications




Certified Cloud Engineer in Data / Analytic engineering track: GCP / AWS/Azure preferred




Good articulation in communicating complex technical subjects to non-technical audience




Working experience with automation and orchestration tools




Knowledge of storage, networking, and IAM services on the cloud (GCP preferred)




Experience with modern API and microservice patterns




Knowledge and working experience in agile frameworks like Scrum. Kanban etc.




Familiarity in Healthcare / Insurance industry




Education




Bachelor’s Degree in Computer Sciences, Data Analytics, Informatics, Information Systems, or related quantitative field




Education: Bachelors Degree
Show more "
3576712470,Data Engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576712470?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=dXudYgnQxnQnL5W%2FSBpRJw%3D%3D&position=7&pageNum=8&trk=public_jobs_jserp-result_search-card,1012,"Job Description




Job Title: Data Engineer




Location: Walnut Creek, CA




Duration: Contract




Job Description




oExperience building and maintaining data pipelines and/or analytical or reporting systems at scale




oKnowledge of Data Science, Machine Learning and Statistical Models is desirable




oExperience in Python, Scala, or R for large scale data analysis




oExperience with Relational Database Systems and Graph Database




oExperience with Cloud computing (AWS)




oExperience with Spark




Experience in developing high volume transaction processing solutions




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3573113244,Data Engineer II,Rose International,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-ii-at-rose-international-3573113244?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=PDWZlbqNVkkoTrhAEGTAVA%3D%3D&position=8&pageNum=8&trk=public_jobs_jserp-result_search-card,4058,"Position TitlePosition NumberLocation




Position Title:




Data Engineer II




Position Number:




440548




Location:




USA, CA, Cupertino 95014 Position TypeSkills/Attributes




Position Type




Temporary




Skills/Attributes




Oracle, PL/SQL, Spark, SQL, Tableau Estimated Duration (In Weeks)




Estimated Duration (In Weeks)




20 Min Hourly Rate($) Max Hourly Rate($)




Min Hourly Rate($)




70.92




Max Hourly Rate($)




70.92




Apply Now




Description




**Only those lawfully authorized to work in the designated country associated with the position will be considered.**
**Please note that all Position start dates and duration are estimates and may be reduced or lengthened based upon a client’s business needs and requirements.**




Title: Data Engineer II




Location: Cupertino, CA OR Seattle, WA (Hybrid, need to be in office 2-3 times per week)




Start Date: 05/08/2023




End Date: 10/08/2023




Job Summary




As a contractor in support of the Competitive Benchmarking Team, you will work with researchers to support analysis from a variety of data sources with varying complexity, automate data processing and analysis that derives insights and impacts product decisions.




Process and validate quantitative data sets
Solve data storage and processing problems with data advanced engineering methods
Build and prototype dataset generation tools to automate new and recurring data sets
Work closely with researchers, data scientists and engineers to ensure timely delivery of analyses




Key Qualifications




2-5 years of relevant experience with a strong desire to learn our work quickly
Data engineering with strong automation, testing, and attention to detail
Experience with modern data architectures and database administration in AWS
Experience with Python and SQL for data engineering
Experience with code repositories and comfort with code reviews
Comfort owning databases, optimizing queries and supporting cross-functional DB users
Ability to document code and communicate DB changes and to cross functional partners
Strong analytical and problem-solving skills and tendency to be proactive in working across research, data, and engineering teams with a high attention to detail
Ability to work effectively in a team environment with exceptional interpersonal skills
Previous experience managing multiple projects of different complexity under tight deadlines
Data analysis and QA experience a plus




Education




An academic background in Computer Science, Information Systems, Mathematics, Physics, Engineering, or similar quantitative field. Prefer BS + 5 years of relevant industry experience or MS degree + 2 years of experience.




Benefits:

For information and details on employment benefits offered with this position, please visit here. Should you have any questions/concerns, please contact our HR Department via our secure website.

California Pay Equity:

For information and details on pay equity laws in California, please visit the State of California Department of Industrial Relations' website here.

Rose International is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender (expression or identity), national origin, arrest and conviction records, disability, veteran status or any other characteristic protected by law. Positions located in San Francisco and Los Angeles, California will be administered in accordance with their respective Fair Chance Ordinances.

If you need assistance in completing this application, or during any phase of the application, interview, hiring, or employment process, whether due to a disability or otherwise, please contact our HR Department.

Rose International has an official agreement (ID #132522), effective June 30, 2008, with the U.S. Department of Homeland Security, U.S. Citizenship and Immigration Services, Employment Verification Program (E-Verify). (Posting required by OCGA 13/10-91.).
Show more "
3577127411,Data Science / Data Engineer - Remote Work,BairesDev,2023-03-28,https://www.linkedin.com/jobs/view/data-science-data-engineer-remote-work-at-bairesdev-3577127411?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=9pLJ%2BGTlsMQDy8LIG1iMEA%3D%3D&position=9&pageNum=8&trk=public_jobs_jserp-result_search-card,2432,"Who We are

BairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.

Data Science / Data Engineer at BairesDev

We are looking for a Data Scientist / Data Engineers to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.

What You’ll Do

Interview other Data Scientists and Data engineers to vet their technical abilities.
Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.
Work together with developers, tech leads, and solution architects to build applications.
Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.
Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.

Here’s What We Are Looking For

5+ years of experience working as a developer.
3+ years of experience working as a Data Scientist / Data Engineer.
Proficient with analysis, troubleshooting, and problem-solving.
Hands-on experience with managing data loads and data quality.
Advanced English level.

How we do make your work (and your life) easier:

100% remote work.
Hardware setup for you to work from home.
Flexible hours - make your schedule.
Paid parental leave, vacation & holidays.
Diverse and multicultural work environment.
An innovative environment with the structure and resources of a leading multinational.
Excellent compensation — well above the market average.
Here you can grow at the speed of your learning curve.

Our people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.

Every BairesDev team member brings something unique to our company.

We want to hear your story. Apply now!
Show more "
3576650048,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576650048?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=%2F6rLdzNpx1WRQyA7eyBS%2Fw%3D%3D&position=10&pageNum=8&trk=public_jobs_jserp-result_search-card,1082,"Job Description




Job Title: Data Engineer




Location: 100% Remote




Position type: Fulltime




Job Description




Design and Develop scalable Big Data Warehouse solutions using cloud technologies.




Create and review technical and user-focused documentation for data solutions (data models, data dictionaries, business glossaries, process and data flows, architecture diagrams, etc.).




Extend and enhance the business Data Lake




Collaborate with management, business partners, analysts, developers, architects, and engineers to support data quality efforts.




Work closely with the Data Science and Business team to improve actionable data




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3576620119,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576620119?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=IHhYOE7LXojtLtrba0%2FdFg%3D%3D&position=11&pageNum=8&trk=public_jobs_jserp-result_search-card,2417,"Job Description




Title : Data Engineer




Location : Cincinnati , US Remote till COVID




Type Full time




Jd




Good knowledge of data integration practices in Python ,Azure. Should have experience in scheduling data flow tasks in Azure Data Factory / SSIS using APIs, cloud connectors, etc.
Strong technical analytics expertise. Good knowledge of data analytics architectures and standards, including data warehouses, master data management, ETL, OLAP, data quality management, advanced analytics, BI, visualization and service execution rigor / discipline
Data Analytics expertise, including tools, techniques, and data visualization best practices (e.g., Azure Data Factory, Alteryx, Power BI)
In-depth knowledge of data integration techniques and data management
Ability to operate digitally and display a digital mind set Works with a geographically dispersed team, highly self-motivated, able to work independently
Ability to complete multiple medium-sized projects and work with project teams
Coaches junior team members
Able to meet deadlines independently
Good communication and inter-personal skills
Creating data integration frameworks, set up data flow solutions, gather and cleanse data, understands the meaning of datasets and dataset compatibility and implements business rules.
Will support data scientists, product managers and analysts on data initiatives and will ensure optimal data delivery is consistent across projects.
Create and schedule data flow tasks in Python . Should be able to create automated data validation methods.
Assemble, gather and cleanse data sets that meet business requirements.
Queries and/or integrates data by using SQL, SQL-like, ELT or ETL skills tools and techniques.
Creates data models by delivering calculations, aggregations and derivations from input data.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure big data technologies.




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3580994759,Software Engineer,Nike,2023-03-28,https://www.linkedin.com/jobs/view/software-engineer-at-nike-3580994759?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=OzQLolDvhqr%2BXM9VEF2nJw%3D%3D&position=12&pageNum=8&trk=public_jobs_jserp-result_search-card,3552,"Become a Part of the NIKE, Inc. Team

NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.

NIKE is a technology company. From our flagship website and five-star mobile apps to developing products, managing big data and providing leading edge engineering and systems support, our teams at NIKE Global Technology exist to revolutionize the future at the confluence of tech and sport. We invest and develop advances in technology and employ the most creative people in the world, and then give them the support to constantly innovate, iterate and serve consumers more directly and personally. Our teams are innovative, diverse, multidisciplinary and collaborative, taking technology into the future and bringing the world with it.

NIKE USA, Inc., Boston, MA. Provide business consulting, systems, data, application, analysis, programming and documentation. Develop, code, configure, and test applications, systems and solutions independently with minimal supervision in order to meet defined digital product specifications. Execute the deployment of code across environments. Advise product owners on discrete technology-related business problems. Formulate technical solution options, including assessing their relative merits and risks. Identify technical issues and determine options for issue resolution and risk mitigation. Define and communicate requirements for technical environments and determine the technical scope for projects. Work with product owners to determine the best solutions. Help design and build scalable software solutions to implement and integrate new technologies. Build and implement scalable applications that leverage prediction models and optimization programs to deliver data driven decisions that result in immense business impact. Lead the development of a technical solution that meets the needs of the business and aligns with architectural standards. Create and apply architecture, governance, security, and global process standards to system changes and deployments. Review and approve performance test results, recommendations, and tuning results.

Applicant must have a Master’s degree in Computer Science, Computer Information Systems, or Electrical Engineering and two (2) years of experience in the job offered or related occupation.

Experience must include:

Python
Algorithms and data structures
AWS
Database technologies
Caching technologies
Scrum model
Data processing technologies
Devops
Backend development
System Design


NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.

]]>


Show more "
3573884257,Data Engineer,Navanti Group,2023-04-20,https://www.linkedin.com/jobs/view/data-engineer-at-navanti-group-3573884257?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=TuqEvK%2BSWGZhbnVVs8l04w%3D%3D&position=13&pageNum=8&trk=public_jobs_jserp-result_search-card,5561,"Project/Team: NATO




Location : Client Site (NATO HQ SACT) – Norfolk, VA




Employment Type: Full-Time




Number of Vacancies: 1




POSITION NOT YET FUNDED. SOLICITING RESUMES FROM INTERESTED CANDIDATES FOR ANTICIPATED POSITION.




Background




Navanti is seeking a Data Engineer to assist a prospective contract with the North Atlantic Treaty Organization (NATO). Navanti is a dynamic small business focused on providing subject matter expertise and data collection capabilities for a range of private and public clients; this position is part of NATO’s Capability Development Management Support (CDMS) and would be stationed in-person, full time in Norfolk, Virginia.




Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT’s understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the center of gravity for HQ SACT’s efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.




Tasks




Contribute to the development and implementation of an enabling data science and AI capability at HQ SACT and for the NATO.
Contribute to ML/AI initiatives across HQ SACT and the NATO Enterprise with a particular focus on the data engineering side.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, proposals on how to re-design infrastructure for greater scalability.
Develop, construct, test, and maintain data pipelines and architectures such as databases and large-scale processing systems, within the constraints of existing but evolving processes and technologies.
Transform data into formats that can be easily analyzed by developing, maintaining, and testing infrastructures for data generation.
Prepare data for prescriptive and predictive modelling.
Provide subject matter expertise to (military and civilian) staff within HQ SACT or the NATO Enterprise and develop proofs of concept, as directed.
Work in tandem with data scientists and software engineers.
Select from existing data sources and prepare data to be used by data science models.
Improve data quality and efficiency.
Support evaluation of operational requirements and objectives.
Interpret trends and patterns and support building of algorithms and prototypes.
Support educational efforts and training development related to data, AI or digital literacy.
Remain up-to-date with new developments in data engineering and data architectures to bring innovative ideas into implementation.
Support building a data-driven culture that uses data and analytics to generate insights, improve decision making at all levels, inform strategy and policy decisions, and improve performance.
Perform additional tasks as required by the COTR related to the LABOR category.




Qualifications




A Bachelor of Science degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years.
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service.
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas – such as Star or Snowflake.
Ability to work with large data sets is required.
A Master’s degree or higher from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field.
Knowledge of NoSQL databases such as MongoDB, CosmoDB recommended but not mandatory.
Ability to work in cloud environments to develop scalable data pipelines highly recommended. Skills in Cloud infrastructure and technologies such as Google Cloud Compute, AWS, Azure Data Factory, distributed computing will be highly advantageous.
Working experience with geospatial data structures such as raster and vector-based data is recommended.
Ability to collect and document project requirements, and to translate the requirements to technical solutions, including working in an agile environment to implement complex database projects is highly desirable.
Working experience in an international environment with both military and civilian elements.
Understanding of the NATO organization and its functions.
A valid NATO SECRET-level security clearance or an active SECRET security clearance issued by a national authority.
Show more "
3576547986,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576547986?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=zeTl0hbLPAChV2YT8fZZSA%3D%3D&position=14&pageNum=8&trk=public_jobs_jserp-result_search-card,457,"Job Description




Data Engineer




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3568508095,Software Engineer- Early Career,Lockheed Martin,2023-04-17,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3568508095?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=m13QV7ja1I90YurTmRlpBQ%3D%3D&position=15&pageNum=8&trk=public_jobs_jserp-result_search-card,4484,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Herndon, VA is seeking a Full Time Associate Software engineer. In this role, you will collaborate with a team of 3 - 5 other developers to develop mission critical software used around the world and engineer solutions to solve complex customer needs. The successful candidate will have experience and/or knowledge of software development, software design practices, and database structures. Must be a US Citizen; this position will require obtaining a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable degree programs include only science and technology related disciplines such as Mathematics, Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Information Technology, Management Information Systems, Cyber Security or related discipline.




Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information, including US Citizenship. Ability to obtain a TS/SCI Clearance required for this role.




Desired Skills




Proficiency in one of the related: C, C++, C#, Java, XML, Windows, .NET, and UNIX, system software and scripting development, Java programming, web development, System analysis, System design, Software Design, Software Development and Implementation, Software Test or Cyber Security.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI w/Poly




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: Standard Monday to Friday 40 hour work week




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3561622761,Data Engineer,Compunnel Inc.,2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3561622761?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=cdB2JvE4ANU44jBxqc3gNw%3D%3D&position=16&pageNum=8&trk=public_jobs_jserp-result_search-card,842,"Description




The Expertise and Skills You Bring




6-9 years experience in Oracle development (PL/SQL)




1+ years of Informatica/ETL experience




At least one year of data modeling experience




3+ years of solutioning experience with one or more of the following:




AWS Cloud environments (development, deployment, and support)




Oracle RDS/Aurora Postgres and Dynamo Databases




Containerization technologies




Application web server technology concepts




Python/PySpark




Experience working in an Agile team setting (Kanban and/or SCRUM)




Experience working in a DevOps and CI/CD environment




AWS certification a strong plus




Snowflake experience a strong plus




A Bachelor’s or Master’s degree in Computer Science, Information Technology, or equivalent experience




Education: Bachelors Degree
Show more "
3561156840,Data Analytics Sales Engineer,Bardess Group Ltd,2023-04-07,https://www.linkedin.com/jobs/view/data-analytics-sales-engineer-at-bardess-group-ltd-3561156840?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=1vZLbu655ym7IJgUT%2FoSSw%3D%3D&position=17&pageNum=8&trk=public_jobs_jserp-result_search-card,3922,"We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives. Responsibilities for Data Engineer




Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems. Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Show more "
3550339412,Data Engineer,Connexus® Energy,2023-03-31,https://www.linkedin.com/jobs/view/data-engineer-at-connexus%C2%AE-energy-3550339412?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=T9BXMS0GchrBjWdB3TkbIQ%3D%3D&position=18&pageNum=8&trk=public_jobs_jserp-result_search-card,3158,"Exciting opportunity to join our Data Services team as a Data Engineer. In this role you will apply leading edge techniques, skills, and tools to move data and develop routines for updates and optimization of the enterprise data warehouse. Connexus Energy is Minnesota's largest electric distribution cooperative providing electricity, renewable energy alternatives and related services to 141,000+ residential and commercial members. Located just north of the Twin Cities, we offer a collaborative work environment with challenging and rewarding work, professional development, strong benefits, and work-life balance in a highly technical, stable, and innovative industry. We give back to the communities we serve by embracing opportunities to volunteer, donate, and support economic development. Our talented and friendly staff focuses on achieving our mission of powering our members and communities toward a smarter energy future with a passionate focus on affordability, innovation, safety, and grid reliability. Key Responsibilities & Results:




Partner with other technology and data experts in the organization to develop solutions that ensure data warehouse reliability and high availability.
Design, develop, and test database solutions.
Maintain and optimize the database schema design.
Assist in determining how the data will be stored and utilized to best provide actionable data to the end user.
Install, maintain, and troubleshoot applications such as ETL tools, reporting tools, SharePoint, and business intelligence environments.
Conduct data warehouse health checks.
Maintain and tune SQL databases and monitor jobs and data loads.
Implement upgrades and patches to databases.
Coordinate and perform the installation and testing of new data sources and code, ensuring version control is enforced.
Ensure security of the database servers and the company's data. Required Talents, Skills, Expertise, Education:
Bachelors degree in computer science or similar
Two or more years SQL experience
Proven ability to work with large data
Skilled in some of the following: SQL, ETL tools, SharePoint, business intelligence environments
Critical thinker with ability to analyze and solve problems
Ability to build strong relationships across the organization and communicate effectively with others
Team player
Flexible, adaptable, and able to adjust to changing priorities Preferred Talents, Skills, Expertise, Education:
Masters degree
Five or more years SQL experience The application review process will begin on March 30 and continue until filled. Compensation: Connexus Energy offers all employees a competitive base salary, an annual incentive, and an attractive benefits package including medical, dental, life, company provided disability insurance, generous company contribution and match to 401(k), tuition reimbursement, and more. An Affirmative Action/Equal Opportunity Employer: Minorities, Women, Veterans, Disabilities All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, or veteran status .
Show more "
3577423213,Software Engineer,ARV Systems Inc,2023-04-22,https://www.linkedin.com/jobs/view/software-engineer-at-arv-systems-inc-3577423213?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=jB%2BYjGysVic%2FC1%2Fj6rvlrw%3D%3D&position=19&pageNum=8&trk=public_jobs_jserp-result_search-card,836,"This job was posted by https://illinoisjoblink.illinois.gov : For more information, please see: https://illinoisjoblink.illinois.gov/jobs/10844041




Designs, develop, codes, tests, and debugs complex new software products, or makes significant enhancements to existing software.




\ Research and integrate design strategies, product specifications, development schedules, and user expectations into product capabilities.




\ Create and document detail design specifications and unit test cases to ensure that all business and functional requirements are met.




\ \ Position Requirements: Position requires a Masters degree or foreign equivalent degree in Computer Science, Information Technology, Information Systems, Computer Applications, Electrical &/ or Electronics Engineering, Engineering(Any) or related field.
Show more "
3568508093,Software Engineer - Early Career,Lockheed Martin,2023-04-17,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3568508093?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=cn34AqZWgiBOTUFYXNkXyQ%3D%3D&position=20&pageNum=8&trk=public_jobs_jserp-result_search-card,4484,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Herndon, VA is seeking a Full Time Associate Software engineer. In this role, you will collaborate with a team of 3 - 5 other developers to develop mission critical software used around the world and engineer solutions to solve complex customer needs. The successful candidate will have experience and/or knowledge of software development, software design practices, and database structures. Must be a US Citizen; this position will require obtaining a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable degree programs include only science and technology related disciplines such as Mathematics, Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Information Technology, Management Information Systems, Cyber Security or related discipline.




Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information, including US Citizenship. Ability to obtain a TS/SCI Clearance required for this role.




Desired Skills




Proficiency in one of the related: C, C++, C#, Java, XML, Windows, .NET, and UNIX, system software and scripting development, Java programming, web development, System analysis, System design, Software Design, Software Development and Implementation, Software Test or Cyber Security.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI w/Poly




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: Standard Monday to Friday 40 hour work week




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3576650419,Data Engineers (Multiple Openings),Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineers-multiple-openings-at-diverse-lynx-3576650419?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=8kJVS2Qjo1mjfRwJhDl%2BrA%3D%3D&position=21&pageNum=8&trk=public_jobs_jserp-result_search-card,1555,"Job Description




Role: Data Engineers profile




Rate: DOE




Location: Hartford, CT or Remote




Responsibility




Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects




Requirements




Previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Hands on experience with NOSQL databases
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g. IBM Certified Data Engineer) is a plus




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3576370070,Data Engineer,GBM,2023-04-19,https://mx.linkedin.com/jobs/view/data-engineer-at-gbm-3576370070?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=oEue3hir7KPL%2Fj2UW5wLaA%3D%3D&position=22&pageNum=8&trk=public_jobs_jserp-result_search-card,3678,"México, Ciudad de México
Full time job




Descripción de la empresa




Acerca de GBM




Somos una organización de gestión patrimonial, de activos y banca privada con 35 años de experiencia; pioneros en inversión digital innovadora y productos de ahorro, y uno de los más recientes unicornios de LATAM.




Nuestra misión es mejorar la calidad de vida de las y los mexicanos a través de las inversiones, y buscamos lograrlo de la mano del mejor talento.




En GBM Digital y Tecnología hemos experimentado un crecimiento exponencial gracias a nuestros productos digitales y a nuestro equipo de clase mundial.




Trabajamos en equipo todos los días para cumplir nuestra misión de democratizar las inversiones en México y buscamos crear al mejor equipo de Tecnología, Producto y Diseño de LATAM. Somos una empresa diversa, incluyente y trabajamos 100% remoto.




Objetivo




GBM requiere la creación de una estrategia global de datos que combine buena ingeniería con analítica avanzada para poder conocer mejor a sus usuarios, su comportamiento y sus necesidades. Para lograr esto se está formando un equipo especializado para crear un Datalake con todas las entidades de datos necesarias para atender las necesidades analíticas de las áreas de negocio, software engineering y growth.




En Una Semana Típica




Construiras pipelines de datos para extraer información de diversas fuentes y prepararla para su consumo
Aplicaras procesos de limpieza, validación y Data Quality
Estructuraras y consolidaras datos para usarlos en aplicaciones analíticas
Construirás y mantendrás las estructuras de datos dentro del Data Lake y DWH
Mantendrás y supervisaras las arquitecturas tecnológicas, los procesos de ingesta y procesamiento de datos
Implementaras nuevas aplicaciones que usan datos
Optimizaras procesos
Certificaras procesos de usuario que pasen al entorno Core de Consumo
Productivización de procesos provenientes de los sandboxes
Apoyaras y darás soporte a los usuarios con preguntas técnicas específicas sobre herramientas de datos, Datalake y DWH




Escolaridad




Ingeniería en Sistemas Computacionales, Telecomunicaciones o Informática o experiencia profesional afín




Idioma




Inglés- Intermedio




Skills técnicos




SQL
noSQL
Python o Scala
R
Infraestructura como Código
Git
IntelliJ, Docker
SQL Server Integration Services
Pipelines de datos o ETL’s
Airflow
Modelado de datos
Metodologías ágiles




Competencias




Facilidad para entender la información en bases de datos
Rápido entendimiento de reglas de negocio
Alta proactividad
Orientado a objetivos
Comunicación efectiva, tanto verbal como escrita
Trabajo colaborativo
Adaptabilidad




Requisitos




4 años de experiencia con bases de datos y su manejo (SQL y noSQL)
2 desarrollo lenguajes de programación como Python, Scala o R
2 años de experiencia con infraestructura como código
2 años desarrollando pipelines de datos o ETL’s
2 años de experiencia con API de datos
2 años de uso de herramientas y programas que agilicen el procesamiento de datos (Git, Gitlab, Jenkins, IntelliJ IDEA, Docker, Gradle)
2 años de experiencia con SQL Server Integration Services




Información adicional




Beneficios




Salario y paquete de compensación competitivo
Contratación indefinida directamente por GBM
Esquema 100% nominal
Home Office
Formación y plan de carrera
Certificaciones AWS patrocinadas y presupuesto de capacitación para cada colaborador
Para postularte es indispensable que leas y aceptes nuestro Aviso de Privacidad para Candidatos que se alinea a la ley de protección de datos personales y especifica el uso que le daremos a los mismos únicamente con fines de reclutamiento.
Show more "
3578298195,Data Engineer,Diverse Lynx,2023-03-26,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3578298195?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=yS0cZCi1LxtlHUfhIW6A1Q%3D%3D&position=23&pageNum=8&trk=public_jobs_jserp-result_search-card,1722,"Job Description




The job description

Data Engineer

Tampa FL / 100 % remote is fine

Full time

BS/MS in Computer Science or related technical field
Proven understanding and application of modern data processing technology stacks. This includes Snowflake, Spark, Airflow, SQL, and others
Strong programming skills on Python, shell scripting and SQL
Experience with data warehouses/RDBMS like Snowflake (preferred), BigQuery, EMR, etc.
Experience with Designing and Engineering of robust cloud based systems (preferably on AWS)
Experience working on Distributed computing with Spark on Scala/Python
Solid foundation in data structures, algorithms and architecture patterns.
Ability to track, recommend, and implement resolutions to issues
Strong leadership skills with the ability to develop, train, inspire, and motivate team to accomplish objectives
Ability to build good relationships with peers, business partners, and clients
Excellent written and verbal communication skills
Desired Skills

Understanding of Serverless computing, Data Lake and API Frameworks
Knowledge of Kubernetes/Container platforms based development
Understanding/Implementation experience of Caching Frameworks - Redis, Hazelcast, Ignite, etc.
Experience with Data Analytics tools like Tableau, QuickSight, Power BI, etc

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3580140373,Data Engineer,Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-ag-3580140373?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=vPP1y8%2F0dJxTSS8ZBWk6%2Fw%3D%3D&position=24&pageNum=8&trk=public_jobs_jserp-result_search-card,5133,"Onsite | Pentagon | 5 Days a week




TS/SCI FSP Poly Clearance Required (Must have an active or recent FSP Poly completed in the past 7 years)




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives. Candidates must have a TS/SCI FSP Poly (must have an active or recent FSP Poly completed in the past 7 years).




Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering




Requirements




Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active TS/SCI FSP Poly (Must have an active or recent FSP Poly completed in the past 7 years) is requred.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3703




External Company URL: http://precision-solutions.com/
Show more "
3576650683,Data engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576650683?refId=pDTPdmols89zt8jA4cEdXA%3D%3D&trackingId=v0YjR8D2fDpbh9XWwhGqow%3D%3D&position=25&pageNum=8&trk=public_jobs_jserp-result_search-card,1036,"Job Description




Location: Cincinnati




Type: subcon/fulltime




Client: Client aviation




Duration: 6-12 months




Start date: 2nd week of March.




Rate: 60-62/hr and full time in range of 80K-90K




Experience and Knowledge in AWS Redshift mandatory, including table design, ETL loads and performance tuning




Very good knowledge of coding using PL\SQL language




Experience in Schema Conversion Tool and DMS tool




2&plus; years hands-on experience on PostgreSQL




DevOps experience with AWS Infrastructure concepts, Establishing version control using GIT, Setting up Alarms and Monitoring




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3580138488,Data Engineer,Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-ag-3580138488?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=5N6PzmpLD2d7DPKEY83ieA%3D%3D&position=1&pageNum=9&trk=public_jobs_jserp-result_search-card,4897,"Hybrid | Crystal City, VA | 3 Days a week Onsite




Interim Secret Clearance Required




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.




Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering




Requirements




Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an Interim Secret clearance is required.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3699




External Company URL: http://precision-solutions.com/
Show more "
3580133786,Data Engineer,Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-ag-3580133786?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=ckqolQ6tEY6kl1XrpzOhrg%3D%3D&position=2&pageNum=9&trk=public_jobs_jserp-result_search-card,4863,"Onsite | Pentagon | 5 Days a week




TS/SCI Clearance Required




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.




Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering




Requirements




Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active TS/SCI is required.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3702




External Company URL: http://precision-solutions.com/
Show more "
3575812114,Software Engineer- Early Career,Lockheed Martin,2023-03-29,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3575812114?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=rGH44AegOHH3NeIi7AT12g%3D%3D&position=3&pageNum=9&trk=public_jobs_jserp-result_search-card,4484,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Herndon, VA is seeking a Full Time Associate Software engineer. In this role, you will collaborate with a team of 3 - 5 other developers to develop mission critical software used around the world and engineer solutions to solve complex customer needs. The successful candidate will have experience and/or knowledge of software development, software design practices, and database structures. Must be a US Citizen; this position will require obtaining a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable degree programs include only science and technology related disciplines such as Mathematics, Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Information Technology, Management Information Systems, Cyber Security or related discipline.




Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information, including US Citizenship. Ability to obtain a TS/SCI Clearance required for this role.




Desired Skills




Proficiency in one of the related: C, C++, C#, Java, XML, Windows, .NET, and UNIX, system software and scripting development, Java programming, web development, System analysis, System design, Software Design, Software Development and Implementation, Software Test or Cyber Security.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI w/Poly




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: Standard Monday to Friday 40 hour work week




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3569703907,"Data Engineer (AWS, Python, SQL)",Cerotid Inc,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-aws-python-sql-at-cerotid-inc-3569703907?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=jfMwKqBqWepJ%2BiM%2BtSXc7g%3D%3D&position=4&pageNum=9&trk=public_jobs_jserp-result_search-card,1111,"Title: Data Engineer

Industry: Banking & Financial

Duration: 12 Months- Long term

Location: NC

Top Skills: Oracle/ DynamoDB Python Looking for SR level candidates 7+ years of experience

Nice to Have Skills: Snowflake API Development Skills Big Data Skills

The Expertise And Skills You Bring

6-9 years experience in Oracle development (PL/SQL)
1+ years of Informatica/ETL experience
At least one year of data modeling experience
3+ years of solutioning experience with one or more of the following:
AWS Cloud environments (development, deployment, and support)
Oracle RDS/Aurora Postgres and Dynamo Databases
Containerization technologies
Application web server technology concepts
Python/PySpark
Experience working in an Agile team setting (Kanban and/or SCRUM)Experience working in a DevOps and CI/CD environmentAWS certification a strong plusSnowflake experience a strong plusA Bachelor's or Master's degree in Computer Science, Information Technology, or equivalent experience

Required Skills

Basic Qualification :

Additional Skills : Only Durham, NC (Preference is for local candidates)
Show more "
3576942081,Business Intelligence Data Engineer,"Barnes & Noble, Inc.",2023-03-26,https://www.linkedin.com/jobs/view/business-intelligence-data-engineer-at-barnes-noble-inc-3576942081?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=kf%2F5HsXw05GF2gj9aWq79g%3D%3D&position=5&pageNum=9&trk=public_jobs_jserp-result_search-card,4812,"NY-New York (Union Square)

Job Summary

The Business Intelligence Data Engineer will be a key member of the Barnes & Noble IT organization. The Business Intelligence Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, support cross functional teams in generating timely insights. The ideal candidate is a data specialist experienced in designing, developing, and deploying complex data pipelines in Azure Cloud platform

The Business Intelligence Data Engineer will support our software developers, database architects, data analysts, dashboard developers and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The role requires solid technical skills in designing and delivering large-scale enterprise data platforms on Azure Cloud combined with very strong communication skills.

An employee in this position can expect an annual starting rate between $140,000 and $160,000, depending on experience, seniority, geographic locations, and other factors permitted by law.

What You Do

Deploy new solutions and configurations to meet business and compliance requirements.
Participate in 24x7 on call rotations.
Discover current technical standards and best practices (R&D).
Deploy security patches, updates, and configuration changes.
Manage consultants to ensure compliance with Barnes & Noble engineering and business standards.

Knowledge & Experience

Work with multiple business stakeholders in defining the right data requirements to fulfill growing analytics / insights needs across the enterprise
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, optimal cost and performance.
Design right infrastructure / compute configuration for optimal extraction, transformation, and loading of data from a wide variety of data sources into ADLS, Databricks and Synapse.
Develop data pipelines using PySpark, Python and DB SQL in Databricks in Lakehouse architecture
5+ years of experience in a Data Engineering environment with hands on experience developing ADF (Azure Data Factory) pipelines for an enterprise solution.
3+ years of experience in writing code in Databricks using Python to transform, manipulate (ETL/ELT) data, along with managing objects in Notebooks, Data Lake, ADLS, Azure Synapse.
Experience with writing complex SQL Queries, User Defined Function, Stored procedures and Materialized views. Someone who comes from database development background and have transitioned to Azure Cloud/Data Lake/Synapse.
Working experience with Azure DevOps and Source controls.
Experience working in a large Retail enterprise and understanding of Retail based data and reporting models.
Experience with reporting tools like PowerBI/Tableau/MicroStrategy
Strong analytic skills related to working with different types of datasets from wide variety of data sources
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Understanding of ELT and ETL patterns and when to use each.
Undergraduate degree required (Graduate degree preferred) in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field.
Experience using the following software/tools/services: Azure Data Factory, Azure Data Lake Storage, Azure Databricks, Azure Synapse, SQL, PySpark
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools

Auto req ID

65382BR

Employment Type

Full-Time

City

New York

State

New York

EEO Statement

Barnes & Noble is an equal opportunity and affirmative action employer. All qualified applicants will receive consideration for employment without regard to age, race, color, ancestry, national origin, citizenship status, military or veteran status, religion, creed, disability, sex, sexual orientation, marital status, medical condition as defined by applicable law, genetic information, gender, gender identity, gender expression, hairstyle, pregnancy, childbirth and related medical conditions, reproductive health decisions, or any other characteristic protected by applicable federal, state, or local laws and ordinances.

Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format using a sign language interpreter, or using specialized equipment. Contact (800) 799-5335.

Job Category

Information Systems & Technology
Show more "
3557834031,Data Engineer,DEFTEC Corporation,2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-at-deftec-corporation-3557834031?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=%2FvCzpfSXkiMe8WBpxvEaLw%3D%3D&position=6&pageNum=9&trk=public_jobs_jserp-result_search-card,7515,"DEFTEC delivers mission-critical solutions through skillfully delivered services and innovative products. We are inspired by the critical missions of our clients, and we are driven to provide the most effective solutions to execute their missions, operational challenges, and requirements. Our dedicated, experienced, and talented employees work closely with our clients to ensure the delivery of exceptional services and products.




Position Overview




Data Engineer directly supports the Allied Command Transformation (ACT). ACT is NATO's leading agent for change: driving, facilitating and advocating the continuous improvement of Alliance capabilities to maintain and enhance the military relevance and effectiveness of the Alliance. The main objectives of ACT are: providing appropriate support to NATO missions and operations; leading NATO military transformation; improving relationships, interaction, and practical cooperation with partners, nations, and international organizations. ACT, therefore, leads Alliance concept development, and capability development. The exploitation of cyberspace as a domain of operations presents an increasingly growing challenge for the Alliance, requiring new strategies, concepts, architectures, processes, and capabilities to enable NATO to defend itself in cyberspace as effectively as it does in the physical domains. Gaining the ability to plan, assess and conduct military operations in cyberspace requires the development of new concepts and capabilities, notably in the area of cyberspace resilience, mission assurance, situational awareness, Intelligence, Surveillance and Reconnaissance (ISR) or C3 (Command, control, and consultation).




Data science, data analytics and Artificial Intelligence (AI) are increasingly gaining momentum in NATO touching all military and political domains and functional areas. In response to HQ SACT's understanding of the disruptive potential of data science and AI, and recognizing the strategic value of data, the Data Science & Artificial Intelligence section, established in 2020 in the Federated Interoperability Branch, is focusing on data science and AI as cross-cutting and enabling capabilities for HQ SACT and the NATO Enterprise. The section provides a broad spectrum from strategy and policy development and support, to technical delivery and implementation to HQ SACT and the NATO Enterprise. In addition to serving as the centre of gravity for HQ SACT's efforts in advancing data centricity and integrating rapidly changing technology related to data exploitation, the section has developed a substantial reputation inside NATO and is regularly invited to offer policy and technical expertise.




Job Responsibilities




Contribute to the development and implementation of an enabling data science and AI capability at HQ SACT and for the NATO
Contribute to ML/AI initiatives across HQ SACT and the NATO Enterprise with a particular focus on the data engineering side
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, proposes how to re-design infrastructure for greater scalability
Develop, construct, test and maintain data pipelines and architectures such as databases and large-scale processing systems, within the constraints of existing but evolving processes and technologies
Transform data into formats that can be easily analyzed by developing, maintaining, and testing infrastructures for data generation
Prepare data for prescriptive and predictive modelling
Provide subject matter expertise to (military and civilian) staff within HQ SACT or the NATO Enterprise and develop proofs of concept, as directed
Work in tandem with data scientists and software engineers
Select from existing data sources and prepare data to be used by data science models
Improve data quality and efficiency
Support evaluation of operational requirements and objectives
Interpret trends and patterns and support building of algorithms and prototypes
Support educational efforts and training development related to data, AI or digital literacy
Remain up-to-date with new developments in data engineering and data architectures to bring innovative ideas into implementation
Support building a data-driven culture that uses data and analytics to generate insights, improve decision making at all levels, inform strategy and policy decisions, and improve performance
Perform additional tasks as required by the COTR related to the LABOR category




Qualifications




Required Qualifications:




A Master's degree from a recognized university in computer science, IT, software or computer engineering, data science, applied math, physics, statistics, or a related field
Experience with advanced level SQL, including query optimization, complex joins, development of stored procedures, user-defined functions and working with Analytic Functions in the last 3 years
Proficient in at least one data manipulation language such as Python, Scala, R, etc.
Ability to develop ETL processes for batch and streaming data, with proficiency in tools and technologies such as Apache Spark, Apache Airflow, Pentaho Data Integration, SQL Server Integration Service
Advanced knowledge of relational database architecture, including design of OLAP and OLTP databases is required. Must have experience working with at least one Data Warehouse schemas - such as Star or Snowflake
Ability to work with large datasets is required
Knowledge of NoSQL databases such as MongoDB, Cosmo DB recommended but not mandatory
Ability to work in cloud environments to develop scalable data pipelines highly recommended. Skills in Cloud infrastructure and technologies such as Google Cloud Compute, AWS, Azure Data Factory, distributed computing will be highly advantageous
Working experience with geospatial data structures such as raster and vector-based data is recommended
Ability to collect and document project requirements, and to translate the requirements to technical solutions, including working in an agile environment to implement complex database projects is highly desirable
Working experience in an international environment with both military and civilian elements
Understanding of the NATO organization and its functions




DEFTEC offers a comprehensive whole-life benefits package that includes medical, dental, vision, holiday, paid time off, 401K with a match, life insurance, short/long-term disability, and educational reimbursement.




DEFTEC is a Drug-Free Workplace where post-offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria are met as outlined in our policies.




AAP/EEO Statement




DEFTEC Corp is an Equal Opportunity and Affirmative Action Employer and prohibits discrimination and harassment of any type based on actual or perceived race, color, national origin, ancestry, sex (including pregnancy, childbirth, breastfeeding , and medical conditions related to pregnancy, childbirth or breastfeeding), gender, gender identity, and gender expression, religious creed, disability (mental and physical) including HIV and AIDS, medical condition (cancer and genetic characteristics ), genetic information, age, marital status, civil union status, sexual orientation, military and veteran status, denial of family and medical care leave, arrest record and/or any other characteristic(s) protected by federal, state or local law.




Job Posted by ApplicantPro
Show more "
3561044201,Data Engineer,RevOpsforce,2023-04-12,https://www.linkedin.com/jobs/view/data-engineer-at-revopsforce-3561044201?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=kkCXaYITsM9XJKmc1Yo%2BGg%3D%3D&position=7&pageNum=9&trk=public_jobs_jserp-result_search-card,2969,"About RevOpsforce:

At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.

Type: Contract 12+ months - Immediate

Job Description:

We are seeking a highly skilled data engineer with expertise in Talend to join our team. As a data engineer, you will be responsible for designing, developing, and maintaining our data pipelines and infrastructure, ensuring our systems are efficient, reliable, and scalable.

Key Responsibilities:

Design, develop, and maintain Talend-based ETL workflows, data integration processes, and data pipelines that transfer data from various sources into our data warehouse and data lake systems.
Develop Talend jobs and workflows for data transformation, cleansing, and data quality checks.
Collaborate with data analysts, data scientists, and business stakeholders to understand data requirements, identify data quality issues and resolve them.
Develop, maintain and monitor data processing pipelines for the organization's data platform and ensure high availability, reliability, and scalability.
Optimize data storage and retrieval processes to ensure efficient data processing and reduce data latency.
Collaborate with cross-functional teams to ensure data processing pipelines align with overall system architecture and business requirements.
Identify and troubleshoot system issues, and provide timely resolution to minimize downtime.
Maintain documentation for the Talend-based data processing pipelines and infrastructure.


Requirements:

Bachelor's degree in Computer Science, Information Technology or a related field.
Minimum of 5 years of experience in designing, developing and maintaining data pipelines using Talend.
Experience in implementing data integration solutions using Talend.
Strong knowledge of SQL and experience in working with databases such as Oracle, MySQL, PostgreSQL, or SQL Server.
Experience with cloud-based technologies such as AWS, Azure, or Google Cloud Platform.
Understanding of data modeling concepts and experience working with data warehousing and data lake technologies.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Ability to work independently and as part of a team.


We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Website is coming soon and will be located at www.revopsforce.com
Show more "
3580137577,Data Engineer (PSA),Precision Solutions Ag,2023-04-24,https://www.linkedin.com/jobs/view/data-engineer-psa-at-precision-solutions-ag-3580137577?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=AdJSM4yo3wW%2F7IW2RP6gGw%3D%3D&position=8&pageNum=9&trk=public_jobs_jserp-result_search-card,5250,"Remote (Must be local) | Washington D.C. | As Needed Onsite




Interim Secret Clearance Required




Summary




Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients' success and ensure that their initiatives are properly aligned with their core mission.




Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.




They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.




Responsibilities




Our client is seeking a Data Engineer with experience in data analytics, Python, Databricks, and AI/ML modeling to join their team. The ideal candidate will be responsible for developing, designing, and implementing data solutions to support PSA initiatives.




Develop, design, and implement data solutions to support the company's PSA initiatives
Collaborate with cross-functional teams to identify business requirements and design data models, ETL processes, and data visualizations
Develop and maintain ETL processes using Python, Databricks, and other relevant technologies to extract, transform, and load data from various sources into the PSA system
Develop and maintain data pipelines and data warehouses to support data analytics and reporting needs
Design and implement data visualizations and dashboards using Tableau, Power BI, or other relevant tools
Build and maintain AI/ML models to support predictive analytics and data-driven decision-making
Work with the Cybersecurity team to ensure data security and compliance with security protocols and standards
Troubleshoot and resolve data engineering issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering, data analytics, and AI/ML modeling and recommend improvements and optimizations to the data engineering process




Requirements




3+ years of experience working as a Data Engineer, with experience in data analytics, Python, Databricks, and AI/ML modeling
Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with ETL tools (e.g., Informatica, Talend, Apache NiFi)
Experience with data analytics and visualization tools (e.g., Tableau, Power BI)
Strong programming skills in Python and experience with relevant libraries (e.g., Pandas, NumPy, SciPy)
Experience with cloud computing platforms (e.g., AWS, Azure, Google Cloud)
Experience with AI/ML modeling and frameworks (e.g., TensorFlow, Keras, PyTorch)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills




Preferred Requirements




A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.




Education Requirements




None.




Clearance Requirements




Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an Interim Secret clearance is required.




Other Duties




Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.




About Us




Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.




Equal Opportunity Employer Statement




Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.




Precision Solutions Requisition Number: 2023-3709




External Company URL: http://precision-solutions.com/
Show more "
3569847026,Data Analytics Engineer (Remote - US),ebbo,2023-04-17,https://www.linkedin.com/jobs/view/data-analytics-engineer-remote-us-at-ebbo-3569847026?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=w%2BadFIiPaHWpGP%2Fw7h1fQg%3D%3D&position=9&pageNum=9&trk=public_jobs_jserp-result_search-card,4297,"Overview Of The Data Analytics Engineer

Join our dynamic Data Engineering team as a Data Analytics Engineer, where you will be a key player in expanding and enhancing our data infrastructure and analysis capabilities. You will work closely with both data and analytics experts and stakeholders such as Managers, Product, Data, and Design teams, to provide technical support and resolve data-related issues.

Duties And Responsibilities Of The Data Analytics Engineer

Build and maintain a robust data pipeline architecture for accounting and financial systems.
Conceptualize and design optimal solutions for data delivery, with a focus on automation and scalability.
Integrate large and complex data sets from multiple sources to meet business requirements.
Develop advanced data analytics tools to drive key business performance insights, such as customer acquisition and operational efficiency.
Create Tableau and Microsoft Power BI dashboards using multiple data sources.
Collaborate with data and analytics experts to continually improve the data systems and processes.

Skills Of The Data Analytics Engineer

Advanced SQL proficiency and 4+ years of experience with relational databases (SQL Server) and query authoring.
Proficient Snowflake database experience (2+ years), including ELT using Matillion with Snowflake (2+ years), particularly in migrating SQL Server CDC to Snowflake
Proficient Business Intelligence and Data Visualization background using Tableau and Microsoft Power BI, and experience as an Analytics Engineer.
Proficiency in AWS cloud services (EC2, S3, LAMBDA) and object-oriented/object function scripting languages (C#, Python).
Experience in building and optimizing big data pipelines, architectures, and data sets, and performing root cause analysis for business improvement.

Perks And Benefits Of The Data Analytics Engineer

Unlimited PTO, closing every Friday at 2pm (all year round) and a paid week off between Christmas and New Year's.
Health Benefits: Medical, Dental, Vision, Disability, Life, Safe Harbor 401K plan with a 3% contribution from ebbo, 100% vested to you right away.
Annual bonus and merit increases
DEI Program and Wellness Program with great prizes.
Every team receives an annual budget for training opportunities.
Tons of opportunity for mentorship and growth (career trajectories for employees).

Salary Range: $125,000 - $130,000

About Ebbo

At ebbo, our focus is on our team members. Your growth and work/life balance are always top of mind. The minute you step through the door (either in-office or virtually), you’ll be joining a company that values everyone’s input, rewards and recognizes exemplary work, prioritizes diversity, equity and inclusion and loves to have fun. You’ll also be a part of a company that is constantly being recognized for excellence. We’ve been chosen as a “Connecticut Top Workplace” nine years in a row, named one of Boston’s Best & Brightest “Top Company Cultures” nationally and featured in Forbes, Fast Company, Glossy and Chain Store Age.

From a business perspective, ebbo is an all-in-one loyalty company. In addition to our suite of uniquely configurable loyalty solutions, ebbo’s data driven, full-service approach helps brands build customer engagement on repeat. ebbo also operates a direct-to-consumer business, called Clarus Direct, where we manage consumer-facing membership shopping products like LivePlayGo, ShopSmarter and FreeShipping.com. Life is short, so join a company where you can turn a job into a career – and have a great time doing it. Learn more about us here !

ebbo is proud to be an Equal Opportunity/Affirmative Action employer. As such we are collecting this data from applicants. Completion of this data is voluntary and will not affect your opportunity for employment or terms or conditions of employment. This data will be used for reporting purposes only and will be kept separate from all other personnel records and only accessed by the Human Resources Department. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other basis prohibited by law.

Department: Data

This is a full time position
Show more "
3576620721,Data Engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576620721?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=1gIVaUa4JRehJwtU0ayy9w%3D%3D&position=10&pageNum=9&trk=public_jobs_jserp-result_search-card,755,"Job Description




Relevant Experience




(in Yrs) 8 years & Above Technical/Functional Skills




8&plus; years of experience as a data engineer
Strong Programming skills in Scala
Strong programming skills in Spark
AWS working experience is a must
Must have experience working in Airflow requirements
Experience working in Agile




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3578861790,Data Engineer,CSS Corp,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-at-css-corp-3578861790?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=2bYjjEaTtVCQpVN9H1ULGw%3D%3D&position=11&pageNum=9&trk=public_jobs_jserp-result_search-card,4547,"Pay Scale: $73 p/h, W2, no benefits




Duration: Full Time/Contract




Status: US Citizen or Green Card only




Reports To: Project Manager




Working Hours: Normal business hours




Work Location: Onsite, Customer Premises, Vancouver, WA 98683




Summary/Objective




Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.




The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.




Responsibilities




Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.




What You Bring




Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java




Knowledge And Skills




Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.




Nice to Have




Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc
EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.




Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
Show more "
3571692749,Data Engineer - DeFi,CryptoRecruit,2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-defi-at-cryptorecruit-3571692749?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=LvJrqZByVuYbXae6w3a4eg%3D%3D&position=12&pageNum=9&trk=public_jobs_jserp-result_search-card,4389,"As a Data Pipeline Engineer, you would have a hand in the design, implementation, deployment, and support of our company's Data Service project. As a member of the Data Service team, you will work on foundational data infrastructure.

You’d optimize the company's external visibility and assist various 3rd party sources to leverage our Data Service. You’d help architect real-time microservices that capture the company's blockchain events and build systems that provide support to our external API services where users can monitor network health and other various analytics over time.

As a company team member, you would be responsible for creating technically viable software with a team of senior engineers specializing in DevOps, distributed systems, system architecture, testing, and other related fields. You would be collaborating with some of the most diligent minds in the cryptocurrency industry on product direction, both on the core company team and among its partners, investors, and advisors. As an early team member, you must feel comfortable working in a fast-paced environment where the solutions aren’t already predefined.

Prior experience with blockchain projects is helpful but we are primarily interested in the capacity to grow into the role. You should have prior experience in developing high-quality backend architecture and some passing knowledge of how such architecture principles should apply to blockchain data services.

We are looking for individuals who are passionate about being at the forefront of a new technological paradigm and can lead the design and development of scalable applications.

Requirements

Build and support a data pipeline platform that allows a customer's behavioral data to directly impact their individualized experiences on the company.
Learn to evaluate multiple technical approaches and drive consensus with your engineering peers
Use data to solve real-world problems and assist both internal and external partners with data integrations
You will be responsible for ensuring access to data and tooling for the Core Engineering/Product team to leverage for direct customer use
Developing sound testing and debugging practices
Creating technical documentation and well-commented code for open-source consumption
Participating in open source development on shared resources with external development teams

Qualifications

Experience with data modeling, data warehousing, and building data pipelines
Experience in SQL and building a time-series database (Postgres)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficiency in, at least, one modern scripting or programming language such as Python, NodeJS
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Strong understanding of distributed systems and Restful APIs.
You’re opinionated about tooling and curious about new trends and technologies in the software development world.
Independence and self-motivation
4+ years of engineering experience
Bonus Points:

Data Processing - experience with building and maintaining large scale and/or real-time complex data processing pipelines using Kafka, Hadoop, Hive, Storm, or Zookeeper
Experience with large-scale distributed storage and database systems (SQL or NoSQL, e.g. MySQL, Cassandra)
Background in academic economics or finance
Familiarity with Cosmos, Tendermint, or Thorchain
Familiarity with the Rust/Golang programming language
Experience in small startup environments
Experience with a distributed team / remote work
Make sure to follow us here to get our most live jobs https://www.linkedin.com/company/cryptorecruit

Cryptorecruit are the worlds leading specialist recruiter for the blockchain/Cryptocurrency industry. We recruit positions from CEO,CTO, Project Manager, Solidity developer, frontend and Backend Blockchain developers to marketing/sales and customer service roles. Please browse our website and at www.cryptorecruit.com to search all our job vacancies.
Show more "
3571478815,Data Engineer,"Precision Solutions, LLC",2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-llc-3571478815?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=ob145zFKQYOUYG%2BQdSD%2ByQ%3D%3D&position=13&pageNum=9&trk=public_jobs_jserp-result_search-card,4744,"Overview

Data Engineer

Hybrid | Crystal City, VA | 3 Days a week Onsite

Interim Secret Clearance Required

Summary

Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients’ success and ensure that their initiatives are properly aligned with their core mission.

Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.

They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.

Responsibilities

Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.

Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering

Requirements

Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills

Preferred Requirements

A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.

Education Requirements

None.

Clearance Requirements

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an Interim Secret clearance is required.

Other Duties

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

About Us

Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.

Equal Opportunity Employer Statement

Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
Show more "
3542751700,Data Engineer (Azure Data bricks/ Pyspark) - Remote,Lorven Technologies Inc.,2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-azure-data-bricks-pyspark-remote-at-lorven-technologies-inc-3542751700?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=yTzCh%2BfDvP8SgBr%2BMBglPA%3D%3D&position=14&pageNum=9&trk=public_jobs_jserp-result_search-card,1420,"Our client is looking for The Data Engineer for a Long-term Remote below are the detailed requirements.

Kindly share your Updated Resume with the Best Reachable Number.

Role: Data Engineer (Azure Data bricks/ Pyspark)

Location: Remote

Duration: Long-term Contract

Required Skills: Azure Data bricks

Nice to have skills: Azure Data Factory, Python.

Job Description

Bachelors in Computer Science and/or equivalent combination of education with 09+ Years of work experience.
The Client is looking for AZURE DEVELOPER (RAMP UP).
Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW.
Hands-on Experience in SQL Server project development writing SQL, TSQL Code, Views development, Stored Procedures and functions is a must.
Knowledge on SQL Code Performance Tuning and optimizing SQL Code.
Develop Cosmos DB Server side program, Collections, Partitions etc.
Work on ADF routines to load data from Azure Data Lake Store to Cosmos DB.
Work closely with business analysts/users / API / Reporting to understand which entities are expected for exposing to Users, so that the Cosmos Collection can be built efficiently.
Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and a high level of focus and attention to detail.
Strong work ethic with good time management with the ability to work with diverse teams.
Show more "
3556967595,Data Engineer,You.com,2023-04-18,https://www.linkedin.com/jobs/view/data-engineer-at-you-com-3556967595?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=kuTQTvEnY23F9hq%2FuSRVMw%3D%3D&position=15&pageNum=9&trk=public_jobs_jserp-result_search-card,4820,"About You.com




You.com is the world's first open search engine platform that summarizes the web for users, with no ads, superior privacy choices, and personalization through preferred sources.




You.com is more than just a new search engine — it's a movement to make the internet a place of trust, facts, and kindness — our guiding principles that we're committing to from day one.




It's an audacious goal, but we're ambitious people. We're looking for a few more ambitious folks to join our team.




We’re not just wide-eyed dreamers – we’re pragmatic doers, too. Our founder and CEO, Richard Socher previously started an AI company called MetaMind. Salesforce acquired the company, and Richard became Chief Scientist, leading the company’s AI efforts. Prior to MetaMind, Richard received the best Ph.D. thesis award from Stanford for his groundbreaking work on deep learning. Bryan McCann , co-founder and CTO, is a scientist and philosopher who led natural language processing teams at Salesforce after completing his Master's in C.S. at Stanford. Our founding team members have built companies worth hundreds of millions of dollars and scaled software to serve millions of users. For fun, we run marathons, paramotor, write poetry, read Latin, hike, and camp in the middle of nowhere.




If this sounds intriguing, say hello!




About The Job




We’re building privacy-respecting analytics at web scale, to learn what our users love and continuously improve our product. Exec, Marketing, Product, and Eng teams rely on our analyses to make decisions, draw insights, and plan their strategy. We are the lantern in the dark, helping truth seekers find the way.




As a data engineer you will help us strengthen the foundation of our work, bringing in expertise in event collection, processing pipelines, and data management. You bring expertise in ETL (or ELT), data warehouses, instrumentation and pipelines, and you get excited when the scale of data increases. You are curious about AI and want to contribute to the future of search. This is not an entry-level position.




Responsibilities




Design, build, and maintain data pipelines to support our search engine product
Develop and maintain data warehousing solutions to enable efficient data analysis and reporting
Collaborate with data scientists and software engineers to ensure data quality and consistency across the platform
Troubleshoot and fix issues related to data processing and storage
Continuously evaluate and improve our data infrastructure to ensure scalability and reliability
You are excited by data at scale.




Technically




3+ years of experience working with distributed processing frameworks, such as Databricks/Spark and stream processing, event driven technologies such as Kafka
You have built ETL and ELT pipelines
You have worked with user event data and time series data
You have worked on feature engineering and data enrichment in the context of a large scale consumer product
You have an eye for data privacy, and are aware of best practices around data security and access
You are comfortable working with data of all formats, from relational DBs, to non relational DBs




Culturally




You are a kind, friendly person and strive to create a kind, inclusive work place filled with smiles and laughter
You take and give feedback graciously as part of growing individually and as a team
You will take joy in collaborative brainstorming and proposing novel extensions
You want to play an active role throughout the process of delivering your work to users
You are always willing to learn what you do not know
You are dependable and take pride in your work
You enjoy jumping in to help others with whatever part of the product they are building
You want to be part of defining and building a team around a vision and technology that has a direct path to improving the daily lives of people all over the world




More about You.com




You.com is an equal opportunity employer: your race, color, religion, sex, sexual orientation, gender identity, national origin, or disability status don’t matter. We’re committed to building a diverse, inclusive, and supportive workplace that is effectively distributed around the world.




We're a remote-first company, but work hours are generally within the Pacific Timezone (7 hours behind UTC). We get together in-person regularly as a team, but as long as your able to maintain significant overlap with Pacific hours during the workday we're comfortable hiring in almost any location (location-specific legal requirements permitting).




Benefits




Competitive salary and equity
Great health, dental, and vision insurance for you and your family
401(k) plan
Unlimited time off (4+ weeks encouraged annually)
Generous parental leave
Flexible work hours




Show more "
3568508093,Software Engineer - Early Career,Lockheed Martin,2023-04-17,https://www.linkedin.com/jobs/view/software-engineer-early-career-at-lockheed-martin-3568508093?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=VtInKZU4hBw352kj%2Bbr%2FCA%3D%3D&position=16&pageNum=9&trk=public_jobs_jserp-result_search-card,4484,"Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.




With our employees as our priority, we provide diverse career opportunities designed to propel development and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work.




At Lockheed Martin, we place an emphasis on empowering our employees by fostering innovation, integrity, and exemplifying the epitome of corporate responsibility.




Your Mission is Ours.




Lockheed Martin Space in Herndon, VA is seeking a Full Time Associate Software engineer. In this role, you will collaborate with a team of 3 - 5 other developers to develop mission critical software used around the world and engineer solutions to solve complex customer needs. The successful candidate will have experience and/or knowledge of software development, software design practices, and database structures. Must be a US Citizen; this position will require obtaining a government security clearance. This position is located at a facility that requires special access.




Basic Qualifications




Acceptable degree programs include only science and technology related disciplines such as Mathematics, Computer Science, Systems Engineering, Electrical Engineering, Computer Engineering, Physics, Information Technology, Management Information Systems, Cyber Security or related discipline.




Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information, including US Citizenship. Ability to obtain a TS/SCI Clearance required for this role.




Desired Skills




Proficiency in one of the related: C, C++, C#, Java, XML, Windows, .NET, and UNIX, system software and scripting development, Java programming, web development, System analysis, System design, Software Design, Software Development and Implementation, Software Test or Cyber Security.




Security Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.




Clearance Level: TS/SCI w/Poly




Other Important Information You Should Know




Expression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.




Ability to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.




Work Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.




Schedule for this Position: Standard Monday to Friday 40 hour work week




Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.




Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.




As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.




Experience Level: 4 yr and up College




Business Unit: SPACE




Relocation Available: Possible




Career Area: Software Engineering




Type: Full-Time




Shift: First
Show more "
3568625423,Data Engineer,SmallBoard.com,2023-04-13,https://www.linkedin.com/jobs/view/data-engineer-at-smallboard-com-3568625423?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=Eu%2FPSMnGIIC9yPg6hzCnqA%3D%3D&position=17&pageNum=9&trk=public_jobs_jserp-result_search-card,737,"Role: Database/Data Engineer Location: Hybrid or Remote - Must be located in Colorado, Michigan, Minnesota, New Mexico, North Dakota, South Dakota, Texas or Wisconsin. (MN or CO preferred) Industry: Public Utility

Requirements:

Minimum 10-12 years experience
Data engineer with a skill set of Aurora DB, PostGres DB, ETL, Stored Procs, ideally with newer Informatica cloud data capabilities (versus legacy informatica ETL).
Significant experience in stored procedures with mysql and/or oracle(plsql).
Experience with Gitlab for database code migration
ETL/Data Warehouse design/dev
AWS/RDS analysis
SFDC Dev/Support
AWS/s3/Lambda experience
Agile
Ability to learn the data models


Flexible work from home options available.
Show more "
3576651154,Data engineer,Diverse Lynx,2023-03-25,https://www.linkedin.com/jobs/view/data-engineer-at-diverse-lynx-3576651154?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=NmtMKbRdrlKVcSwx8d%2BL3w%3D%3D&position=18&pageNum=9&trk=public_jobs_jserp-result_search-card,2165,"Job Description




Role Data Engineer Mandatory Technical / Functional Skills Strong analytic skills related to working with structured datasets.




Experience with object-oriented/object function scripting languages: Python, Pyspark




Experience with relational SQL : SQL




Experience with data pipeline and workflow management tool: Airflow




Experience with cloud services & Data Analytics tools : Azure Databricks, Azure DataLake (Exposure in other cloud platform also acceptable).




Experience in supporting and monitoring big data data pipelines.




Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.




Retail domain Experience . Roles and responsibility Support analytics tools that utilize data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.




Support and monitor infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Pyspark and Azure big data technologies.




Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.




Work closely with client to Client needs, collect appropriate data, and deliver valuable data products.




Provide valuable insights from data sets quickly.




Communicate complex findings and ideas in plain language and visualizations that is friendly to business and operational audiences. Desirable Technical / Functional Skills Highly Skilled in Python and PySpark. Excellent communication skill




Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more "
3541509523,Data Engineer (Offering Sponsorship),BroadAxis,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-offering-sponsorship-at-broadaxis-3541509523?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=eXYZSrIq5OtJHZLJTDm6Cg%3D%3D&position=19&pageNum=9&trk=public_jobs_jserp-result_search-card,1391,"Candidates who are looking for Visa Sponsorship/Transfer are eligible to Apply!

Candidates Who Currently Reside In The US Are Eligible.

Job Title: Data Engineer (Sponsorship Available)

Our company is seeking a highly skilled Data Engineer who is looking for sponsorship to work in the United States.

If you are a talented Data Engineer looking for sponsorship to work in the United States, we encourage you to apply for this exciting opportunity. We offer competitive salary and benefits packages, as well as a supportive and collaborative work environment where you can thrive and grow your career.

Requirements

CANDIDATE SKILLS AND QUALIFICATIONS

Professional experience as a data analyst, developer, or data scientist.
Professional experience creating statistical or machine models to execute forecasting based on disparate and unfamiliar datasets.
Coding and release experience using bash, Python, and/or other common languages.
Building dashboard and reporting suites keeps stakeholders and consumers in mind through the design and iteration phases.
Experience working with, analyzing, and testing deeper analytical hypothesis using utility or energy- related data sets
Working within a cloud infrastructure, DevOps experience

Benefits

Bench Marketing Services
Legal & Processing Fees covered
Job Placement (within 2 Weeks)
Competitive Pay rate
Relocation Assistance
Show more "
3557818330,Data Engineer,"APCO Holdings, LLC",2023-04-07,https://www.linkedin.com/jobs/view/data-engineer-at-apco-holdings-llc-3557818330?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=8Jzhh7xQlW6qeL4g7ww%2BdQ%3D%3D&position=20&pageNum=9&trk=public_jobs_jserp-result_search-card,4172,"Overview




Data is transforming the way businesses think, decide and act. APCO values collaboration and the ability to communicate insights that drive decisions. A successful candidate will have a strong desire to make an outsized contribution to organizational transformation. Candidate must reside in GA, FL or PA.




As a Data Engineer, you will be making the firm's data ubiquitous and accessible. You have to be passionate about technology and driving results. Moving data and calling API that validate and cleanse data are important elements of the role.




Essential Duties And Responsibilities




As a professional, you will be responsible for ingesting, cleansing, and representing data for the Analytics team. Duties include:




Analyze business and application needs to assist with determining and evaluating potential solutions.
Help develop specifications and requirements.
Motivated to deliver on committed timeframes and communicate and address issues as they arise.
Ensure that all production systems are supported and maintained according to departmental standards.
Willingness to learn new technologies and concepts.
Enforce change control processes and controls.
Willingness to learn database management tools currently used to support APCO's database environment.
Assist with the implementing database replication strategies, including database mirroring/Always-On, peer-to-peer, merge and transactional replication.




Education And Experience




BS Degree in Computer Science or other related technical discipline.
At least 2 years of experience with RDBMS databases and familiar with database structures and objects.
At least 2 years of Microsoft SQL Server T-SQL experience, preferably on versions 2008 and up.
At least 1 year experience with SSIS packages.
Azure experience is desired.
Replication experience is desired.
Working experience with SSAS and SSRS is strongly desired.




Skills




A data engineer's job is to provide the data used the data architecture and data science teams. The following lists common skills and abilities:




Problem Solving - Ability to work through problems methodically to arrive at optimal solutions. Independent thinking and a comfort with sometimes ambiguous requirements is a must.
Programming Skills - knowledge of scripting languages like JavaScript and Python as well as database query languages like SQL, Hive, and Pig.
Data Cleansing - proficiency in handling imperfections in data such. Collaborate with team regarding when to stage raw data versus methods for handling noisy or inconsistent data.
Tools - ability to recommend the right tools for any given task and guiding organizational thinking on best practices.
Communication - ability to effectively communicate highly complicated, technical, and detailed findings to end users in all areas of our business.




Physical Demands




While performing the duties of this job, the employee is regularly required to type and look at a computer screen for long periods of the day. The employee must be able to sit for long periods of time.




Qualifications




To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations will be made to enable individuals with disabilities to perform the essential functions.




We provide full-time comprehensive benefits packages to all of our employees including: Medical, dental, vision, paid company holidays, paid time off, paid community service day, wellness program, 401K with company match, referral bonuses, discounted gym membership and much more.




APCO Holdings, LLC is a Drug Free Workplace as well as an Equal Opportunity Employer. APCO Holdings, LLC does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.




Job Posted by ApplicantPro
Show more "
3582324999,Data Engineer,hatch I.T.,2023-03-29,https://www.linkedin.com/jobs/view/data-engineer-at-hatch-i-t-3582324999?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=bnthBt02H%2Bw3Ben08tOAuA%3D%3D&position=21&pageNum=9&trk=public_jobs_jserp-result_search-card,3622,"Hatch I.T. is partnering with JDSAT to find a Data Engineer. See details below:

About The Role

Their team is looking for an experienced data engineer to contribute to the design and buildout of a data architecture to support advanced analytics services for federal clients. Applicants should have experience building enterprise-grade data ingestion solutions and, ideally, on cloud platforms like Amazon Web Services

About The Company

JDSAT is a solutions company before anything else, and they believe that data, mathematics, and software are the most effective path to finding a solution that’s tailored to organization’s needs.

Candidates must be able to obtain a secret level clearance

What does a typical day look like?

Assess, analyze, and organize raw data
Prepare data for use in simulation, optimization, or data science tools
Combine information from many disparate sources, conduct data validation/verification, and store the data in a manner that enables database users to intuitively join information across multiple domains
Leverage industry best practices across all relevant levels of the technology stack to include the version control system, ETL processes, and the use of AWS cloud products
Identify opportunities for new data acquisition
Interface with source data in a variety of different forms to include APIs, flat file extracts, direct database querying, etc.
Apply strong attention to detail to identify errors or data quality concerns and communicate with customers in a professional manner

What qualifications are they looking for?

Bachelor’s degree in Analytics, Data Analysis, Statistics, Finance, Economics, Computer Science, or related field with minimum 8 years’ experience working or a combination of postgraduate studies and working in a technical field as described above.
Ability to assess and profile raw data and reassemble raw data from multiple sources into a single, enterprise model
Ability to work in a consulting role, building technology and communicating with end-users and customers of varying levels of technical capability
Ability to produce high-quality, professional documentation and communication materials
Ability to build and deploy team-friendly code in SQL

They're Extra Impressed With Folks Who Have

Master’s degree in Analytics, Data Analysis, Statistics, Finance, Economics, Computer Science, or related field
Ability to build and deploy team-friendly code written in Python and R
Experience with Amazon Web Services (AWS)
Experience leveraging cloud platforms to build secure and scalable data infrastructure
Experience building batch or streaming data ingestion pipelines
Experience working with Medallion Architectures and Master Data Management

$130,000 - $170,000 a year

Don't think you're 100% qualified for this position? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At hatch I.T., we're dedicated to helping companies build diverse, inclusive and authentic workplaces, so if your experience doesn't perfectly align with every qualification in the job description, we encourage you to apply anyway. You may just be the right candidate for this or other roles.

If you are interested in learning more about this company or any Startups/Small Businesses in the area, please contact us and check us out here !!

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more "
3580464662,Data Engineer - ETL,"Buchanan & Edwards, Inc.",2023-03-28,https://www.linkedin.com/jobs/view/data-engineer-etl-at-buchanan-edwards-inc-3580464662?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=UAJX8g5%2BoPP%2FzLW6FDrvYg%3D%3D&position=22&pageNum=9&trk=public_jobs_jserp-result_search-card,4220,"Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.




They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.




Let Buchanan & Edwards help you unleash your potential and reach your goals.




Buchanan & Edwards are seeking a Data Engineer with expertise in ETL in Huntsville, AL.




Hybrid - 50/50




A challenging and dynamic work environment isn’t all that BE has to offer. When you join BE, you become an integral part of our team. Not just an employee number, but a valued opinion with a voice that carries and drives change and innovation. You’ll have access to some of the most well-respected IT industry leaders in virtually every field and learn new tools and techniques while expanding your knowledge and skillset. We’re dedicated to helping you develop your career, and our culture applauds outside-the-box thinking and initiative. Our employee-focused leadership team knows that your success is our success and empowers employees with the necessary tools to excel. Join us as we continue to move the future forward, together.




Top Secret




Experience with ETL processing pipeline using Databricks or equivalent platform using Apache Spark with Scala, Python, Java.
Understanding of programming and data engineering concepts and best practice.
Experience with Python, SQL,and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
High levels of curiosity, creativity, and problem-solving capabilities.
Strong written and verbal communication skills.
Experience with data pipelines or be willing to learn a pipeline from bottom to top
Be able to troubleshoot files against an architecture to see where the upload process is failing.
Be able to understand unit tests and add to them to increase stability to the entire pipeline.
Be prepared to use, GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Be prepared to express ideas and solutions and walk together with teammates through coding challenges
BS Degree or equivalent and 10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
A current Top Secret security clearance with SCI eligibility
Have experience with AWS cloud services including Glue, Kinesis and Container services.
Have knowledge of setting up REST APIs.
Have knowledge of Databricks.
Have experience SQL and its associated tables and queries.
Have worked in an Agile environment.
You have worked on teams that practice Agile methodologies.




Buchanan & Edwards, Inc. (BE) is an Information Technology and Professional Services consulting firm located in Arlington, VA. BE is a diversified high-technology services company, providing government, commercial and nonprofit agencies technology solutions and organizational management services to ensure mission success. Serving the federal sector since 1998, we base our solutions on an in-depth understanding of our clients, their mission and the unique challenges they face. BE is the winner of the 2015 Microsoft U.S. Federal Solution Partner of the Year award, a 2015 and 2016 Washington Post Top Workplace and has been an Inc. 500/5000 awardee for six consecutive years. Buchanan & Edwards, Inc. is dedicated to fostering, cultivating and preserving a culture of diversity and inclusion. We are committed to crafting a workplace that endorses creativity and innovation, and promotes engagement through open communication, acceptance of new people and ideas, and a supportive team dynamic. Buchanan & Edwards, Inc. is an equal Opportunity Employer–Minorities / Women / Veterans / Individuals with Disabilities / Gender Identity / Sexual Orientation. Buchanan & Edwards is an E-Verify employer.




#CJ
Show more "
3571480421,Data Engineer,"Precision Solutions, LLC",2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-llc-3571480421?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=TKWhivAqHMoUI4BB85jyNg%3D%3D&position=23&pageNum=9&trk=public_jobs_jserp-result_search-card,4752,"Overview

Data Engineer

Onsite | Transcom (Illinois) | 5 Days a week Onsite

Interim Secret Clearance Required

Summary

Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients’ success and ensure that their initiatives are properly aligned with their core mission.

Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.

They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.

Responsibilities

Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.

Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools Next
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering

Requirements

Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI)
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills

Preferred Requirements

A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.

Education Requirements

None.

Clearance Requirements

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an Interim Secret clearance is required.

Other Duties

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

About Us

Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.

Equal Opportunity Employer Statement

Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
Show more "
3561260730,Junior Software Engineer,TechTammina LLC,2023-04-11,https://www.linkedin.com/jobs/view/junior-software-engineer-at-techtammina-llc-3561260730?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=wYS4zskVHz%2BXsP78oTilIg%3D%3D&position=24&pageNum=9&trk=public_jobs_jserp-result_search-card,2381,"Role: Junior Software Engineer

Rate: Market

Location: This role be 100% onsite in Irvine, CA

Duration: 3+ months

Hours: 8-hour Day Shift (Flexible start time: 8am 10am)

Job Summary

Assist in the development, testing, release, and integration efforts of software and hardware associated activities.

Major Responsibilities

Hardware and software integration of new and/or existing systems, functions and LRUs designed by the Development Engineering and the Software Development groups.

Integration test of LRU and system functionality.

Ensure all testing is properly documented and submitted.

Responsible for the integration and maintenance of a test rack configuration with regards to the systems installation.

Software and system testing of all In-flight Entertainment (IFE) functionality for existing systems.

Participate in the design and development of all IFE system functionality.

Participate in all efforts in the testing and regression testing of software phase releases.

Assist And Support All Engineering Efforts Company-wide As Required.

Assist in developing test plans and procedures to validate system requirements.

Troubleshoot problems reported for all Panasonic Avionics IFE products.

Focal point for software development, integration and problem investigation for all assigned equipment and systems.

Assist in troubleshooting and analyzing root cause of any system anomalies/discrepancies in hardware and/or software.

Knowledge/Skill Requirements

A technical background in development and test.

Familiar with Unix and Linux OS. Basic understanding of systems administrator capabilities and command language for Linux is preferred.

Ability to plan engineering activities to perform tasks.

Learn to use professional concepts and applies company policies and procedures to resolve routine issues.

Generally, applies existing practices and procedures in analyzing situations or data.

Possess solid writing and communication skills.

Ability to effectively communicate in English, in person and on the phone.

Proficiency with Microsoft Office products.

Education/Experience Requirements

Bachelor of Science Degree in Computer Sciences, Computer Engineering, Electrical Engineering or Software Engineering, or equivalent experience.

0-3 years' experience in software/system engineering development and/or software/hardware testing.
Show more "
3554311995,Data Engineer,CAIS,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-at-cais-3554311995?refId=fQTdi%2Frj7CgcWQWbdmWeLw%3D%3D&trackingId=cpJvT2uJja6NpGcifK%2BCTA%3D%3D&position=25&pageNum=9&trk=public_jobs_jserp-result_search-card,4176,"CAIS is a fintech firm on a mission to build the first truly open marketplace for alternative investments, where financial advisors and asset managers can engage and transact directly on a massive scale.  We are a rapidly growing, high-energy, and collaborative group looking for people with great attitudes, grit, and creative problem-solving skills. Join us in disrupting a multi-trillion-dollar industry.  

We are seeking a full time Data Engineer to join our Technology team in our New York City office. This role will report to our SVP of Data and will be responsible for developing and maintaining the infrastructure that makes data a central part of CAIS. The Data Engineer will collaborate with analytics and product teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the firm. In addition, this role will work closely with our Product Engineering and Operations Engineering teams to provide data support on greenfield work. The ideal candidate is a strong cross-functional communicator with a proven ability to solve open-ended problems in an elegant, scalable manner. They possess creativity, flexibility, and the drive to consistently deliver on mission critical projects with tight timelines and competing priorities. 

Responsibilities 

Develop and automate secure, robust, and high-performing data platform infrastructure to drive CAIS business growth
Process data through to Snowflake using SQL and code-based ELT (and/or ETL)
Analyze production workloads and develop data workflow strategies to optimize Snowflake warehouse with scale and efficiency 
Support BAU operations on Snowflake, including access management, audits, and other administrative activities 
Design data models for optimal storage and retrieval that represent the product entities and meet business requirements 
Leverage open-source technologies and cloud solutions to build sophisticated features that help support business analytics 
Build and support pipeline orchestration via Airflow
Data Architecture (Snowflake preferred)


What We're Looking For 

Engineering, Mathematics, or other technical degree(s)   
Understanding of AWS and how to create secure infrastructure and data pipelines 
Experience with AWS solutions such as Lambda, S3, and Kafka
Experience with Athena
Experience with DBT
Experience with Snowflake database
Experience with Integration tools like Fivetran/Airbyte
5+ years of experience with SQL
Strong understanding of a coding language to extract and transform data such as Python/Java  
5+ years of data warehousing experience with examples of complex ETL/ELT data workflows 
5+ years of experience with schema design and dimensional data modeling 
5+ years of experience with object-oriented/functional programming, strong ability to write easy-to-scale, high-quality code
Experience with or knowledge of Agile Software Development methodologies 


Desirable Skills/Knowledge 

Prior experience working within Alternative Investments and/or Financial Services preferred
Good knowledge of financial markets and financial instruments a plus 
Experience with GitHub Actions a plus
Experience in developing, maintaining, and managing Tableau (or similar Business Intelligence tool) a plus


CAIS' compensation package includes a market competitive salary, a performance bonus, and exceptional benefits. If you are located in New York, New York, the base salary range for this role is $120,000 - $200,000. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location.

CAIS offers a comprehensive benefits package that includes generously subsidized healthcare with 100% employer paid dental and vision insurance, an employer matched retirement plan, wellness programs, and generous PTO and parental leave. Additionally, CAIS offers a flexible, hybrid in-office model; for most roles, we do not require a minimum number of days in office per week. For more information on our benefits and career opportunities, please visit our website: https://www.caisgroup.com/careers
Show more "
3554311995,Data Engineer,CAIS,2023-04-04,https://www.linkedin.com/jobs/view/data-engineer-at-cais-3554311995?refId=i3ox6OLFALpgoo%2F4OipFqw%3D%3D&trackingId=P3ZKaON2YOy8hOdiVMbx8g%3D%3D&position=1&pageNum=10&trk=public_jobs_jserp-result_search-card,4176,"CAIS is a fintech firm on a mission to build the first truly open marketplace for alternative investments, where financial advisors and asset managers can engage and transact directly on a massive scale.  We are a rapidly growing, high-energy, and collaborative group looking for people with great attitudes, grit, and creative problem-solving skills. Join us in disrupting a multi-trillion-dollar industry.  

We are seeking a full time Data Engineer to join our Technology team in our New York City office. This role will report to our SVP of Data and will be responsible for developing and maintaining the infrastructure that makes data a central part of CAIS. The Data Engineer will collaborate with analytics and product teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the firm. In addition, this role will work closely with our Product Engineering and Operations Engineering teams to provide data support on greenfield work. The ideal candidate is a strong cross-functional communicator with a proven ability to solve open-ended problems in an elegant, scalable manner. They possess creativity, flexibility, and the drive to consistently deliver on mission critical projects with tight timelines and competing priorities. 

Responsibilities 

Develop and automate secure, robust, and high-performing data platform infrastructure to drive CAIS business growth
Process data through to Snowflake using SQL and code-based ELT (and/or ETL)
Analyze production workloads and develop data workflow strategies to optimize Snowflake warehouse with scale and efficiency 
Support BAU operations on Snowflake, including access management, audits, and other administrative activities 
Design data models for optimal storage and retrieval that represent the product entities and meet business requirements 
Leverage open-source technologies and cloud solutions to build sophisticated features that help support business analytics 
Build and support pipeline orchestration via Airflow
Data Architecture (Snowflake preferred)


What We're Looking For 

Engineering, Mathematics, or other technical degree(s)   
Understanding of AWS and how to create secure infrastructure and data pipelines 
Experience with AWS solutions such as Lambda, S3, and Kafka
Experience with Athena
Experience with DBT
Experience with Snowflake database
Experience with Integration tools like Fivetran/Airbyte
5+ years of experience with SQL
Strong understanding of a coding language to extract and transform data such as Python/Java  
5+ years of data warehousing experience with examples of complex ETL/ELT data workflows 
5+ years of experience with schema design and dimensional data modeling 
5+ years of experience with object-oriented/functional programming, strong ability to write easy-to-scale, high-quality code
Experience with or knowledge of Agile Software Development methodologies 


Desirable Skills/Knowledge 

Prior experience working within Alternative Investments and/or Financial Services preferred
Good knowledge of financial markets and financial instruments a plus 
Experience with GitHub Actions a plus
Experience in developing, maintaining, and managing Tableau (or similar Business Intelligence tool) a plus


CAIS' compensation package includes a market competitive salary, a performance bonus, and exceptional benefits. If you are located in New York, New York, the base salary range for this role is $120,000 - $200,000. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location.

CAIS offers a comprehensive benefits package that includes generously subsidized healthcare with 100% employer paid dental and vision insurance, an employer matched retirement plan, wellness programs, and generous PTO and parental leave. Additionally, CAIS offers a flexible, hybrid in-office model; for most roles, we do not require a minimum number of days in office per week. For more information on our benefits and career opportunities, please visit our website: https://www.caisgroup.com/careers
Show more "
3572715231,Data Engineer,"Precision Solutions, LLC",2023-04-19,https://www.linkedin.com/jobs/view/data-engineer-at-precision-solutions-llc-3572715231?refId=i3ox6OLFALpgoo%2F4OipFqw%3D%3D&trackingId=IfcdMXEzksRkLhLGLXq%2FLQ%3D%3D&position=2&pageNum=10&trk=public_jobs_jserp-result_search-card,4742,"Overview

Data Engineer

Hybrid | Crystal City, VA | 2-3 Days a week Onsite

Secret Clearance Required

Summary

Our client provides leading edge cybersecurity services that improve security, promote innovation, and transform cybersecurity for government and commercial customers. Our client is on the front lines of the cyber landscape, specializing in unique cybersecurity solutions, infrastructure support, and end user support. They also remain committed to their clients’ success and ensure that their initiatives are properly aligned with their core mission.

Our client works closely with their partners and clients to understand their needs and ensure their short and long term targets are met or exceeded. Their expert team works to design and execute progressive, innovative solutions that are helping our clients modernize their infrastructure and stay ahead of the constantly evolving thread landscape.

They also believe in hiring smart people then giving them space to thrive. Staying ahead of the pack requires not just economic vigilance but ambitious business goals and a purposeful, cohesive workforce. Our client is able to attract and retain their professionals by consistently creating an environment based on trust, fairness and opportunity. Additionally, they believe in establishing open communication that encourages achievable performance expectations. With also incorporating collaboration within their organization, it creates a positive energy and true ownership in providing services that are essential to deliver results of the highest quality.

Responsibilities

Our client is seeking a Data Engineer to join their team! The ideal candidate will be responsible for designing, developing, and maintaining data pipelines, data warehouses, and data models to support the company's data-driven initiatives.

Design, develop, and maintain data pipelines, data warehouses, and data models
Implement and maintain data integration processes to collect, process, and manage large datasets
Collaborate with data analysts, data scientists, and other stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Troubleshoot and resolve data-related issues and incidents
Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to the data architecture and infrastructure
Participate in code reviews and contribute to the development of best practices and standards for data engineering

Requirements

Strong proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)
Experience with data modeling, data warehousing, and ETL processes
Proficiency in at least one programming language (e.g., Python, Java, Scala)
Experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Hadoop, Spark, Kafka, Cassandra)
Knowledge of data visualization and BI tools (e.g., Tableau, Power BI) Next
Excellent problem-solving skills and attention to detail
Strong verbal and written communication skills

Preferred Requirements

A Bachelor's degree in Computer Science, Information Systems, or a related field is preferred.

Education Requirements

None.

Clearance Requirements

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active Secret clearance is required.

Other Duties

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

About Us

Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.

Equal Opportunity Employer Statement

Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
Show more "
3580146232,Big Data Developer (Data Engineer),Veridic Solutions,2023-03-30,https://www.linkedin.com/jobs/view/big-data-developer-data-engineer-at-veridic-solutions-3580146232?refId=i3ox6OLFALpgoo%2F4OipFqw%3D%3D&trackingId=LY71exIP3rQYwC%2BwUXsAEA%3D%3D&position=3&pageNum=10&trk=public_jobs_jserp-result_search-card,2279,"Role:- Developer- Big Data (Data Engineer)




Location: Raleigh, North Carolina (Onsite)




we are looking for a data engineer who will help build new or improve existing data pipelines. You should be comfortable working with large or fast moving data, have a solid understanding of distributed processing frameworks, and a software engineering mindset




Requirements




Over all 4 to 10 years of IT experience. Extensive experience in Big Data, Analytics, ETL technologies
Application Development background along with knowledge of Analytics libraries, statistical and big data computing libraries
Minimum 3+ years of experience in Spark/PySpark, Python/Scala/java programming.
Hands on experience in coding, designing and development of complex data pipelines using big data technologies
Experience in developing applications on Big Data. Design and build highly scalable data pipelines
Expertise in Python, SQL Database, Spark, non-relational databases
Responsible to ingest data from files, streams and databases. Process the data using Spark, Python
Develop programs in PySpark and Python as part of data cleaning and processing
Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems
Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform
Provide high operational excellence guaranteeing high availability and platform stability
Implement scalable solutions to meet the ever-increasing data volumes, using big data/Palantir technologies Pyspark, any Cloud computing etc.
Individual who can work under their own direction towards agreed targets/goals and with creative approach to work
Intuitive individual with an ability to manage change and proven time management
Proven interpersonal skills while contributing to team effort by accomplishing related results as needed




Technologies require : DB : Hive , Impala , HBASE. Data Processing : Spark core and SQL. build tool : Maven , Testing framework : Cucumber




Additional Skills




Experience in building CI/CD Pipelines, Git, Jenkins
Have worked with large datasets
Proficient reading and understanding enterprise-grade PySpark OR Spark with Scala code
Show more "
