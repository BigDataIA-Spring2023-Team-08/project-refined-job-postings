Job ID,Title,CleanedDescription
3585966085,Analytics Engineer,"we strongly encourage people of colour, lesbian, gay, bisexual, transgender, queer and non-binary people, veterans, parents, and individuals with disabilities to apply. bumble is an equal opportunity employer and welcomes everyone to our team. if you need reasonable adjustments at any point in the application or interview process, please let us know.
in your application, please feel free to note which pronouns you use (for example - she/her/hers, he/him/his, they/them/theirs, etc).
analytics engineer
key accountabilities
support the building of robust data models most in snowflake that support our various domain teams such as community operations, product, and marketing
focus on optimisation of our data warehouse in conjunction with our reporting platform and other sources of cloud cost consumption
set standards and ways of working with data across bumble, working collaboratively with other team to make this happen
investigate and work with colleagues from other disciplines to monitor and improve data quality within the warehouse
experience we are looking for
you have experience and a passion for data modelling, etl projects
you have very strong sql skills
you are comfortable with general data warehousing concepts
you have proven experience in terms of working with cloud data platform at previous companies
you have python experience
you have previously used dbt, data form, or similar tooling
desired skills and knowledge
used agile ways of working (kanban, scrum)
you have requirement gathering experience and know how to use white boarding techniques to map business processes
you have experience with bi tools / platforms such as microstrategy, tableau, or looker
any experience working within a previous (consumer) tech company
experience working with orchestration frameworks such as airflow
"
3587236471,Data Engineer,"about bobbie
bobbie is creating a parenting culture of confidence, not comparison. and it starts with how we choose to feed our babies. we crafted our european style infant formula with purposefully sourced, organic ingredients to give parents a product they are proud to feed their babies, direct to their doorstep. bobbie is proud to be the only us formula that is designed to meet both fda and eu standards.
although 83% of parents turn to formula in the first year of their baby's life, this is the silent majority that is often shamed for not being able to exclusively breastfeed. co-founded by two moms and created by a team of mom scientists, nutritionists, pediatricians, and lactation consultants, the bobbie team knows first hand that there is no one size fits all for feeding. with bobbie, we hope you can bottle boldly.
the role
the data engineer is responsible for designing, building, and maintaining the data infrastructure that supports the company's data-driven initiatives. this includes developing and managing data pipelines, data warehouses, and data lakes; building and maintaining data processing and analytics tools; and ensuring the quality and security of the company's data. not only will you be working closely with the data team but you will be collaborating with marketing, sales, logistics, finance and customer experience.
what you will accomplish:
design, build, and maintain the company's data infrastructure
develop and manage data pipelines, data warehouses, and data lakes
ensure the quality and security of the company's data
work with cross-functional teams to understand and meet their data needs
stay up-to-date on the latest data technologies and trends
what we would like you to have
must haves:
advanced knowledge in python, sql and githhub
google cloud and tools or equivalent; cloud functions, dataflow, big query, cloud scheduler, google buckets, pub/sub, datastream
elt/etl processes, including but not limited to: airflow, apache beam, dbt
some prior working experience as a data engineer
nice to have:
2+ years of experience in a similar role
understanding of containerization technologies such as docker and kubernetes to package, deploy and manage applications.
knowledge of ci/cd tools and processes, including experience in any of the tools like jenkins, gitlab ci/cd, and circleci.
you're inspired by our core values:
be radical - we are change makers, rule breakers and stigma shakers. we are unapologetically bold and use our megaphone for good.
nurture the tension - parenthood is full of healthy tension, and so is building a company. we embrace the unknowns, practice humility and are a culture of learners.
deliver ounce by ounce - our customers have entrusted us to feed their babies. we lose sleep over the details, so they don't have to.
don't assume - we embrace our unique perspectives, withhold judgment, and find beauty in the paths that brought us all to bobbie. our strength is celebrating each other and our collective voice.
compensation and benefits:
salary
san francisco / nyc: $132,000 - $154,000
all other locations: $110,000 - $132,000
our salary ranges are based on paying competitively for our size and industry. as a fully remote company, we adjust our salaries to match geographic location. employees who live in a high cost-of-living city (san francisco, nyc) have a higher salary range than employees who live elsewhere.
our total compensation package also includes comprehensive benefits and robust equity offerings. we believe all bobbie employees should have an option to purchase ownership in the company and benefit from what we expect will be a lot of upside growth.
our benefits include
us-based work from anywhere model
flexible time off policy
summer fridays (early office closure every friday)
13 paid company holidays plus an end of year holiday office shut down
16 weeks of paid parental leave with the option to take an additional 8 months unpaid
1 year bobbie subscription
balance coordinator
$75 monthly internet stipend
co-working space reimbursement
bobbie personnel privacy policy and notice at collection
at bobbie, we are committed to building a diverse and inclusive company. we seek to create a culture where everyone can belong because we believe that people do their best work when they can show up every day as their authentic selves. we welcome people of different backgrounds, experiences, abilities, and perspectives.
bobbie is an equal opportunity employer. we do not make hiring or employment decisions on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, disability status or genetic information, in compliance with applicable federal, state and local law.
"
3587253770,Data Engineer,"about us
at exxonmobil, our vision is to lead in energy innovations that advance modern living and a net-zero future. as one of the world’s largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.
the success of our upstream, product solutions and low carbon solutions businesses is the result of the talent, curiosity and drive of our people. they bring solutions every day to optimize our strategy in energy, chemicals, lubricants and lower-emissions technologies.
about houston
by bringing many global functional groups together, the campus provides employees with the tools and capabilities needed today, and in the future, to achieve business objectives and accelerate the discovery of new resources, technologies and products. it was designed to foster improved collaboration, creativity and innovation and enhance the company’s ability to attract, develop and retain the top talent in the industry.
the campus is located in spring, texas, on 385 wooded acres immediately to the west of interstate highway 45 (i-45), at the intersection of i-45 and the hardy toll road, approximately 25 miles from the cultural vibrancy of downtown houston.
the campus was constructed to the highest standards of energy efficiency and environmental stewardship. its design incorporates extensive research into best practices in building and workplace design through extensive benchmarking of the world’s top academic, research, and corporate facilities.
what role you will play in our team
as a senior data engineer, working alongside other campus’-based data engineers, you develop and maintain essential data pipelines that ingest, process, and deliver technical data for advanced analytics, visualizations, and application integration.
responsible for streamlining data processing, data wrangling, dimensional modeling and automating data pipelines for big data.
this position will be based at exxonmobil campus in houston, texas.
what you will do
perform etl, elt operations and administration using modern tools, programming languages and systems securely and in accordance with enterprise data standards
assemble, model, transform large complex sets of data that meet non-functional and functional business requirements into a format that can be analyzed
automate data processing of data from multiple data sources
develop, deploy and version control code for data consumption, reuse for apis
employ machine learning techniques to create and sustain data structures
perform root cause analysis on external and internal processes and data to identify opportunity for improvement, resolve data quality issues
lead data-related workshops with stakeholders to capture data requirements and acceptance criteria
about you
skills and qualifications
we are looking for someone who has technical depth and broad tool experience related to data engineering and has the following qualifications:
minimum bachelor’s degree in: data science, business intelligence, statistics, computer engineering or related field, or the equivalent combination of education, professional training, and work experience
minimum 8 years it/data professional experience with at least four years performing duties related to data engineering
english – professional proficiency
expert proficiency in at least one of these programming languages: python, nosql, sql, r, c++, and competent in source code management
build processes supporting data transformation, data structures, metadata, dependency, and workload management
create data validation methods and data analysis tools
preferred knowledge / skills / experience
excellent problem-solving skills and ability to learn through scattered resources
expert proficiency with data warehouses, data pipeline technologies (., snowflake, databricks, airflow)
automate routine tasks via scripts, code
capacity to successfully manage a pipeline of duties with minimal supervision and using enterprise project management methodology (. agile)
experience supporting and working with cross-functional teams in a dynamic environment
modify existing reports, extracts, dashboards, and cubes as necessary
commitment to operations integrity and ability to hold self and others accountable for results
your benefits
an exxonmobil career is one designed to last. our commitment to you runs deep: our employees grow personally and professionally, with benefits built on our core categories of health, security, finance, and life.
we offer you:
pension plan: enrollment is automatic and at no cost to you. the basic benefit is a monthly annuity to be paid to you in retirement for the rest of your life
savings plan: you can contribute between 6% and 20% of your pay and are encouraged to enroll right away. if you contribute at least 6% to your savings plan, the company will contribute a 7% match.
workplace flexibility: we have several programs such as “flex your day”, providing ad-hoc flexibility around when and where you work, as well as longer-term programs such as leaves of absence and part-time work
comprehensive medical, dental, and vision plans
culture of health: programs and resources to support your wellbeing
employee health advisory program: provides confidential professional counseling for you and your family, including tools and resources promoting mental health and resiliency at no additional cost to you
disability plan: income replacement for when you cannot work due to illness or injury occurring on or off the job. enrollment is automatic and at no cost to you
please note benefits may be changed from time to time without notice, subject to applicable law
stay connected with us
follow us on linkedin and instagram
like us on facebook
subscribe our channel at youtube
nothing herein is intended to override the corporate separateness of local entities. working relationships discussed herein do not necessarily represent a reporting connection, but may reflect a functional guidance, stewardship, or service relationship.
exxon mobil corporation has numerous affiliates, many with names that include exxonmobil, exxon, esso and mobil. for convenience and simplicity, those terms and terms like corporation, company, our, we and its are sometimes used as abbreviated references to specific affiliates or affiliate groups. abbreviated references describing global or regional operational organizations and global or regional business lines are also sometimes used for convenience and simplicity. similarly, exxonmobil has business relationships with thousands of customers, suppliers, governments, and others. for convenience and simplicity, words like venture, joint venture, partnership, co-venturer, and partner are used to indicate business relationships involving common activities and interests, and those words may not indicate precise legal relationships.
"
3584991452,Data Engineer,"job description
assists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs
applies understanding of key business drivers to accomplish own work
uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems
leads portions of initiatives of limited scope, with guidance and direction
writes etl (extract / transform / load) processes, designs database systems and develops tools for real-time and offline analytic processing
collaborates with client team to transform data and integrate algorithms and models into automated processes
uses knowledge in hadoop architecture, hdfs commands and experience designing & optimizing queries to build data pipelines
uses programming skills in python, java or any of the major languages to build robust data pipelines and dynamic systems
builds data marts and data models to support clients and other internal customers
integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards
pay range
the typical pay range for this role is:
minimum: $ 70,000
maximum: $ 140,000
please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. the actual salary offer will take into account a wide range of factors, including location.
required qualifications
1+ years of progressively complex related experience
experience with bash shell scripts, unix utilities & unix commands
preferred qualifications
ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources
ability to understand complex systems and solve challenging analytical problems
strong problem-solving skills and critical thinking ability
strong collaboration and communication skills within and across teams
knowledge in java, python, hive, cassandra, pig, mysql or nosql or similar
knowledge in hadoop architecture, hdfs commands and experience designing & optimizing queries against data in the hdfs environment
experience building data transformation and processing solutions
has strong knowledge of large-scale search applications and building high volume data pipelines
education
bachelor's degree or equivalent work experience in computer science, engineering, machine learning, or related discipline
master’s degree or phd preferred
business overview
"
3590397227,Big Data Engineer,"position summary
making the future is everyday life at samsung. we’re seeking innovators who are called to not just change the world, but build a better one. we enable the best technology hardware on the planet, but our best is always a prototype for something better and our people thrive with a driven mindset – better builds on better. we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. we aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. together, we are building a better tomorrow for our customers, partners and communities.
role and responsibilities
data engineers form an important part of the data analytics team within samsung care organization. they have experience with etl design, coding, and testing as well as engineering software platforms and large-scale data infrastructures. big data engineers should have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms.
big data engineers understand how to apply technologies to solve big data problems on cloud and on premise systems with expert knowledge in programming languages like python, linux shell scripts, sql and technologies like hive, impala, presto and spark.
big data engineers implement complex big data projects with a focus on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into actionable deliverables across customer-facing platforms. they have a strong aptitude to decide on the needed hardware and software design and can guide the development of such designs through both proof of concepts and complete implementations.
additional qualifications should include:
experience interacting with data analysts and stakeholders to understand the project scope and requirements
tune big data solutions to improve performance and end-user experience
proficient in designing efficient and robust data workflows
documenting requirements as well as resolve conflicts or ambiguities
experience working in teams and collaborate with others to clarify requirements
excellent oral and written communication skills.
experience in creating architecture for the etl processes.
responsibilities and duties:
understand and document scope and requirements through interactions with analysts and stakeholders
translate complex functional and technical requirements into detailed design
design for now and future success
design and develop scalable industry standard etl pipelines using big data technologies.
loading from disparate data sets. by leveraging various big data technologies emr, hive, spark. presto etc.
code and query optimization.
design and implement data modeling
high-speed querying using in-memory technologies such as spark.
following and contributing best engineering practice for source control, release management, deployment etc.
production support, job scheduling/monitoring, etl data quality, data freshness reporting
background/experience required:
bachelors or masters in computer engineering.
4 years of experience in the big data solutions such as pyspark, python, hive and experience on aws.
necessary skills and attributes:
4+ years of experience writing complex sql queries (hive ql/spark sql preferred).
3+ years of python development experience
3+ years of demonstrated technical proficiency with hadoop and big data projects
3+ years of hands on development experience on developing etl pipelines
fluent in writing shell scripts [bash]
experience working with aws components [emr, s3]
working experience in apache airflow
writing high-performance, reliable and maintainable code.
knowledge and ability to implement workflow/schedulers within oozie
knowledge of data modelling concepts
analytical and problem-solving skills, applied to big data domain
proven understanding and hands on experience with hadoop, hive, pig, presto, and spark
good aptitude in multi-threading and concurrency concepts.
. or . in computer science or engineering
demonstrated ability to interact at all levels within customer's organizations.   the ability to negotiate with and convince others, in a potentially adversarial environment, including customer leadership, directors and managers with opposing views to accept/approve plans, technical and project recommendations. the ability to plan, organize and prioritize multiple strategic programs and simultaneous performance objectives. the ability to write, read, interpret, explain and act based on a thorough understanding of technical documents, engineering materials and contracts or related documents.  ability to make professional sales and business presentations in writing, through email, reports, or orally, including complex business and technical matters to an audience of high technical skills, management and operational experience. ability to support a cross-functional samsung team to achieve customer contracted objectives and specific team goals within established time frames and requirements.  assist in and if needed direct the samsung team in lab and field trials related to introduction of products sold to the customer.
physical/mental demands:
skills and qualifications
background/experience required:
bachelors or masters in computer engineering.
4 years of experience in the big data solutions such as pyspark,python, hive and experience on aws.
necessary skills and attributes:
3+ years of experience writing complex sql queries (hive ql/spark sql preferred).
3+ years of python development experience
3+ years of demonstrated technical proficiency with hadoop and big data projects
2+ years of hands on development experience on developing etl pipelines
fluent in writing shell scripts [bash]
experience working with aws components [emr, s3]
working experience in apache airflow
writing high-performance, reliable and maintainable code.
knowledge and ability to implement workflow/schedulers within oozie
knowledge of data modelling concepts
analytical and problem-solving skills, applied to big data domain
proven understanding and hands on experience with hadoop, hive, pig, impala, and spark
good aptitude in multi-threading and concurrency concepts.
. or . in computer science or engineering
* please visit samsung membership to see privacy policy, which defaults according to your location. you can change country/language at the bottom of the page. if you are european economic resident, please click here.
at samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. we aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. together, we are building a better tomorrow for our customers, partners, and communities.
* samsung electronics america, inc. and its subsidiaries are committed to employing a diverse workforce, and  provide equal employment opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law.
covid-19 vaccine mandate
reasonable accommodations for qualified individuals with disabilities during the application process
samsung electronics america is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. if you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our reasonable accommodation team (855-557-3247) or sea_accommodations_ assistance. this number is for accommodation requests only and is not intended for general employment inquiries.
"
3589773977,Data Engineer,"overview
job description
we are authentic, professional providers of fun, focused on building a great place to work for all by staying true to our mission: 'life's a party, we're makin' it fun!' and 'so much fun it's scary!'
at spencer's and spirit halloween, we do the right thing always-integrity, fairness, respect, and transparency are our foundation. you will find our culture to be inclusive, passionate, resilient, and one that values differences and embraces all.
one team / one goal
we are leaders and owners of our business success. whether it's developing new and exclusive costumes, quality testing products, or implementing technology solutions, our teams understand the value of working collaboratively to embrace change through innovation, curiosity, and thoughtfulness.
we offer a comprehensive benefits package that includes
flexible work environment
career advancement
competitive base salary
bonus opportunity
vacation, personal, sick and holiday pay
medical, dental, vision, disability, life and ad&d insurance
30% merchandise discount
responsibilities
data engineer to optimize our data integration at scale. inside our data integration team, you will be designing pipelines and warehouses to model data from multiple sources that will allow us to derive business insight. using azure and other open-source technologies, such as azure data factory and pyspark, you will design and build our next-generation etl pipelines and data models.
build data pipelines and python-based etl tools for acquiring, processing, and delivering data
develop data models and schemas in our data warehouse that enable performant, intuitive analysis
handle the challenges that come with managing terabytes of data
collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
develop the server applications and apis that are used by our data team
qualifications
bachelor's degree or higher in computer science, computer engineering, information technology or related field
fluent in several programming languages such as python, r, or scala
6+ years of work experience in building etl pipelines in production data processing and analysis
experience designing sql tables, choosing indexes, tuning queries, and optimizations across different functional environments.
hands-on experience writing complex sql queries and using a bi tool
experience with data lakes and designing and maintaining data solutions using spark and azure serverless services such as adf
familiarity with data ingestion apis, data sharing technologies, and warehouse infrastructure and development
"
3589722677,Data Engineer,"software guidance & assistance, inc., (sga), is searching for a data engineer for a contract assignment with one of our premier saas clients in san jose, ca or austin, tx.
client is looking for a data engineer to build the foundational data sets and analytical reports for the b2b customer journey analytics. the data sets and reports will provide quantitative and qualitative analysis for acquisition, engagement, and retention. in addition, the candidate will contribute to cross-functional projects to help business achieve its potential in terms of revenue, customer success, and operational excellence. for this role, we are looking for an ambitious and driven individual - an analytical with outstanding communication skills, high initiative, and strong leadership potential
responsibilities
design and develop new systems and tools to enable folks to consume and understand data faster, along with maintain and enhancing existing ones
analyze the business needs, profile large data sets, design, develop and tune data products on big data platforms (hadoop, databricks) with emphasis on data quality and performance
design, build and launch extremely efficient & reliable data pipelines to move data into and out of the data warehouse
build foundational data sets and reporting solutions and make them available for consumption through dash boards and ad hoc reports
develop and extend data models, processes, standards, frameworks, and reusable components for various data engineering functions/areas.
transform and migrate on-prem hosted data applications to cloud platform
collaborate with key partners including product marketing teams, sales and support teams, engineering leads, architects, bsa's & program managers
create dashboards in tableau and excel modelling in excel.
write complex sql queries that help client identify and measure the non-genuine software base.
apply business logic to determine categorize the non-genuine base into opportunities
use big data best practices to help scale the piracy conversion program
create dashboards in tableau
apply statistical methodologies and data mining skill sets on large volume of data
analyze data and provide insights to senior executives.
required
required skills
ba/bs in computer science, engineering, mathematics, or other technical fields.
experience with databricks, hadoop, hive, hdfs, and big data technologies
experience with databricks data science workspace, presto, tidal and oozie workflows
highly proficient in writing apache spark and python queries
good knowledge writing udf's in hive and spark
advanced working sql knowledge and experience working with cloud columnar databases,
acid transactions, spark mlib, machine learning for tabular data
excellent knowledge about etl processes like sqoop, snap logic and alteryx
familiarity with cloud environments (aws, azure)
sound knowledge of unix and shell scripting
proficient in excel, power bi and tableau
familiarity with processes supporting data transformation, data structures, metadata, dependency and workload management
knowledge about the best engineering practices such as jira, version control system, ci/cd, code review, pair programming
ability to quickly learn about client's business processes, products, data platforms and various tools
ability to synthesize information across a wide variety of data sources including sales force,
client's analytics, campaign systems, client's internal business systems, sap, salesforce, microsoft dynamics etc.
able to communicate effectively with business stake holders, engineers, and users
sql expertise.
hadoop expertise.
can work in hive, pig etc
power pivot expertise a must
understanding of statistics concepts
ability to interpret data and present
excellent written and verbal communication skills
preferred skills
preferred skills
masters is preferred
"
3590355475,Data Engineer,"overview
job description
we are authentic, professional providers of fun, focused on building a great place to work for all by staying true to our mission: 'life's a party, we're makin' it fun!' and 'so much fun it's scary!'
at spencer's and spirit halloween, we do the right thing always-integrity, fairness, respect, and transparency are our foundation. you will find our culture to be inclusive, passionate, resilient, and one that values differences and embraces all.
one team / one goal
we are leaders and owners of our business success. whether it's developing new and exclusive costumes, quality testing products, or implementing technology solutions, our teams understand the value of working collaboratively to embrace change through innovation, curiosity, and thoughtfulness.
we offer a comprehensive benefits package that includes
flexible work environment
career advancement
competitive base salary
bonus opportunity
vacation, personal, sick and holiday pay
medical, dental, vision, disability, life and ad&d insurance
30% merchandise discount
responsibilities
develop data pipeline to provide actionable insights into business acquisition, operational efficiency, and other key business performance metrics.
work with stakeholder's product, data, and design teams to assist with data-related technical issues and support bi and data teams operational requirements.
assist in creating data visualization for analytics and data scientist team members.
work with data and analytics experts to strive for greater functionality in our data systems.
analyze and understand data footprints of our organization.
assist development and architecture team on data quality and various poc initiatives.
qualifications
graduate degree in computer science, statistics, informatics, information systems, or another quantitative field.
at least a year experience working with enterprise technlogy.
working sql knowledge and experience with relational databases, query authoring (sql).
working knowledge building and optimizing \u2018big data' data pipelines, and data sets.
working knowledge performing root cause analysis on internal and external data processing related issues.
understanding processes supporting data transformation, data structures, metadata, dependency, and workload management.
at least completed one project that involves data extraction and transformation pipelines with large data sets.
knowledge with big data tools: hadoop, spark, etc. knowledge of relational sql and nosql databases.
familiar with azure/aws cloud services.
knowledge of object-oriented/object function scripting languages: python, java, etc.
good communication skills, and team player.
"
3590394666,Big Data Engineer,"position summary
making the future is everyday life at samsung. we’re seeking innovators who are called to not just change the world, but build a better one. we enable the best technology hardware on the planet, but our best is always a prototype for something better and our people thrive with a driven mindset – better builds on better. we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. we aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. together, we are building a better tomorrow for our customers, partners and communities.
role and responsibilities
data engineers form an important part of the data analytics team within samsung care organization. they have experience with etl design, coding, and testing as well as engineering software platforms and large-scale data infrastructures. big data engineers should have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms.
big data engineers understand how to apply technologies to solve big data problems on cloud and on premise systems with expert knowledge in programming languages like python, linux shell scripts, sql and technologies like hive, impala, presto and spark.
big data engineers implement complex big data projects with a focus on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into actionable deliverables across customer-facing platforms. they have a strong aptitude to decide on the needed hardware and software design and can guide the development of such designs through both proof of concepts and complete implementations.
additional qualifications should include:
experience interacting with data analysts and stakeholders to understand the project scope and requirements
tune big data solutions to improve performance and end-user experience
proficient in designing efficient and robust data workflows
documenting requirements as well as resolve conflicts or ambiguities
experience working in teams and collaborate with others to clarify requirements
excellent oral and written communication skills.
experience in creating architecture for the etl processes.
responsibilities and duties:
understand and document scope and requirements through interactions with analysts and stakeholders
translate complex functional and technical requirements into detailed design
design for now and future success
design and develop scalable industry standard etl pipelines using big data technologies.
loading from disparate data sets. by leveraging various big data technologies emr, hive, spark. presto etc.
code and query optimization.
design and implement data modeling
high-speed querying using in-memory technologies such as spark.
following and contributing best engineering practice for source control, release management, deployment etc.
production support, job scheduling/monitoring, etl data quality, data freshness reporting
background/experience required:
bachelors or masters in computer engineering.
4 years of experience in the big data solutions such as pyspark, python, hive and experience on aws.
necessary skills and attributes:
4+ years of experience writing complex sql queries (hive ql/spark sql preferred).
3+ years of python development experience
3+ years of demonstrated technical proficiency with hadoop and big data projects
3+ years of hands on development experience on developing etl pipelines
fluent in writing shell scripts [bash]
experience working with aws components [emr, s3]
working experience in apache airflow
writing high-performance, reliable and maintainable code.
knowledge and ability to implement workflow/schedulers within oozie
knowledge of data modelling concepts
analytical and problem-solving skills, applied to big data domain
proven understanding and hands on experience with hadoop, hive, pig, presto, and spark
good aptitude in multi-threading and concurrency concepts.
. or . in computer science or engineering
demonstrated ability to interact at all levels within customer's organizations.   the ability to negotiate with and convince others, in a potentially adversarial environment, including customer leadership, directors and managers with opposing views to accept/approve plans, technical and project recommendations. the ability to plan, organize and prioritize multiple strategic programs and simultaneous performance objectives. the ability to write, read, interpret, explain and act based on a thorough understanding of technical documents, engineering materials and contracts or related documents.  ability to make professional sales and business presentations in writing, through email, reports, or orally, including complex business and technical matters to an audience of high technical skills, management and operational experience. ability to support a cross-functional samsung team to achieve customer contracted objectives and specific team goals within established time frames and requirements.  assist in and if needed direct the samsung team in lab and field trials related to introduction of products sold to the customer.
physical/mental demands:
skills and qualifications
background/experience required:
bachelors or masters in computer engineering.
4 years of experience in the big data solutions such as pyspark,python, hive and experience on aws.
necessary skills and attributes:
3+ years of experience writing complex sql queries (hive ql/spark sql preferred).
3+ years of python development experience
3+ years of demonstrated technical proficiency with hadoop and big data projects
2+ years of hands on development experience on developing etl pipelines
fluent in writing shell scripts [bash]
experience working with aws components [emr, s3]
working experience in apache airflow
writing high-performance, reliable and maintainable code.
knowledge and ability to implement workflow/schedulers within oozie
knowledge of data modelling concepts
analytical and problem-solving skills, applied to big data domain
proven understanding and hands on experience with hadoop, hive, pig, impala, and spark
good aptitude in multi-threading and concurrency concepts.
. or . in computer science or engineering
* please visit samsung membership to see privacy policy, which defaults according to your location. you can change country/language at the bottom of the page. if you are european economic resident, please click here.
at samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. we aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. together, we are building a better tomorrow for our customers, partners, and communities.
* samsung electronics america, inc. and its subsidiaries are committed to employing a diverse workforce, and  provide equal employment opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law.
covid-19 vaccine mandate
reasonable accommodations for qualified individuals with disabilities during the application process
samsung electronics america is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. if you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our reasonable accommodation team (855-557-3247) or sea_accommodations_ assistance. this number is for accommodation requests only and is not intended for general employment inquiries.
"
3589635372,Data Engineer (Alteryx),"
design and develop workflows that consume data from file based, sql, and rest api sources.
etl scripting, data prep, and cleanse of file-based and sql-based data.
write complex sql queries with multiple joins to automate/manipulate these extracts in alteryx
troubleshoot automation programs/reports already created in alteryx
work with internal customers to refine and clarify requirements
optimize our end-to-end use of alteryx from how we take in data from different sources to how we distribute outputs
make use of alteryx server to schedule, monitor, and enhance workflow performance
serve as the back-up admin for alteryx server and mongodb requirements:
ba in mis, bis, or other it concentration, bscs, or related degree
minimum of 3 years working with databases, data modeling, data management, and data curation
3-5 years of experience using alteryx designer to create data automation workflows
data preparation experience using sql or scripting languages to create etl processes, perform data cleansing, check data integrity
experience writing sql queries against any rdbms with query optimization
sql rdbms experience in microsoft sql server.
minimum 3 years using alteryx server
experience performing installation, migration and/or upgrade of alteryx servers
experience using and managing mongodb preferred/desired
experience in microsoft power bi
some experience in uipath salary range: salary: $96,370.33 to $144,000 per year 9th way insignia's range for this job level is a general guideline only and not a guarantee of compensation or salary. additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. clearance, background investigation: applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
tier1 / low risk location: 100% telework. 9th way insignia reserves the right to adjust work location based upon mission requirements. covid-19: the covid-19 vaccination requirement for federal employees and contractors pursuant to executive order 14043 does not currently apply. some jobs, however, may be subject to agency- or job-specific vaccination requirements. legal: we're an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change - no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law. 
"
3571650945,Data Engineer,"title: data engineer
top skills
strong python (for oop)
postgres database built on aws
data warehousing
light tableau experience
work type: 6-12 month contract with potential to convert full-time
description
opportunity to support data engineering needs at one of the top it companies, located in the bay area.
engaging with business and analytic teams to understand data requirements and translating those requirements into technical solutions.
standardizing and implementing operational processes for data delivery, maintenance, and ensuring data accuracy.
automating technical solutions the integrate data from a variety of data sources.
spotting and implementing optimizations across data usage from multidisciplinary teams.
ensuring data health and quality through monitoring and alerting tools.
proven experience in
designing, building and maintaining data marts.
programming in python, sql (emphasis on postgres), and writing efficient and complex queries and etl jobs that span terabytes of data
working with postgres databases built on aws or other distributed data technologies
implementing data pipelines and applications in a general programming language such as python
working with devops and agile methodologies such as jira, jenkins, gitlab and airflow
understanding of etl, database and object oriented programming concepts (oop)
analyzing logs in tool such as splunk
debugging and reconciling complex systems and integrations spanning tools and teams
building and enhancing existing tableau dashboards with minimal support
picking up and embracing new technologies and languages
would be nice to have an exposure to large-scale data warehouse solutions such as snowflake and redshift
this opportunity cannot support c2c at this time.
about teksystems
we're partners in transformation. we help clients activate ideas and solutions to take advantage of a new world of opportunity. we are a team of 80,000 strong, working with over 6,000 clients, including 80% of the fortune 500, across north america, europe and asia. as an industry leader in full-stack technology services, talent services, and real-world application, we work with progressive leaders to drive change. that's the power of true partnership. teksystems is an allegis group company.
the company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.
"
3587293503,Software Engineer - Python,"who we are
bixal is a mission-driven, woman-owned small business determined to improve people's lives through human-centered strategies and transformative technologies, with a firm belief that everyone has the right to an effective government. 
we deliver on this belief by partnering with leading federal agencies to design, develop, and deliver powerful customer experiences through holistic digital product solutions and strategic communications initiatives––bringing a high standard and unique creative energy to our clients––and our wonderfully diverse culture is what makes it all possible.
bixal unites different people with different perspectives from all over the world! we provide our team with an open and empowered environment where collaboration thrives and solutions flourish.
location
this role has the ability to work remotely within the united states with occasional travel to the client site in the washington dc metropolitan area for onsite meetings. you must be legally authorized to work in the usa. bixal does not provide visa sponsorship.
what will you do?
as a software engineer, you will join a diverse team of software architects, engineers, and developers creating innovative solutions positively impacting client operations and employee experiences. you will develop django, python, web application solutions supporting the cfpb application development and design teams to maintain ongoing production needs as well as create new offerings on  and related digital platforms.
important note
this is a full-time proposal position contingent on contract award.
responsibilities
provide software development, continuous integration, software delivery, systems administration, software quality, and systems documentation support to the cfpb’s digital assets, including the bureau’s public-facing web site,  , as well as internal software tools;
meet with the application development team and content contributors to develop and discuss strategies and plans for cfpb’s web products, to analyze web content needs, and to propose ways the cfpb's web products can address those needs;
create web applications and web services using various web programming languages, frameworks, and databases including python, sql, django, mysql, postgresql, and elasticsearch;
assist with the management and development of cfpb’s website, including configuring and modifying the wagtail cms;
assist in both the architectural and visual modernization and maintenance of the cfpb 's website at  ;
assist in building a strong technical foundation in build, release, and production using continuous integration tools such as jenkins;
engage with various cfpb personnel to understand requirements in order to develop better software for the bureau and identify new ways in which the development team can easily solve cfpb issues;
collaborate with cfpb on the design, development, and data teams to build valuable tools that benefit the cfpb’s day-to-day operations and broader missions;
support, maintain and build out the front-end code base using 508- compliant css, html, javascript, react, and related frameworks;
provide training on a variety of systems development methodologies, best-practices, and tools along with insight into new technologies and solutions that could help the application team and the bureau at large; and
assist in the development of use cases, requirements definition documents, user and administration manuals, detailed design specifications, and training manuals and plans.
qualifications
bachelor's degree.
5 years of relevant working experience.
develops, tests, implements, and maintains web-based application systems.
troubleshoots system problems and issues and looks for ways to improve the application.
has knowledge of commonly- used concepts, practices, and procedures, including: python, django, cloud services, such as aws, container technology, such as docker, and build automation deployment.
ability to undergo a background investigation by the . government and meet eligibility requirements for suitability.
perks & benefits
competitive base salary
flex hours
work from home flexibility
parental leave
medical/dental/vision benefits
flex spending account
company provided short-term disability
company provided life insurance
commuter benefits
11 paid holidays
professional development opportunities
new business referral bonus
bixal is an equal opportunity and affirmative action employer. it ensures equal employment opportunity without discrimination or harassment based on race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, age, disability, national origin, marital or domestic/civil partnership status, genetic information, citizenship status, veteran status, or any other characteristic protected by law.
"
3584116359,Data Engineer (Alteryx) - 16177244,"
team (project) introduction the va\'s office of information security (ois) utilizes the leading data science and automation platform, alteryx, to develop automated data pipelines that populate the agency\'s cybersecurity data warehouse. the data warehouse supports analytics and compliance reporting for the agency\'s enterprise information systems and is a critical component of the va\'s information security and risk management directorate. operated on-premises and supporting approximately 2,000 users, the ois data automation platform\'s architecture consists of alteryx server, mongodb enterprise, microsoft sql server, and the microsoft power platform - including power bi and
9th way insignia is looking for an accomplished data engineer/alteryx developer to join our highly motivated and dedicated team of data engineering and analytic professionals. we are designing and delivering automated data pipelines using process automation tools to create workflows that cleanse and prepare data before populating the cybersecurity data warehouse. we also develop data analytic tools and dashboards that inspect cyber data for evidence of non-compliance with established security controls and automatically alert system owners to mitigate and resolve these issues.
professional level information: the data engineer/alteryx developer position aligns as an engineer - level 3, at 9th way insignia. an engineer-3 plans and directs research and development on complex projects, leads design and development efforts, and makes recommendations affecting cost decisions and architectural design components. an engineer-3 oversees the design, development, implementation, and analysis of technical products and systems, has broad knowledge of engineering methodologies, and assists in complex problem resolution. an engineer-3 has strong technical skills and background, a knack for learning new technologies, and a blend of problem-solving skills and innovative thinking to resolve a wide variety of technical challenges.
functional job (lcat) information: the data engineer will be using the alteryx data science and automation platform to prepare and cleanse data for populating a data warehouse used for cybersecurity business intelligence reporting. this is an exciting opportunity to work on a team supporting cybersecurity analytics and robotic process automation using your data engineering and data wrangling skills to help create the data foundation needed to support this important work. you and your teammates will be working together to implement state-of-the-art intelligent automation of processes and data preparation using advanced tools like alteryx, uipath, and power bi.
qualified candidates should be experienced with etl scripting, data prep, and cleanse of file-based and sql-based data. experience creating data automation workflows with the alteryx platform is required. experience developing visually compelling dashboards using power bi is a plus. you will help the development team by determining how data will be harvested from available cybersecurity information systems, preparing the data for use, and making it available to the analytics development team by developing automated workflows in alteryx. you will be familiar with etl concepts and techniques and have used alteryx to automate data preparation workflows.
responsibilities: design and develop workflows that consume data from file based, sql, and rest api sources. etl scripting, data prep, an cleanse of file-based and sql-based data. write complex sql queries with multiple joins to automate/manipulate these extracts in alteryx troubleshoot automation programs/reports already created in alteryx work with internal customers to refine and clarify requirements optimize our end-to-end use of alteryx from how we take in data from different sources to how we distribute outputs make use of alteryx server to schedule, monitor, and enhance workflow performance serve as the back-up admin for alteryx server and mongodb
requirements: ba in mis, bis, or other it concentration, bscs, or related degree minimum of 3 years working with databases, data modeling, data management, and data curation 3-5 years of experience using alteryx designer to create data automation workflows data preparation experience using sql or scripting languages to create etl processes, perform data cleansing, check data integrity experience writing sql queries against any rdbms with query optimization sql rdbms experience in microsoft sql server. minimum 3 years using alteryx server experience performing installation, migration and/or upgrade of alteryx servers experience using and managing mongodb
preferred/desired: experience in microsoft power bi some experience in uipath
salary range
salary: \$96,370.33 to \$144,000 per year
"
3586176341,Software Engineer (San Francisco),"
what's in it for you:
software engineers at january aren't just responsible for writing code. you'll also own product and team outcomes, advocate for customers, and help other engineers grow.
at the same time, january is deeply invested in your growth as an engineer. you'll work with your manager to craft a development plan with personalized career goals and we'll give you the support and resources to help achieve those goals.
the main ways you'll impact the business are:
client experience. build simple, best-in-class tools to help creditors engage with their borrowers, analyze effectiveness, and automate operations.
technical leverage. develop simple solutions to complex technical scaling problems, improving developer leverage, reliability, security, and compliance. depending on your areas of expertise, this may include infrastructure scaling, domain modeling, performance optimization, and continuous deployment.
developing peers. when you work at january, you work with a team that grows together. regardless of your current level, you'll share your knowledge of systems, engineering practices, and the problem domain, helping ensure everyone's success. you'll do this through code review, pair programming, documentation, and just plain being supportive.
as the company grows, you will be involved in scoping, architecting, and implementing new product lines. you'll have the opportunity to build new lines of business from scratch.
you might be a fit if you value:
genuine collaboration. we recognize and explicitly reward feedback, alignment, and learning from each other.
managers as advocates. you'll report to leaders invested in your growth and promotion, not just their own. managers only succeed when their teams thrive.
product ownership. engineers are owners, not mercenaries. everyone plays a role in discovering, advocating for, and implementing product ideas.
technical quality. we work collaboratively with product to ensure great outcomes without sacrificing quality. sustainable development is a first-class priority.
what you bring to the table:
have 0-6 years of experience as a software engineer
are proficient in at least one programming language such as python, java, or c++
have strong problem-solving and analytical skills
want the chance to collaborate closely with your peers
we find that candidates who perform particularly well in this role have full-stack experience
what we can offer you:
hybrid and remote work opportunities, allowing you to work in the office and from home on a schedule that works for you
pet-friendly office (only in nyc, for now!)
competitive equity packages, giving you a chance to feel true ownership of your work
flexible work hours for better work/life balance
generous new parent leave program for all employees to enjoy bonding time
free lunches for team socializing activities
annual learning and development budget to invest in your professional growth
gym membership reimbursement
medical, dental, and vision insurance
free access to the mental well-being platform spring health plus 3 free virtual sessions with a spring health licensed therapist
commuter benefits for your travel to/from the office
endless growth opportunity and the ability to take on new and exciting challenges
recognitions:
built in's best startups to work for in new york (2023)
built in's best startups to work for in san francisco (2023)
crain's best places to work in nyc (2022)
we are currently hiring for this position in our new york and san francisco offices.
january believes in doing its part to help close the wage gap that continues to plague much of the us workforce. we offer transparent and equitable compensation packages to all existing and future january team members.
our compensation range for the software engineer i & ii role is $115,000 to $155,000. we determine the final package by considering experience, applicable education and training, and relevant skills derived throughout our interview process.
this role also includes a competitive equity package, giving you a chance to feel true ownership of your work.
january is an equal opportunity employer and does not discriminate on the basis of race, color, creed, ethnicity, sex, gender identity, sexual orientation, religion, disability, age, veteran status, or any other category protected by law.
"
3585999116,BIGDATA ENGINEER,"role: bigdata engineer
location: pittsburgh ,pa
duration: fulltime
 
 job description
big data architect – hadoop, hive, scala, spark,pyspark. airflow and kafka exp preferred. cloud era exp preferred
hive, pyspark , python, impala
 ​hadoop architecture
sql
​kudu
"
3583597494,Data Engineer,"category: .
country: united states
location: san diego - hybrid
overview
we are seeking a talented data engineer to join our data and analytics team. the primary purpose of this role is to spearhead the design, development, and maintenance of pipelines and data models that enable the organization to make informed decisions based on high-quality data.
as a data engineer, you will be responsible for taking ownership of data availability, ensuring that our business users and analysts have access to the data they need to evaluate and optimize their performance. you will work closely with cross-functional teams to understand their data requirements and design efficient and scalable solutions to meet those needs.
if you are passionate about leveraging data to drive business outcomes and thrive in a dynamic and fast-paced environment, we encourage you to apply for this exciting opportunity.
we are looking for local san diego candidates who can work a hybrid schedule in this role.
what you’ll be doing
lead the development of sophisticated database pipelines in azure data factory and other microsoft technologies, ensuring the integrity of data and maximizing performance for optimal results.
build and maintain highly scalable and efficient data pipelines in azure data factory, capable of processing large volumes of data without compromising quality or speed.
develop stored procedures, views, functions, and indexes in sql server and azure sql database to support etl/elt (extract transfer load) processes.
collaborate closely with external vendors to understand their technical requirements and align their data architecture with the company's standards and practices.
utilize industry best practices to create and modify databases and perform advanced data modeling, ensuring that critical business information is accurately stored and readily available for informed decision-making.
provide support for existing ssis integrations to maintain legacy operations and ensure seamless data integration across the organization.
innovate and implement new pipelines and processes to improve data quality, accuracy, and reliability, enhancing overall business operations.
collaborate closely with business partners to identify their reporting needs and acquire relevant datasets that align with business objectives.
merge and transform datasets into actionable information that can be used to make informed business decisions, driving operational efficiencies and business success.
work closely with management to understand company strategic objectives and develop datasets that support must-win battles and inform overall business strategy.
create new data validation methods and data analysis tools using power bi to assist with business objectives.
ensure compliance with data governance and security policies, meeting all sox requirements and maintaining the highest levels of data privacy and security.
provide mentorship and training to junior team members, sharing knowledge and expertise on the latest technologies and industry trends to strengthen the team's overall capabilities.
what you’ll bring to the role
4-5+ years of impressive expertise as a highly skilled data engineer with a focus on microsoft technologies.
deep understanding of cloud computing technologies, with a particular emphasis on microsoft azure
proven track record of success in managing third-party vendors
solid knowledge of azure data factory, azure databricks, azure stream analytics, azure sql database, azure cosmos db, and azure blob storage
extensive experience with etl design and implementation, including data quality checks and data integrity
proficient in sql and data warehousing concepts.
exceptional grasp of data structures, algorithms, and software design principles
proven problem-solving, critical thinking, and communication skills
bachelor's or master’s degree in computer science, computer engineering, or a related field,preferred
total rewards
$96000 – $120000/ year includes base salary plus 10% - 15% profit-sharing contribution to your retirement account. salary is determined by multiple factors including but not limited to relevant experience, knowledge, skills, other job-related qualifications, and alignment with market data
medical insurance, 80-95% company-paid premium
annual cash incentive rewards
tuition reimbursement
short-day fridays
vacation
paid holidays, including winter shut down
about us
our “why”
at wd-40 company, we believe that purpose-driven, passionate people guided by our values create amazing outcomes. our ""why"" is refreshingly simple - we exist to create positive lasting memories in everything we do.
why you should apply
learning-based culture that supports the growth and development of all tribe members
strong values aligned culture whose #1 strategic driver is to build a business for the future that we will be proud to hand down to the next generation
over 93.3% employee engagement as of january 2022 global employee survey
a culture where 93% of the tribe report experience a sense of belonging
voted most democratic workplace and certified worldblu freedom centered workplace
do you align with our values?
please, only consider employment with wd-40 company if you feel as strongly about our values as we do: we live, breathe, and play by our values every day.
thank you for considering wd-40 company in your career search! #wd40triballife #positivelastingmemories
at wd-40 company we foster a culture of inclusion where all individuals are recognized, valued, respected, and experience a sense of belonging. all qualified applicants will receive consideration for employment without regard to individual characteristics that make us unique such as our backgrounds, experiences, qualities, talents, traits, beliefs, and preferences.
"
3584960513,Data Engineer Level II,"job description
about us:
we are a full stack data science company and a wholly owned subsidiary of the kroger company. we own 10 petabytes of data, and collect 35+ terabytes of new data each week sourced from 62 million households. as a member of our software engineering team you will use various cutting-edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at kroger.
what you'll do
as a senior data engineer, you are part of the software development team. we develop strategies and solutions to ingest, store, and distribute our big data. our developers use big data technologies including (but not limited to) hadoop, pyspark, hive, json, and sql to develop products, tools and software features.
minimum skills required
bachelor's degree (typically in computer science, management information systems, mathematics, business analytics or another technically strong program), plus 2 years of experience
proven big data technology development experience including hadoop, spark (pyspark), and hive
understanding of agile principles (scrum)
experience developing with python
cloud development (azure)
exposure to vcs (git, svn)
position specific skill preferences
experience developing with sql (oracle, sql server)
exposure to nosql (mongo, cassandra)
apache nifi
airflow
docker
key responsibilities
innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including sql-based, nosql-based, and cloud-based data platforms
define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
mentor team members in data principles, patterns, processes and practices
promote the reuse of data assets, including the management of the data catalog for reference
draft and review architectural diagrams, interface specifications and other design documents
proactively and holistically lead activities that create deliverables to guide the direction, development, and delivery of technological responses to targeted business outcomes.
provide facilitation, analysis, and design tasks required for the development of an enterprise's data and information architecture, focusing on data as an asset for the enterprise.
develop target-state guidance (., reusable standards, design patterns, guidelines, individual parts and configurations) to evolve the technical infrastructure related to data and information across the enterprise, including direct collaboration with 84.51.
required skills : azure,python
basic qualification
additional skills :
background check :yes
drug screen :yes
notes
selling points for candidate :
project verification info
candidate must be your w2 employee :yes
exclusive to apex :no
face to face interview required :no
candidate must be local :no
candidate must be authorized to work without sponsorship ::no
interview times set : :no
type of project :development/engineering
master job title :other
branch code :cincinnati
"
3589726371,Tableau/Data/Python Engineer,"software guidance & assistance, inc., (sga), is searching for a very strong tableau developer with strong python-based data extraction skills for a long-term contractor (right-to-hire option as well) contract assignment with one of our premier financial services clients in mid-town manhattan, nyc. he or she will need to work in the office 3 days/week.
responsibilities
work with different teams to support the design and development of new tableau dashboards.
gather and document business requirements for reports, dashboards and analytics through stakeholder interviews and workshops.
design and develop dashboards in tableau using different data sources.
perform data engineering and data extraction using python
pull data from multiple cloud-based databases (., sql, mongo)
integrate dashboards with 3rd party tools using python, etc.
translate business requirements to functional/technical documentation
active support of existing systems including issue monitoring/resolution and internal client communication.
requirements
very strong tableau developer skills (., tableau desktop, tableau prep and tableau server);proven experience in building dashboards.
strong data engineering using python is also at the core of a successful candidate to pull data from cloud-based dbs (., sql server, postgresql, mongodb.
some exposure working with investment banks/capital markets/trading/financial products or  is required to work effectively with the business teams
high level of attention to detail and diligence in maintaining coding standards and group development practices
bachelor's degree/university degree or equivalent experience
preferred
knowledge of investment/asset management data is a huge plus. for example,(., asset management, hedge funds, and/or broker/dealers are highly preferred)
2+ years' experience processing data using pyspark, pandas,  experience
"
3580163379,Data Engineer - Azure Spark (Databricks),"summary/objective
isolved i s seeking a skilled azure spark (databricks) engineer to join our team. the successful candidate will be responsible for designing, building, and maintaining data processing pipelines using azure spark on databricks. they will also be responsible for optimi zing spark jobs for performance and scalability and troubleshooting issues that arise. will m aintain the technology roadmap of data utilities aligning to business objectives and priorities. data engineer will work with stakeholders (product managers and engineer leads)
to ensure that all the various requirements are considered while designing for target state platform and the capabilities.
c ore job duties
design, build and maintain data processing pipelines using azure spark on databricks .
optimize spark jobs for performance and scalability .
troubleshoot issues that arise with the spark environmen t or data processing pipelines .
collaborate with peers to integrate spark with other technologies in our tech stack .
continuously improve our spark infrastructure and processes .
stay up to date with the latest developments in spark and databricks technolog y .
leverage disparate data sources, relevant external data, and strong domain expertise, to achieve
deliverables.
modeling, ultimately ensuring that the deliverable is high - quality and provides impactful insights .
supervision
normally receives little instruction on day - to - day work, general instructions on new assignments.
experience
typically requires a minimum of 3 years of related experience with a bachelor's degree.
scope
n/a
discretion
n/a
minimum qualifications
minimum of 3 years of experience working with azure spark on databricks
strong programming skills in scala or python
experience with big data technologies such as hadoop and hive
familiarity with cloud computing platforms such as microsoft azure
excellent problem - solving and troubleshooting skills
strong communication and collaboration skills
additional preferred qualifications
bs or ba degree preferred
physical demands
prolonged periods of sitting at a desk and working on a computer. must be able to lift up to 15 pounds.
travel required
yes, up to 15% domestic travel may be required.
wor k authorization
employee must be legally authorized to work in the united states.
flsa classification
exempt
location
office/hybrid/remote
about isolved
isolved is an employee experience leader, providing intuitive, people - first hcm (human capital
"
3585991996,Analytics Engineer,"role & requirements
as an analytics engineer, you can look forward to hitting the ground running with an excellent team. you will play an essential role in supporting the analytics team. day-to-day responsibilities will consist of working with business users to develop analytical solutions to meet business needs and, building reports and analyses for both ongoing and ad-hoc needs, and collaborating with cross-functional teams to build and execute project plans. you will also be tasked with researching, compiling, and analyzing moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable factors. you will be expected to tech and advocate the use of self-service analytical tools and provide technical support to end users for existing reports or other tools.
bachelor’s degree or greater in relative field of study (mathematics, statistics, computer science, finance, etc)
proficiency in writing sql; 1+ year experience of etl
advanced ability to build reports and dashboards with bi tools (tableau, power bi, etc.)
ability to scrutinize the quality of data, solve ambiguous problems
ability to explain technical information in a simple way to non-technical audiences
experience with statistical programming languages such as python or r
restapi familiarity; knowledge of java script development is a plus (node js, azure, react, vue, etc.) *
knowledge or experience with mongodb (mongoose) is a plus *
what else?
remote work opportunities
covered parking
onsite gym
centrally located in the heart of phoenix
kick-ass coworkers
*at this time, freightvana cannot transfer nor sponsor a work visa for this position.
"
3574886982,Data Engineer,"calling all innovators ready to take a proactive approach to insurance in the digital world!
slide is a cutting-edge tampa based insurtech company, recently named a
2023 best places to work** by tampa bay business journal
slide is looking for a data engineer who will assist in the designing, building, and maintaining large-scale data processing systems.
this position is located in tampa at our corporate headquarters and candidate must reside already in tampa region**
duties & responsibilities:
design, build, and maintain scalable data pipelines and etl processes.
collaborate with cross-functional teams to integrate new data sources and optimize data flows.
ensure the reliability and availability of our data infrastructure through monitoring and proactive maintenance.
automate and streamline data processing workflows to increase efficiency and reduce manual effort.
continuously evaluate and implement new technologies to improve the performance and scalability of our data infrastructure.
manage data storage and retrieval using cloud-based technologies such as amazon s3, azure blob storage, or google cloud storage.
perform other duties as assigned.
education, experience & licensing requirements:
bachelors degree or equivalent education and work experience required.
3+ year's experience in data warehouse development required.
3+ year's experience with sql required.
qualifications/skills/competencies:
strong knowledge of etl tools (ssis, ssas, ssrs).
knowledge of sql code performance tuning and optimizing sql code.
familiarity with code repositories such as github, gitlab, and bitbucket.
cloud data warehouse technologies (adf, azure sql, databricks) experience a plus
experience with cloud-based data storage and processing technologies like aws, azure, or gcp.
experience with data modeling and schema design.
comfortable in a fast-paced agile process, embracing tdd and ci/cd practices.
strong interpersonal skills.
excellent verbal and written communication skills.
ability to work independently and with a team and prioritize effectively.
ability to think critically and objectively.
desire to live slide's core values.
what's in it for you?? a paycheck of course but really so much more!
the slide vibe - an opportunity to be a part of a fun and innovation-driven culture fueled by passion, purpose and technology!
benefits - we have extensive and cost-effective benefits that cover you and your family from every angle... physical health, emotional health, financial health, social health, and professional health
full job description
slide 
"
3590354937,Data Engineer – Remote | 953284,"our goal:treat our consultants and clients the way we would like others to treat us!
interested in joining our team? check out the opportunity below and apply today!
a dublin, ohio client has a remote contract opportunity for a data engineer.
job description:
department overview:
supply chain digital partners – supply chain digital solutions team works with inventory management, logistics and warehouse operations business teams on solving some of the business’ biggest challenges, gain efficiency and improve the customer experience by leveraging data engineering, data science and visualization, data automation and data governance & management.
the team drives business innovation by leveraging emerging technologies and turning them into differentiating business capabilities.
responsibilities:
develop bq views per business requirements and best practices. perform data mapping with source systems.
ensure on time delivery of project work solve technical issues and provide quick resolution.
should have advanced sql programming experience with gcp bq. hands on skills with gcp, bigquery, airflow, needed.
work closely with product owners on creating estimates/designs and realizing business value
ensure quality by conducting code review, providing direction to other data engineers
participate in technical platform strategy as tools, products, and business needs evolve
define and execute database and data movement standards, design reviews, pipeline ci/cd process, and data container policies to ensure high quality data management
define how our data analytics and ml/ai capabilities will apply to business needs and result in dependable business solutions
partner with external consultants, solution providers, and managed services organizations to enable product/solution development as well as meeting documented standards
interact with multiple organizations to track project progress, identify risks, communicate risks and status to leadership, and to assess potential impacts to the business.
ensure platforms and tools meet or exceed data security standards, including internal and external audits performed
use strong verbal and written communication skills that non-technical business and end-users can understand.
desired qualifications:
8+ years’ experience with data platforms including gcp, hana, teradata, my sql and sql server, airflow
expert working knowledge of sql, python,
demonstrated expertise of database design and modeling.
expert knowledge of bi reporting and data discovery tools
expert knowledge of cloud technologies
experience with business-critical applications.
experience on large-scale implementation programs preferred.
experience with sap, manhattan score/warehouse management data highly desired
excellent written and oral communication skills.
reference: 953284
about revel it:
revel it (formerly known as fast switch) is one of the fastest-growing, privately held, it staffing companies in the nation. our client base includes 32% of the fortune 25. we have major offices in dublin, oh, phoenix, az, los angeles, ca, and austin, tx and are rapidly expanding into new markets from coast to coast.
why revel it:
5-year client retention: 99%
no. 1 supplier with customers: 53%
top 3 supplier with customers: 77%
consultant retention: 94%
revel it is an equal opportunity employer. revel it does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. all employment is decided on the basis of qualifications, merit, and business need.
#gdr4900
"
3580163379,Data Engineer - Azure Spark (Databricks),"summary/objective
isolved i s seeking a skilled azure spark (databricks) engineer to join our team. the successful candidate will be responsible for designing, building, and maintaining data processing pipelines using azure spark on databricks. they will also be responsible for optimi zing spark jobs for performance and scalability and troubleshooting issues that arise. will m aintain the technology roadmap of data utilities aligning to business objectives and priorities. data engineer will work with stakeholders (product managers and engineer leads)
to ensure that all the various requirements are considered while designing for target state platform and the capabilities.
c ore job duties
design, build and maintain data processing pipelines using azure spark on databricks .
optimize spark jobs for performance and scalability .
troubleshoot issues that arise with the spark environmen t or data processing pipelines .
collaborate with peers to integrate spark with other technologies in our tech stack .
continuously improve our spark infrastructure and processes .
stay up to date with the latest developments in spark and databricks technolog y .
leverage disparate data sources, relevant external data, and strong domain expertise, to achieve
deliverables.
modeling, ultimately ensuring that the deliverable is high - quality and provides impactful insights .
supervision
normally receives little instruction on day - to - day work, general instructions on new assignments.
experience
typically requires a minimum of 3 years of related experience with a bachelor's degree.
scope
n/a
discretion
n/a
minimum qualifications
minimum of 3 years of experience working with azure spark on databricks
strong programming skills in scala or python
experience with big data technologies such as hadoop and hive
familiarity with cloud computing platforms such as microsoft azure
excellent problem - solving and troubleshooting skills
strong communication and collaboration skills
additional preferred qualifications
bs or ba degree preferred
physical demands
prolonged periods of sitting at a desk and working on a computer. must be able to lift up to 15 pounds.
travel required
yes, up to 15% domestic travel may be required.
wor k authorization
employee must be legally authorized to work in the united states.
flsa classification
exempt
location
office/hybrid/remote
about isolved
isolved is an employee experience leader, providing intuitive, people - first hcm (human capital
"
3584193165,Data Engineer - TFT,"data engineer
job posting #:918364 union:non-union or cupe, opseu, ona, :toronto general hospitaldepartment:data operations - clinical support & performance reports to:senior manager, data engineering solutions hours:37.5 hours per weekwage range $34.62 - $43.28 hourly (commensurate with experience and consistent with uhn compensation policy)status:temporary full-time (12 months)posted date:april 27, 2023 closing date:may 12, 2023
uhn data & analytics supports uhn’s growing demand for data, data-driven insights and helps teamuhn ensure the needs of the patients come first. our collective goal is to better see, understand and use data to help uhn become a data-driven organization, which will help us achieve uhn’s strategic goals and ultimately advance care, research and education. uhn data & analytics supports uhn in data-driven decisions, data literacy, performance improvement, data governance, analytics, and automation as well as advances these capabilities through collaboration across the organization.
position summary
duties
developing, supporting, and maintaining etl processes leveraging tools like microsoft’s sql server integration services (ssis)
writing advanced / complex sql queries and scripts to pull data sources
creating / maintaining data models, processes and etl package to feed various data destinations
understanding database management systems, including microsoft sql server in support of the movement of data between systems
developing automation tools for daily tasks. specifically, has the ability to automate, end to end, processes from source system to destination leveraging etl technology.
has experience in maintaining and troubleshooting scripts to ensure repeatable and reliable implementations whether they are new or enhancements
providing tuning expertise to etl packages
pulling data from rest api's including hl7-fhir / xml / json
elt solutions in cloud based / data lake data storage technologies
performing data validation by creating and executing testing plans and routines to ensure integrity of data
ensuring data can be fluidly and securely shared across the enterprise
fulfilling data warehouse maintenance responsibilities including data requests
writing technical documentation
qualifications
bachelor’s degree in computer programming, computer science, or a related field.
2 years of extensive etl/elt building and monitoring experience for data warehouses and / or data lakes required.
familiarity with database technology such as microsoft sql server, ms azure sql database.
familiarity or experience with etl technology such as ms ssis
masterful sql, pl/sql, t-sql, shell scripting and programming skills
fluency or understanding of specific languages, such as python, c#, .net and operating systems.
working knowledge of microsoft power bi is an asset.
experience with design and development of api is an asset.
experience in healthcare / foundation information systems strongly preferred
microsoft azure experience, including azure data factory preferred
focus on efficiency, user experience, and process improvement.
excellent project and time management skills.
strong problem solving and verbal and written communication skills.
ability to work independently or with a group.
why join uhn?
in addition to working alongside some of the most talented and inspiring healthcare professionals in the world, uhn offers a wide range of benefits, programs and perks. it is the comprehensiveness of these offerings that makes it a differentiating factor, allowing you to find value where it matters most to you, now and throughout your career at uhn.
competitive offer packages
government organization and a member of the healthcare of ontario pension plan (hoopp /)
close access to transit and uhn shuttle service
a flexible work environment
opportunities for development and promotions within a large organization
additional perks (multiple corporate discounts including: travel, restaurants, parking, phone plans, auto insurance discounts, on-site gyms, etc.)
current uhn employees must have successfully completed their probationary period, have a good employee record along with satisfactory attendance in accordance with uhn's attendance management program, to be eligible for consideration.
vaccines (covid and others) are a requirement of the job unless you have an exemption on a medical ground pursuant to the ontario human rights code.
uhn is a respectful, caring, and inclusive workplace. we are committed to championing accessibility, diversity and equal opportunity. requests for accommodation can be made at any stage of the recruitment process. applicants need to make their requirements known in advance. any information received related to an accommodation will be addressed confidentially.
university health network thanks all applicants, however, only those selected for an interview will be contacted
"
3574886982,Data Engineer,"calling all innovators ready to take a proactive approach to insurance in the digital world!
slide is a cutting-edge tampa based insurtech company, recently named a
2023 best places to work** by tampa bay business journal
slide is looking for a data engineer who will assist in the designing, building, and maintaining large-scale data processing systems.
this position is located in tampa at our corporate headquarters and candidate must reside already in tampa region**
duties & responsibilities:
design, build, and maintain scalable data pipelines and etl processes.
collaborate with cross-functional teams to integrate new data sources and optimize data flows.
ensure the reliability and availability of our data infrastructure through monitoring and proactive maintenance.
automate and streamline data processing workflows to increase efficiency and reduce manual effort.
continuously evaluate and implement new technologies to improve the performance and scalability of our data infrastructure.
manage data storage and retrieval using cloud-based technologies such as amazon s3, azure blob storage, or google cloud storage.
perform other duties as assigned.
education, experience & licensing requirements:
bachelors degree or equivalent education and work experience required.
3+ year's experience in data warehouse development required.
3+ year's experience with sql required.
qualifications/skills/competencies:
strong knowledge of etl tools (ssis, ssas, ssrs).
knowledge of sql code performance tuning and optimizing sql code.
familiarity with code repositories such as github, gitlab, and bitbucket.
cloud data warehouse technologies (adf, azure sql, databricks) experience a plus
experience with cloud-based data storage and processing technologies like aws, azure, or gcp.
experience with data modeling and schema design.
comfortable in a fast-paced agile process, embracing tdd and ci/cd practices.
strong interpersonal skills.
excellent verbal and written communication skills.
ability to work independently and with a team and prioritize effectively.
ability to think critically and objectively.
desire to live slide's core values.
what's in it for you?? a paycheck of course but really so much more!
the slide vibe - an opportunity to be a part of a fun and innovation-driven culture fueled by passion, purpose and technology!
benefits - we have extensive and cost-effective benefits that cover you and your family from every angle... physical health, emotional health, financial health, social health, and professional health
full job description
slide 
"
